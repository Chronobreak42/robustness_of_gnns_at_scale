@article{Wong2020,
abstract = {Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45{\%} robust accuracy to PGD attacks with {\$}\backslashepsilon=8/255{\$} in 6 minutes, and a robust ImageNet classifier with 43{\%} robust accuracy at {\$}\backslashepsilon=2/255{\$} in 12 hours, in comparison to past work based on "free" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as "catastrophic overfitting" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast{\_}adversarial.},
archivePrefix = {arXiv},
arxivId = {2001.03994},
author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
eprint = {2001.03994},
file = {:Users/simon/Downloads/2001.03994.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR},
pages = {1--17},
title = {{Fast is better than free: Revisiting adversarial training}},
url = {http://arxiv.org/abs/2001.03994},
year = {2020}
}
@article{Geisler2020,
abstract = {Perturbations targeting the graph structure have proven to be extremely effective in reducing the performance of Graph Neural Networks (GNNs), and traditional defenses such as adversarial training do not seem to be able to improve robustness. This work is motivated by the observation that adversarially injected edges effectively can be viewed as additional samples to a node's neighborhood aggregation function, which results in distorted aggregations accumulating over the layers. Conventional GNN aggregation functions, such as a sum or mean, can be distorted arbitrarily by a single outlier. We propose a robust aggregation function motivated by the field of robust statistics. Our approach exhibits the largest possible breakdown point of 0.5, which means that the bias of the aggregation is bounded as long as the fraction of adversarial edges of a node is less than 50$\backslash${\%}. Our novel aggregation function, Soft Medoid, is a fully differentiable generalization of the Medoid and therefore lends itself well for end-to-end deep learning. Equipping a GNN with our aggregation improves the robustness with respect to structure perturbations on Cora ML by a factor of 3 (and 5.5 on Citeseer) and by a factor of 8 for low-degree nodes.},
archivePrefix = {arXiv},
arxivId = {2010.15651},
author = {Geisler, Simon and Z{\"{u}}gner, Daniel and G{\"{u}}nnemann, Stephan},
eprint = {2010.15651},
file = {:Users/simon/Downloads/2010.15651.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
number = {NeurIPS},
title = {{Reliable Graph Neural Networks via Robust Aggregation}},
url = {http://arxiv.org/abs/2010.15651},
year = {2020}
}
@article{Wang2019,
abstract = {Graph-based classification methods are widely used for security analytics. Roughly speaking, graph-based classification methods include collective classification and graph neural network. Attacking a graph-based classification method enables an attacker to evade detection in security analytics. However, existing adversarial machine learning studies mainly focused on machine learning for non-graph data. Only a few recent studies touched adversarial graph-based classification methods. However, they focused on graph neural network, leaving collective classification largely unexplored. We aim to bridge this gap in this work. We consider an attacker's goal is to evade detection via manipulating the graph structure. We formulate our attack as a graph-based optimization problem, solving which produces the edges that an attacker needs to manipulate to achieve its attack goal. However, it is computationally challenging to solve the optimization problem exactly. To address the challenge, we propose several approximation techniques to solve the optimization problem. We evaluate our attacks and compare them with a recent attack designed for graph neural networks using four graph datasets. Our results show that our attacks can effectively evade graph-based classification methods. Moreover, our attacks outperform the existing attack for evading collective classification methods and some graph neural network methods.},
archivePrefix = {arXiv},
arxivId = {1903.00553},
author = {Wang, Binghui and Gong, Neil Zhenqiang},
doi = {10.1145/3319535.3354206},
eprint = {1903.00553},
file = {:Users/simon/Downloads/1903.00553.pdf:pdf},
isbn = {9781450367479},
issn = {15437221},
journal = {ACM Conference on Computer and Communications Security},
keywords = {Adversarial graph neural network,Adversarial graph-based classification,Adversarial machine learning},
pages = {2023--2040},
title = {{Attacking graph-based classification via manipulating the graph structure}},
year = {2019}
}
@article{Nesterov2012,
author = {Nesterov, Yu},
file = {:Users/simon/Downloads/9bab20d116438409d6dde358db1572f4e547.pdf:pdf},
journal = {SIAM J. Optim.},
keywords = {and econometrics,center for operations research,convex optimization,coordinate relaxation,core,e catholique de louvain,fast gradient schemes,google problem,ucl,universit,worst-case efficiency estimates},
pages = {341--362},
title = {{Efficiency of coordinate descent methods on huge-scale optimization problems}},
volume = {22},
year = {2012}
}
@article{Jin2019,
abstract = {Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be prone to topological attacks. Despite substantial efforts to search for new architectures, it still remains a challenge to improve performance in both benign and adversarial situations simultaneously. In this paper, we re-examine the fundamental building block of GCN---the Laplacian operator---and highlight some basic flaws in the spatial and spectral domains. As an alternative, we propose an operator based on graph powering, and prove that it enjoys a desirable property of "spectral separation." Based on the operator, we propose a robust learning paradigm, where the network is trained on a family of "'smoothed" graphs that span a spatial and spectral range for generalizability. We also use the new operator in replacement of the classical Laplacian to construct an architecture with improved spectral robustness, expressivity and interpretability. The enhanced performance and robustness are demonstrated in extensive experiments.},
archivePrefix = {arXiv},
arxivId = {1905.10029},
author = {Jin, Ming and Chang, Heng and Zhu, Wenwu and Sojoudi, Somayeh},
eprint = {1905.10029},
file = {:Users/simon/Downloads/1905.10029.pdf:pdf},
title = {{Power up! Robust Graph Convolutional Network against Evasion Attacks based on Graph Powering}},
url = {http://arxiv.org/abs/1905.10029},
year = {2019}
}
@article{Diakonikolas2018,
abstract = {Block-coordinate descent algorithms and alter- . nating minimization methods are fundamen- ▪ tal optimization algorithms and an important primitive in large-scale optimization and ma-chine learning. While various block-coordinate- descent-type methods have been studied exten-sively, only alternating minimization - which applies to the setting of only two blocks - is known ! to have convergence time that scales independently of the least smooth block. A natural question is then: is the setting of two blocks special? We show that the answer is "no" as long as the ▪ least smooth block can be optimized exactly - An assumption that is also needed in the setting of alternating minimization. We do so by introducing a novel algorithm AR-BCD, whose con-, vergence time scales independently of the least smooth (possibly non-smooth) block. The basic algorithm generalizes both alternating minimization and randomized block coordinate (gradient) descent, and we also provide its accelerated version - AAR-BCD.},
archivePrefix = {arXiv},
arxivId = {1805.09185},
author = {Diakonikolas, Jelena and Orecchia, Lorenzo},
eprint = {1805.09185},
file = {:Users/simon/Downloads/diakonikolas18a.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML},
pages = {2006--2018},
title = {{Alternating Randomized Block Coordinate Descent}},
volume = {3},
year = {2018}
}
@article{Necoara2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06340v3},
author = {Necoara, I and Glineur, F},
eprint = {arXiv:1504.06340v3},
file = {:Users/simon/Downloads/1504.06340.pdf:pdf},
number = {July 2011},
pages = {1--21},
title = {{Random block coordinate descent methods for linearly constrained optimization over networks}},
volume = {2011},
year = {2014}
}
@article{Chen2016,
abstract = {We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.},
archivePrefix = {arXiv},
arxivId = {1604.06174},
author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
eprint = {1604.06174},
file = {:Users/simon/Downloads/1604.06174.pdf:pdf},
journal = {arXiv preprint arXiv:1604.06174},
title = {{Training Deep Nets with Sublinear Memory Cost}},
url = {http://arxiv.org/abs/1604.06174},
year = {2016}
}
@article{Wang2018AttackGC,
author = {Wang, Xiaoyun and Eaton, J and Hsieh, Cho-Jui and Wu, S F},
journal = {ArXiv},
title = {{Attack Graph Convolutional Networks by Adding Fake Nodes}},
volume = {abs/1810.1},
year = {2018}
}
@article{Wright2015,
author = {Wright, Stephen J},
file = {:Users/simon/Downloads/4679.pdf:pdf},
journal = {Mathematical Programming},
keywords = {coordinate descent,parallel numerical,randomized algorithms},
pages = {3--34},
title = {{Coordinate Descent Algorithms}},
volume = {151},
year = {2015}
}
@article{Nesterov2017,
author = {Nesterov, Yu and Stich, S},
file = {:Users/simon/Downloads/view.pdf:pdf},
journal = {Siam J. Optim.},
pages = {110--123},
title = {{Efficiency of accelerated coordinate descent method on structured optimization problems}},
volume = {27},
year = {2017}
}
@unpublished{Hu2020,
abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
archivePrefix = {arXiv},
arxivId = {2005.00687},
author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
eprint = {2005.00687},
file = {:Users/simon/Downloads/2005.00687.pdf:pdf},
pages = {1--33},
title = {{Open Graph Benchmark: Datasets for Machine Learning on Graphs}},
url = {http://arxiv.org/abs/2005.00687},
year = {2020}
}
@article{Bojchevski2020a,
abstract = {Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge - many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.},
archivePrefix = {arXiv},
arxivId = {2007.01570},
author = {Bojchevski, Aleksandar and Klicpera, Johannes and Perozzi, Bryan and Kapoor, Amol and Blais, Martin and R{\'{o}}zemberczki, Benedek and Lukasik, Michal and G{\"{u}}nnemann, Stephan},
doi = {10.1145/3394486.3403296},
eprint = {2007.01570},
file = {:Users/simon/Downloads/2007.01570.pdf:pdf},
isbn = {9781450379984},
journal = {ACM SIGKDD 2020},
keywords = {acm reference format,aleksandar bojchevski,amol kapoor,bryan perozzi,graph neural networks,johannes klicpera,personalized pagerank,scalability},
title = {{Scaling Graph Neural Networks with Approximate PageRank}},
url = {http://arxiv.org/abs/2007.01570{\%}0Ahttp://dx.doi.org/10.1145/3394486.3403296},
year = {2020}
}
@article{Bojchevski2018,
abstract = {Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss – an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty – by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.},
archivePrefix = {arXiv},
arxivId = {1707.03815},
author = {Bojchevski, Aleksandar and G{\"{u}}nnemann, Stephan},
eprint = {1707.03815},
file = {:Users/simon/Downloads/1707.03815.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR},
pages = {1--13},
title = {{Deep Gaussian embedding of graphs: Unsupervised inductive learning via ranking}},
year = {2018}
}
@misc{Biendata2020,
abstract = {Graph structures are ubiquitous in nature and society, ranging from molecules, social networks, paper citations networks and cell signaling pathways. The citation network, first proposed in 1965[1], is a typical type of graph. Analysis of citation networks, where academic papers are the vertices and citations as the linking relations, have many applications, such as scientific impact evaluation, knowledge discovery, and technological and scientific forecasting. Deep learning has brought successes to numerous areas, such as computer vision, natural language processing, speech recognition, healthcare analysis and autonomous driving. Lately, researchers found that various deep learning techniques can be applied to graph by automatically learning to encode graphs into embeddings. [2] However, many deep learning models are highly vulnerable. An adversarial example, which is extremely similar to original input data and sometimes even unnoticeable for human, can confuse machine learning classifier and lead to dramatically worse performance. [3] Deep learning models on graph structures has also been reported to be vulnerable to attacks. In 2018, a KDD best awarded paper demonstrated that the accuracy of node classification significantly drops by performing only several few adversarial perturbations. [4] In citation networks, various types of potential adversarial perturbations also exist. For example, papers published on open-access repositories of electronic paper preprints, such as arXiv, tend to cite more papers despite their lack of peer review process. [5] Another example is coercive citation. In 2019, Nature reported that the well-known academic publisher Elsevier had been investigating hundreds of researchers on deliberately manipulating the peer-review process to boost their own citation numbers. [6] These adversarial perturbations can not only undermine the public trusts and hamper the future development of science and technology, but also greatly interfere efforts in analyzing academic data by machine learning tools. Therefore, a competition which investigates the vulnerability of academic graphs and the methods to defend them will have large and long impact on both theoretical research and applications.},
author = {Biendata},
title = {{KDD Cup 2020: Graph Adversarial Attack and Defense}},
url = {https://www.biendata.xyz/competition/kddcup{\_}2020{\_}formal/},
urldate = {2020-07-31},
year = {2020}
}
@article{Chiang2019,
abstract = {Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy-using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16].},
archivePrefix = {arXiv},
arxivId = {1905.07953},
author = {Chiang, Wei Lin and Li, Yang and Liu, Xuanqing and Bengio, Samy and Si, Si and Hsieh, Cho Jui},
doi = {10.1145/3292500.3330925},
eprint = {1905.07953},
file = {:Users/simon/Downloads/1905.07953.pdf:pdf},
isbn = {9781450362016},
journal = {International Conference on Knowledge Discovery and Data Mining, KDD},
pages = {257--266},
title = {{Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks}},
year = {2019}
}
@article{Wu2019b,
abstract = {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the dc facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlincaritics and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.},
archivePrefix = {arXiv},
arxivId = {1902.07153},
author = {Wu, Felix and Zhang, Tianyi and de Souza, Amauri Holanda and Fifty, Christopher and Yu, Tao and Weinberger, Kilian Q.},
eprint = {1902.07153},
file = {:Users/simon/Downloads/1902.07153.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML},
pages = {11884--11894},
title = {{Simplifying graph convolutional networks}},
volume = {2019-June},
year = {2019}
}
@article{Ying2019,
abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1{\%} on average. GNNExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
archivePrefix = {arXiv},
arxivId = {1903.03894},
author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
eprint = {1903.03894},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Ying et al. - 2019 - GNNExplainer Generating Explanations for Graph Neural Networks.pdf:pdf},
issn = {1049-5258},
number = {iii},
title = {{GNNExplainer: Generating Explanations for Graph Neural Networks}},
url = {http://arxiv.org/abs/1903.03894},
year = {2019}
}
@article{Langeberg2019,
abstract = {Recently, there has been an abundance of works on designing Deep Neural Networks (DNNs) that are robust to adversarial examples. In particular, a central question is which features of DNNs influence adversarial robustness and, therefore, can be to used to design robust DNNs. In this work, this problem is studied through the lens of compression which is captured by the low-rank structure of weight matrices. It is first shown that adversarial training tends to promote simultaneously low-rank and sparse structure in the weight matrices of neural networks. This is measured through the notions of effective rank and effective sparsity. In the reverse direction, when the low rank structure is promoted by nuclear norm regularization and combined with sparsity inducing regularizations, neural networks show significantly improved adversarial robustness. The effect of nuclear norm regularization on adversarial robustness is paramount when it is applied to convolutional neural networks. Although still not competing with adversarial training, this result contributes to understanding the key properties of robust classifiers.},
archivePrefix = {arXiv},
arxivId = {1901.10371},
author = {Langeberg, Peter and Balda, Emilio Rafael and Behboodi, Arash and Mathar, Rudolf},
eprint = {1901.10371},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Langeberg et al. - 2019 - On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks.pdf:pdf},
pages = {1--14},
title = {{On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks}},
url = {http://arxiv.org/abs/1901.10371},
year = {2019}
}
@article{Croux2002a,
abstract = {A maxbias curve is a powerful tool to describe the robustness of an estimator. It tells us how much an estimator can change due to a given fraction of contamination. In this paper, maxbias curves are computed for some univariate location estimators based on subranges: midranges, trimmed means and the univariate Minimum Volume Ellipsoid (MVE) location estimators. These estimators are intuitively appealing and easy to calculate.},
author = {Croux, Christophe and Haesbroeck, Gentiane},
doi = {10.1080/10485250212378},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Croux, Haesbroeck - 2002 - Maxbias curves of robust location estimators based on subranges.pdf:pdf},
issn = {10485252},
journal = {Journal of Nonparametric Statistics},
keywords = {Breakdown value,Location estimator,Maxbias curve,Robustness},
number = {3},
pages = {295--306},
title = {{Maxbias curves of robust location estimators based on subranges}},
volume = {14},
year = {2002}
}
@article{Zuo2004,
author = {Zuo, Yijun},
doi = {10.1007/s101820400169},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Zuo - 2004 - Robustness of weighted Lp-depth and Lp-median.pdf:pdf},
issn = {0002-6018},
journal = {Allgemeines Statistisches Archiv},
keywords = {breakdown point,c14,depth function,efficiency,equivariance,influence func-,jel c10,l p -norm,median,robustness,tion},
number = {2},
pages = {215--234},
title = {{Robustness of weighted Lp-depth and Lp-median}},
volume = {88},
year = {2004}
}
@article{Sen2008,
abstract = {Many real-world applications produce networked data such as the worldwide web (hypertext documents connected through hyperlinks), social networks (such as people connected by friendship links), communication networks (computers connected through communication links), and biological networks (such as protein interaction networks). A recent focus in machine-learning research has been to extend traditional machine-learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data. Copyright {\textcopyright} 2008, Association for the Advancement of Artificial Intelligence. All rights reserved.},
author = {Sen, Prithviraj and Namata, Galileo Mark and Bilgic, Mustafa and Getoor, Lise and Gallagher, Brian and Eliassi-Rad, Tina},
doi = {10.1609/aimag.v29i3.2157},
issn = {07384602},
journal = {AI Magazine},
title = {{Collective classification in network data}},
year = {2008}
}
@article{Zhou2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.08434v4},
author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng},
eprint = {arXiv:1812.08434v4},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - 2018 - Graph Neural Networks A Review of Methods and Applications.pdf:pdf},
journal = {arXiv preprint arXiv:1812.08434},
title = {{Graph Neural Networks : A Review of Methods and Applications}},
year = {2018}
}
@article{Zhang2018,
author = {Zhang, Yingxue and Pal, Soumyasundar and Coates, Mark},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Pal, Coates - 2018 - Bayesian Graph Convolutional Neural Networks for Semi-Supervised Classification.pdf:pdf},
title = {{Bayesian Graph Convolutional Neural Networks for Semi-Supervised Classification}},
year = {2018}
}
@article{Chakraborty,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.00069v1},
author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
eprint = {arXiv:1810.00069v1},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Chakraborty et al. - Unknown - Adversarial Attacks and Defences A Survey.pdf:pdf},
number = {x},
title = {{Adversarial Attacks and Defences : A Survey}},
volume = {x}
}
@inproceedings{Lecuyer2019,
abstract = {Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.},
archivePrefix = {arXiv},
arxivId = {1802.03471},
author = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
booktitle = {IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2019.00044},
eprint = {1802.03471},
isbn = {9781538666609},
issn = {10816011},
keywords = {Adversarial-Examples,Deep-Learning,Defense,Machine-Learning,Security},
title = {{Certified robustness to adversarial examples with differential privacy}},
year = {2019}
}
@article{Wu2020,
abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
archivePrefix = {arXiv},
arxivId = {1901.00596},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
doi = {10.1109/tnnls.2020.2978386},
eprint = {1901.00596},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
title = {{A Comprehensive Survey on Graph Neural Networks}},
year = {2020}
}
@article{Wu2019a,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.00596v4},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Member, Senior and Yu, Philip S},
eprint = {arXiv:1901.00596v4},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2020 - A Comprehensive Survey on Graph Neural Networks.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
title = {{A Comprehensive Survey on Graph Neural Networks}},
year = {2020}
}
@misc{,
abstract = {Understanding trust between humans and AI systems is integral to promoting the development and deployment of socially beneficial and responsible AI. Successfully doing so warrants multidisciplinary collaboration. In order to better understand trust between humans and artificially intelligent systems, the Partnership on AI (PAI), supported by members of its Collaborations Between People and AI Systems (CPAIS) Expert Group, conducted an initial survey and analysis of the multidisciplinary literature on AI, humans, and trust. This project includes a thematically-tagged Bibliography with 78 aggregated research articles, as well as an overview document presenting seven key insights. These key insights, themes, and aggregated texts can serve as fruitful entry points for those investigating the nuances in the literature on humans, trust, and AI, and can help align understandings related to trust between people and AI systems. This work can also inform future research, which should investigate gaps in the research and our bibliography to improve our understanding of how human-AI trust facilitates, or sometimes hinders, the responsible implementation and application of AI technologies.},
booktitle = {Partnership On AI},
title = {{Human-AI Collaboration Trust Literature Review - Key Insights and Bibliography}},
url = {https://www.partnershiponai.org/human-ai-collaboration-trust-literature-review-key-insights-and-bibliography/},
urldate = {2020-05-31}
}
@article{Mosier1998,
author = {Mosier, Kathleen L. and Skitka, Linda J. and Heers, Susan and Burdick, Mark},
doi = {10.1207/s15327108ijap0801_3},
issn = {10508414},
journal = {International Journal of Aviation Psychology},
title = {{Automation bias: Decision making and performance in high-tech cockpits}},
year = {1998}
}
@article{Dietvorst2016,
abstract = {Although evidence-based algorithms consistently outperform human forecasters, people often fail to use them after learning that they are imperfect, a phenomenon known as algorithm aversion. In this paper, we present three studies investigating how to reduce algorithm aversion. In incentivized forecasting tasks, participants chose between using their own forecasts or those of an algorithm that was built by experts. Participants were considerably more likely to choose to use an imperfect algorithm when they could modify its forecasts, and they performed better as a result. Notably, the preference for modifiable algorithms held even when participants were severely restricted in the modifications they could make (Studies 1-3). In fact, our results suggest that participants' preference for modifiable algorithms was indicative of a desire for some control over the forecasting outcome, and not for a desire for greater control over the forecasting outcome, as participants' preference for modifiable algorithms was relatively insensitive to the magnitude of the modifications they were able to make (Study 2). Additionally, we found that giving participants the freedom to modify an imperfect algorithm made them feel more satisfied with the forecasting process, more likely to believe that the algorithm was superior, and more likely to choose to use an algorithm to make subsequent forecasts (Study 3). This research suggests that one can reduce algorithm aversion by giving people some control-even a slight amount-over an imperfect algorithm's forecast.},
author = {Dietvorst, Berkeley J. and Simmons, Joseph P. and Massey, Cade},
doi = {10.1287/mnsc.2016.2643},
issn = {15265501},
journal = {Management Science},
keywords = {Confidence,Decision AIDS,Decision making,Forecasting,Heuristics and biases},
title = {{Overcoming algorithm aversion: People will use imperfect algorithms if they can (even slightly) modify them}},
year = {2016}
}
@article{Report2018,
author = {{Akin Hamid}, Unver},
doi = {10.13140/RG.2.2.19598.00329},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Akin Hamid - 2018 - Artificial Intelligence , Authoritarianism and the Future of Political Systems H . Akın {\"{U}}nver EDAM , Oxford CTGA.pdf:pdf},
journal = {EDAM Research Reports},
number = {July 2018},
title = {{Artificial Intelligence , Authoritarianism and the Future of Political Systems H . Akın {\"{U}}nver | EDAM , Oxford CTGA {\&} Kadir Has University}},
volume = {3},
year = {2018}
}
@article{Vaughan2019,
author = {Vaughan, Jennifer Wortman and Wallach, Hanna},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Yin, Vaughan, Wallach - 2019 - Understanding the Effect of Accuracy on Trust in Machine Learning Models.pdf:pdf},
isbn = {9781450359702},
keywords = {2019,Machine learning, trust, human-subject experiments,acm reference format,and hanna wallach,human-subject experiments,jennifer wortman vaughan,machine learning,ming yin,trust},
pages = {1--12},
title = {{Understanding the Effect of Accuracy on Trust in Machine Learning Models}},
year = {2019}
}
@article{8456834,
abstract = {This paper surveys reasons for and against pursuing the field of machine ethics, understood as research aiming to build “ethical machines.” We clarify the nature of this goal, why it is worth pursuing, and the risks involved in its pursuit. First, we survey and clarify some of the philosophical issues surrounding the concept of an “ethical machine” and the aims of machine ethics. Second, we argue that while there are good prima facie reasons for pursuing machine ethics, including the potential to improve the ethical alignment of both humans and machines, there are also potential risks that must be considered. Third, we survey these potential risks and point to where research should be devoted to clarifying and managing potential risks. We conclude by making some recommendations about the questions that future work could address.},
author = {Cave, S and Nyrup, R and Vold, K and Weller, A},
doi = {10.1109/JPROC.2018.2865996},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2019 - Motivations and Risks of Machine Ethics.pdf:pdf},
issn = {1558-2256},
journal = {Proceedings of the IEEE},
keywords = {Biomedical monitoring,Biosensors,Cognitive processes,Complexity theory,Ethical alignment,Ethics,Social implications of technology,Stakeholders,cognitive systems,ethical aspects,ethical machine,ethical reasoning,human-machine ethical alignment,machine agency,machine ethics,man-machine systems,philosophical aspects,philosophical issues,potential risk management,risk management},
month = {mar},
number = {3},
pages = {562--574},
title = {{Motivations and Risks of Machine Ethics}},
volume = {107},
year = {2019}
}
@techreport{Balaram2018,
author = {Balaram, Brhmie and Greenham, Tony and Leonard, Jasmine},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Balaram, Greenham, Leonard - 2018 - Artificial Intelligence Real Public Engagement.pdf:pdf},
institution = {RSA (Royal Society for the encouragement of Arts, Manufactures and Commerce)},
title = {{Artificial Intelligence: Real Public Engagement}},
year = {2018}
}
@inproceedings{inproceedings,
author = {Yin, Ming and Vaughan, Jennifer and Wallach, Hanna},
booktitle = {Conference on Human Factors in Computing Systems, CHI 2019},
doi = {10.1145/3290605.3300509},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Yin, Vaughan, Wallach - 2019 - Understanding the Effect of Accuracy on Trust in Machine Learning Models.pdf:pdf},
isbn = {978-1-4503-5970-2},
pages = {1--12},
title = {{Understanding the Effect of Accuracy on Trust in Machine Learning Models}},
year = {2019}
}
@article{,
doi = {10.1109/JPROC.2018.2865996},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2019 - Motivations and Risks of Machine Ethics.pdf:pdf},
number = {3},
title = {{Motivations and Risks of Machine Ethics}},
volume = {107},
year = {2019}
}
@article{Maronna1976,
abstract = {Let x1,⋯, xn be a sample from an m-variate distribution which is spherically symmetric up to an affine transformation. This paper deals with the robust estimation of the location vector t and scatter matrix V by means of "M-estimators," defined as solutions of the system: ∑i u1(di)(xi - t) = 0 and n-1∑i u2(di 2)(xi - t)(xi - t)' = V, where di 2 = (xi - t)'V-1(xi - t). Existence and uniqueness of solutions of this system are proved under general assumptions about the functions u1 and u2. Then the estimators are shown to be consistent and asymptotically normal. The breakdown bound and the influence function are calculated, showing some weaknesses of the estimates for high dimensionality. An algorithm for the numerical calculation of the estimators is described. Finally, numerical values of asymptotic variances, and Monte Carlo small-sample results are exhibited.},
author = {Maronna, Ricardo Antonio},
doi = {10.1214/aos/1176343347},
issn = {0090-5364},
journal = {The Annals of Statistics},
title = {{Robust {\$}M{\$}-Estimators of Multivariate Location and Scatter}},
year = {1976}
}
@article{Donoho1992,
abstract = {We describe multivariate generalizations of the median, trimmed mean and W estimates. The estimates are based on a geometric construction related to "projection pursuit." They are both affine equivariant (coordinate-free) and have high breakdown point. The generalization of the median has a breakdown point of at least 1/(d + 1) in dimension d and the breakdown point can be as high as 1/3 under symmetry. In contrast, various estimators based on rejecting apparent outliers and taking the mean of the remaining observations have breakdown points not larger than 1/(d + 1) in dimhttp://projecteuclid.org/download/pdf{\_}1/euclid.aos/1176348890ension d. CR - Copyright {\&}{\#}169; 1992 Institute of Mathematical Statistics},
author = {Donoho, David L. and Gasko, Miriam},
doi = {10.1214/aos/1176348890},
issn = {0090-5364},
journal = {The Annals of Statistics},
title = {{Breakdown Properties of Location Estimates Based on Halfspace Depth and Projected Outlyingness}},
year = {1992}
}
@article{Davies1987,
abstract = {It is shown under appropriate conditions that$\backslash$nRousseeuw's minimum volume estimator and other S-estimators of$\backslash$nmultivariate location and dispersion parameters are consistent. Under$\backslash$ncertain differentiability conditions the estimates are asymptotically$\backslash$nnormally distributed with a norming factor of n1/2.},
author = {Davies, P. L.},
doi = {10.1214/aos/1176350505},
issn = {0090-5364},
journal = {The Annals of Statistics},
title = {{Asymptotic Behaviour of {\$}S{\$}-Estimates of Multivariate Location Parameters and Dispersion Matrices}},
year = {1987}
}
@article{Small1990a,
abstract = {In this paper we survey a sequence of papers whose primary aim is the generalization of the concept of the median into higher dimensional settings. While a variety of distinct definitions of the median of a multivariate data set are possible these definitions have the common property of producing the usual definition when applied to univariate data or a univariate distribution. Some common ideas of equivariance, symmetry and breakdown are discussed as well as computational convenience for each definition. The extension of these ideas to directional statistics is also discussed. /// L'auteur revoit une s{\'{e}}rie d'articles qui g{\'{e}}n{\'{e}}ralisent de plusieurs mani{\`{e}}res le concept de m{\'{e}}diane dans les espaces multidimensionnels. Ces diff{\'{e}}rentes mesures reproduisent la definition usuelle de la m{\'{e}}diane dans le cas de loi unidimensionnelle. Il examine en outre les concepts d'{\'{e}}quivariance, de symm{\'{e}}trie, de rupture ainsi que la difficult{\'{e}} de calcul de la m{\'{e}}diane pour chaque definition. Enfin, il g{\'{e}}n{\'{e}}ralise ces id{\'{e}}es pour les lois directionnelles.},
author = {Small, Christopher G.},
doi = {10.2307/1403809},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Small - 1990 - A Survey of Multidimensional Medians(2).pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
keywords = {1 background and history,a natural robu,affine,as a nonparametric and,breakdown,data set arises chiefly,directional statistics,equivariance,estimate for the center,in nonparametric problems as,invariance,m,median,of a distribution,robust estimate for,the median of a},
number = {3},
pages = {263},
title = {{A Survey of Multidimensional Medians}},
volume = {58},
year = {1990}
}
@article{McCallum2000,
abstract = {Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains. {\textcopyright} 2000 Kluwer Academic Publishers.},
author = {McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
doi = {10.1023/A:1009953814988},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Crawling,Expectation-maximization,Hidden markov models,Information extraction,Naive bayes,Reinforcement learning,Spidering,Text classification,Unlabeled data},
title = {{Automating the construction of internet portals with machine learning}},
year = {2000}
}
@article{Brubaker2009,
abstract = {This paper presents a polynomial algorithm for learning mixtures of logconcave distributions in ℝ n in the presence of malicious noise. That is, each sample is corrupted with some small probability, being replaced by a point about which we can make no assumptions. A key element of the algorithm is Robust Principle Components Analysis (PCA), which is less susceptible to corruption by noisy points. While noise may cause standard PCA to collapse well-separated mixture components so that they are indistinguishable, Robust PCA preserves the distance between some of the components, making a partition possible. It then recurses on each half of the mixture until every component is isolated. The success of this algorithm requires only a O*(log n) factor increase in the required separation between components of the mixture compared to the noiseless case. Copyright {\textcopyright} by SIAM.},
author = {Brubaker, S. Charles},
doi = {10.1137/1.9781611973068.117},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Brubaker - 2009 - Robust PCA and clustering in noisy mixtures.pdf:pdf},
isbn = {9780898716801},
journal = {Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1078--1087},
title = {{Robust PCA and clustering in noisy mixtures}},
year = {2009}
}
@article{Diakonikolas2019c,
abstract = {We study high-dimensional distribution learning in an agnostic setting where an adversary is allowed to arbitrarily corrupt an ϵ-fraction of the samples. Such questions have a rich history spanning statistics, machine learning, and theoretical computer science. Even in the most basic settings, the only known approaches are either computat ionally inefficient or lose dimensiondependent factors in their error guarantees. This raises the following question: Is high-dimensional agnostic distribution learning even possible, algorithmically? In this work, we obtain the first computationally efficient algorithms with dimension-independent error guarantees for agnostically learning several fundamental classes of high-dimensional distributions: (1) a single Gaussian, (2) a product distribution on the hypercube, (3) mixtures of two product distributions (under a natural balancedness condition), and (4) mixtures of spherical Gaussians. Our a lgorithms achieve error that is independent of the dimension, and in many cases scales nearly linearly with the fraction of adversarially corrupted samples. Moreover, we develop a general recipe for detecting and correcting corruptions in high-dimensions that may be applicable to many other problems.},
archivePrefix = {arXiv},
arxivId = {1604.06443},
author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
doi = {10.1137/17M1126680},
eprint = {1604.06443},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas et al. - 2019 - Robust estimators in high-dimensions without the computational intractability(2).pdf:pdf},
issn = {10957111},
journal = {SIAM Journal on Computing},
keywords = {Gaussian distribution,High-dimensions,Mixture models,Product distributions,Robust learning},
number = {2},
pages = {742--864},
title = {{Robust estimators in high-dimensions without the computational intractability}},
volume = {48},
year = {2019}
}
@article{Lai2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.06968v2},
author = {Lai, Kevin A and Rao, Anup B},
eprint = {arXiv:1604.06968v2},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Lai, Rao - 2016 - Agnostic Estimation of Mean and Covariance.pdf:pdf},
title = {{Agnostic Estimation of Mean and Covariance}},
year = {2016}
}
@inproceedings{Wong2018,
abstract = {We propose a method to learn deep ReLU-based classifiers that are provably robust against normbounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8{\%} test error for any adversarial attack with bounded ℓ∞ norm less than ϵ = 0.1), and code for all experiments is available at http://github.com/locuslab/convex-adversarial.},
archivePrefix = {arXiv},
arxivId = {1711.00851},
author = {Wong, Eric and Kolter, J. Zico},
booktitle = {35th International Conference on Machine Learning, ICML},
eprint = {1711.00851},
isbn = {9781510867963},
title = {{Provable defenses against adversarial examples via the convex outer adversarial polytope}},
year = {2018}
}
@article{Akhtar2019,
author = {Akhtar, Zahid and Dasgupta, Dipankar},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Akhtar, Dasgupta - 2019 - A brief survey of Adversarial Machine Learning and Defense Strategies Abstract Overview.pdf:pdf},
title = {{A brief survey of Adversarial Machine Learning and Defense Strategies Abstract : Overview :}},
year = {2019}
}
@inproceedings{Hein2017,
abstract = {Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier with no or small loss in prediction performance.},
archivePrefix = {arXiv},
arxivId = {1705.08475},
author = {Hein, Matthias and Andriushchenko, Maksym},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1705.08475},
issn = {10495258},
title = {{Formal guarantees on the robustness of a classifier against adversarial Manipulation}},
year = {2017}
}
@incollection{Donoho1983,
address = {Wadsworth, Belmont, CA, 1983},
author = {Donoho, David and Huber, Peter J.},
booktitle = {A Festschrift For Erich L. Lehmann},
pages = {157--184},
publisher = {Wadsworth Statist./Probab. Ser.},
title = {{The notion of breakdown point}},
year = {1983}
}
@inproceedings{Hamilton2017a,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.02216},
issn = {10495258},
title = {{Inductive representation learning on large graphs}},
year = {2017}
}
@article{Wu2019,
abstract = {Graph deep learning models, such as graph convolutional networks (GCN) achieve state-of-the-art performance for tasks on graph data. However, similar to other deep learning models, graph deep learning models are susceptible to adversarial attacks. However, compared with non-graph data the discrete nature of the graph connections and features provide unique challenges and opportunities for adversarial attacks and defenses. In this paper, we propose techniques for both an adversarial attack and a defense against adversarial attacks. Firstly, we show that the problem of discrete graph connections and the discrete features of common datasets can be handled by using the integrated gradient technique that accurately determines the effect of changing selected features or edges while still benefiting from parallel computations. In addition, we show that an adversarially manipulated graph using a targeted attack statistically differs from un-manipulated graphs. Based on this observation, we propose a defense approach which can detect and recover a potential adversarial perturbation. Our experiments on a number of datasets show the effectiveness of the proposed techniques.},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.01610v3},
author = {Wu, Huijun and Wang, Chen and Tyshetskiy, Yuriy and Docherty, Andrew and Lu, Kai and Zhu, Liming},
doi = {10.24963/ijcai.2019/669},
eprint = {arXiv:1903.01610v3},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2019 - Adversarial examples for graph data Deep insights into attack and defense.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4816--4823},
title = {{Adversarial examples for graph data: Deep insights into attack and defense}},
volume = {2019-Augus},
year = {2019}
}
@article{Hamilton2017,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Hamilton, Ying, Leskovec - 2017 - Inductive representation learning on large graphs.pdf:pdf},
issn = {03706699},
journal = {Advances in Neural Information Processing Systems},
title = {{Inductive representation learning on large graphs}},
year = {2017}
}
@article{Waniek2018,
abstract = {The Internet and social media have fuelled enormous interest in social network analysis. New tools continue to be developed and used to analyse our personal connections, with particular emphasis on detecting communities or identifying key individuals in a social network. This raises privacy concerns that are likely to exacerbate in the future. With this in mind, we ask the question 'Can individuals or groups actively manage their connections to evade social network analysis tools?' By addressing this question, the general public may better protect their privacy, oppressed activist groups may better conceal their existence and security agencies may better understand how terrorists escape detection. We first study how an individual can evade 'node centrality' analysis while minimizing the negative impact that this may have on his or her influence. We prove that an optimal solution to this problem is difficult to compute. Despite this hardness, we demonstrate how even a simple heuristic, whereby attention is restricted to the individual's immediate neighbourhood, can be surprisingly effective in practice; for example, it could easily disguise Mohamed Atta's leading position within the World Trade Center terrorist network. We also study how a community can increase the likelihood of being overlooked by community-detection algorithms. We propose a measure of concealment-expressing how well a community is hidden-and use it to demonstrate the effectiveness of a simple heuristic, whereby members of the community either 'unfriend' certain other members or 'befriend' some non-members in a coordinated effort to camouflage their community.},
archivePrefix = {arXiv},
arxivId = {1608.00375},
author = {Waniek, Marcin and Michalak, Tomasz P. and Wooldridge, Michael J. and Rahwan, Talal},
doi = {10.1038/s41562-017-0290-3},
eprint = {1608.00375},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Waniek et al. - 2018 - Hiding individuals and communities in a social network.pdf:pdf},
issn = {23973374},
journal = {Nature Human Behaviour},
number = {2},
pages = {139--147},
title = {{Hiding individuals and communities in a social network}},
volume = {2},
year = {2018}
}
@article{Flores2015,
abstract = {A quantitative study of the robustness properties of the 1 and the Huber M-estimator on finite samples is presented. The focus is on the linear model involving a fixed design matrix and additive errors restricted to the dependent variables consisting of noise and sparse outliers. We derive sharp error bounds for the estimator in terms of the leverage constants of a design matrix introduced here. A similar analysis is performed for Huber's estimator using an equivalent problem formulation of independent interest. Our analysis considers outliers of arbitrary magnitude, and we recover breakdown point results as particular cases when outliers diverge. The practical implications of the theoretical analysis are discussed on two real datasets.},
author = {Flores, Salvador},
doi = {10.1007/s11749-015-0435-5},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Flores - 2015 - Sharp non-asymptotic performance bounds for and Huber robust regression estimators.pdf:pdf},
issn = {11330686},
journal = {Test},
keywords = {(formula presented.)norm minimization,Breakdown point,Huber M-estimator,Leverage constants,Leverage plot,Sparse outliers},
number = {4},
pages = {796--812},
title = {{Sharp non-asymptotic performance bounds for and Huber robust regression estimators}},
volume = {24},
year = {2015}
}
@article{Triggs2000,
abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing pa- rameter estimates. Topics covered include: the choice ofcost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
author = {Triggs, Bill and Mclauchlan, Philip and Hartley, Richard and Fitzgibbon, Andrew and Triggs, Bill and Mclauchlan, Philip and Hartley, Richard and Fitzgibbon, Andrew and Adjustment, Bundle and Synthesis, A Modern and Triggs, Bill and Zisserman, Andrew and International, Richard Szeliski},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Triggs et al. - 2000 - Bundle Adjustment – A Modern Synthesis.pdf:pdf},
isbn = {3540444807},
journal = {International Workshop on Vision Algorithms},
pages = {298--372},
title = {{Bundle Adjustment – A Modern Synthesis}},
year = {2000}
}
@article{Tang2020,
abstract = {Graph neural networks (GNNs) are widely used in many applications. However, their robustness against adversarial attacks is criticized. Prior studies show that using unnoticeable modifications on graph topology or nodal features can significantly reduce the performances of GNNs. It is very challenging to design robust graph neural networks against poisoning attack and several efforts have been taken. Existing work aims at reducing the negative impact from adversarial edges only with the poisoned graph, which is sub-optimal since they fail to discriminate adversarial edges from normal ones. On the other hand, clean graphs from similar domains as the target poisoned graph are usually available in the real world. By perturbing these clean graphs, we create supervised knowledge to train the ability to detect adversarial edges so that the robustness of GNNs is elevated. However, such potential for clean graphs is neglected by existing work. To this end, we investigate a novel problem of improving the robustness of GNNs against poisoning attacks by exploring clean graphs. Specifically, we propose PA-GNN, which relies on a penalized aggregation mechanism that directly restrict the negative impact of adversarial edges by assigning them lower attention coefficients. To optimize PA-GNN for a poisoned graph, we design a meta-optimization algorithm that trains PA-GNN to penalize perturbations using clean graphs and their adversarial counterparts, and transfers such ability to improve the robustness of PA-GNN on the poisoned graph. Experimental results on four real-world datasets demonstrate the robustness of PA-GNN against poisoning attacks on graphs.},
archivePrefix = {arXiv},
arxivId = {1908.07558},
author = {Tang, Xianfeng and Li, Yandong and Sun, Yiwei and Yao, Huaxiu and Mitra, Prasenjit and Wang, Suhang},
doi = {10.1145/3336191.3371851},
eprint = {1908.07558},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Tang et al. - 2020 - Transferring robustness for graph neural network against poisoning attacks.pdf:pdf},
isbn = {9781450368223},
journal = {Conference on Web Search and Data Mining, WSDM},
keywords = {Adversarial defense,Robust graph neural networks},
pages = {600--608},
title = {{Transferring robustness for graph neural network against poisoning attacks}},
year = {2020}
}
@article{Croux2002,
abstract = {Estimating multivariate location and scatter with both affine equivariance and positive breakdown has always been difficult. A well-known estimator which satisfies both properties is the Minimum Volume Ellipsoid Estimator (MVE). Computing the exact MVE is often not feasible, so one usually resorts to an approximate algorithm. In the regression setup, algorithms for positive-breakdown estimators like Least Median of Squares typically recompute the intercept at each step, to improve the result. This approach is called intercept adjustment. In this paper we show that a similar technique, called location adjustment, can be applied to the MVE. For this purpose we use the Minimum Volume Ball (MVB), in order to lower the MVE objective function. An exact algorithm for calculating the MVB is presented. As an alternative to MVB location adjustment we propose L1 location adjustment, which does not necessarily lower the MVE objective function but yields more efficient estimates for the location part. Simulations compare the two types of location adjustment. We also obtain the maxbias curves of both L1 and the MVB in the multivariate setting, revealing the superiority of L1.},
author = {Croux, Christophe and Haesbroeck, Gentiane and Rousseeuw, Peter J.},
doi = {10.1023/A:1020713207683},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Croux, Haesbroeck, Rousseeuw - 2002 - Location adjustment for the minimum volume ellipsoid estimator.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Intercept adjustment,L1 estimation,Location adjustment,Location estimation,Minimum volume ellipsoid,Robustness},
number = {3},
pages = {191--200},
title = {{Location adjustment for the minimum volume ellipsoid estimator}},
volume = {12},
year = {2002}
}
@article{Li2020,
abstract = {DeepRobust is a PyTorch adversarial learning library which aims to build a comprehensive and easy-to-use platform to foster this research field. It currently contains more than 10 attack algorithms and 8 defense algorithms in image domain and 9 attack algorithms and 4 defense algorithms in graph domain, under a variety of deep learning architectures. In this manual, we introduce the main contents of DeepRobust with detailed instructions. The library is kept updated and can be found at https://github.com/DSE-MSU/DeepRobust.},
archivePrefix = {arXiv},
arxivId = {2005.06149},
author = {Li, Yaxin and Jin, Wei and Xu, Han and Tang, Jiliang},
eprint = {2005.06149},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2020 - DeepRobust A PyTorch Library for Adversarial Attacks and Defenses.pdf:pdf},
pages = {1--19},
title = {{DeepRobust: A PyTorch Library for Adversarial Attacks and Defenses}},
url = {http://arxiv.org/abs/2005.06149},
year = {2020}
}
@article{Miller2020,
abstract = {Vertex classification is vulnerable to perturbations of both graph topology and vertex attributes, as shown in recent research. As in other machine learning domains, concerns about robustness to adversarial manipulation can prevent potential users from adopting proposed methods when the consequence of action is very high. This paper considers two topological characteristics of graphs and explores the way these features affect the amount the adversary must perturb the graph in order to be successful. We show that, if certain vertices are included in the training set, it is possible to substantially an adversary's required perturbation budget. On four citation datasets, we demonstrate that if the training set includes high degree vertices or vertices that ensure all unlabeled nodes have neighbors in the training set, we show that the adversary's budget often increases by a substantial factor---often a factor of 2 or more---over random training for the Nettack poisoning attack. Even for especially easy targets (those that are misclassified after just one or two perturbations), the degradation of performance is much slower, assigning much lower probabilities to the incorrect classes. In addition, we demonstrate that this robustness either persists when recently proposed defenses are applied, or is competitive with the resulting performance improvement for the defender.},
archivePrefix = {arXiv},
arxivId = {2003.05822},
author = {Miller, Benjamin A. and {\c{C}}amurcu, Mustafa and Gomez, Alexander J. and Chan, Kevin and Eliassi-Rad, Tina},
eprint = {2003.05822},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Miller et al. - 2020 - Topological Effects on Attacks Against Vertex Classification.pdf:pdf},
journal = {arXiv preprint arXiv:2003.05822},
title = {{Topological Effects on Attacks Against Vertex Classification}},
url = {http://arxiv.org/abs/2003.05822},
year = {2020}
}
@article{Jin2020,
abstract = {Deep neural networks (DNNs) have achieved significant performance in various tasks. However, recent studies have shown that DNNs can be easily fooled by small perturbation on the input, called adversarial attacks. As the extensions of DNNs to graphs, Graph Neural Networks (GNNs) have been demonstrated to inherit this vulnerability. Adversary can mislead GNNs to give wrong predictions by modifying the graph structure such as manipulating a few edges. This vulnerability has arisen tremendous concerns for adapting GNNs in safety-critical applications and has attracted increasing research attention in recent years. Thus, it is necessary and timely to provide a comprehensive overview of existing graph adversarial attacks and the countermeasures. In this survey, we categorize existing attacks and defenses, and review the corresponding state-of-the-art methods. Furthermore, we have developed a repository with representative algorithms (https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph). The repository enables us to conduct empirical studies to deepen our understandings on attacks and defenses on graphs.},
archivePrefix = {arXiv},
arxivId = {2003.00653},
author = {Jin, Wei and Li, Yaxin and Xu, Han and Wang, Yiqi and Tang, Jiliang},
eprint = {2003.00653},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - 2020 - Adversarial Attacks and Defenses on Graphs A Review and Empirical Study.pdf:pdf},
pages = {1--22},
title = {{Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study}},
url = {http://arxiv.org/abs/2003.00653},
year = {2020}
}
@article{Hubert2018,
abstract = {The minimum covariance determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. Since estimating the covariance matrix is the cornerstone of many multivariate statistical methods, the MCD is an important building block when developing robust multivariate techniques. It also serves as a convenient and efficient tool for outlier detection. The MCD estimator is reviewed, along with its main properties such as affine equivariance, breakdown value, and influence function. We discuss its computation, and list applications and extensions of the MCD in applied and methodological multivariate statistics. Two recent extensions of the MCD are described. The first one is a fast deterministic algorithm which inherits the robustness of the MCD while being almost affine equivariant. The second is tailored to high-dimensional data, possibly with more dimensions than cases, and incorporates regularization to prevent singular matrices. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Multivariate Analysis Statistical and Graphical Methods of Data Analysis {\textgreater} Robust Methods Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Knowledge Discovery.},
archivePrefix = {arXiv},
arxivId = {1709.07045},
author = {Hubert, Mia and Debruyne, Michiel and Rousseeuw, Peter J.},
doi = {10.1002/wics.1421},
eprint = {1709.07045},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Hubert, Debruyne, Rousseeuw - 2018 - Minimum covariance determinant and extensions.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {algorithms,covariance matrix,multivariate statistics,outlier detection,robust estimation},
number = {3},
pages = {1--11},
title = {{Minimum covariance determinant and extensions}},
volume = {10},
year = {2018}
}
@book{White1931,
author = {White, Langdon and Weber, Alfred and Friedrich, C. J.},
booktitle = {The Univserity of Chicago press},
doi = {10.2307/140943},
issn = {00130095},
title = {{Theory of the Location of Industries}},
year = {1931}
}
@article{Tukey1960,
abstract = {Terminology Barnett. (robustness of validity,efficiency) Long comparison efficiency standard deviation and mean (absolute)deviation. Inh: History,Nature of Problem, Sharpening and Reduciotn of PR. Estimate average value of function e-cz 2},
author = {Tukey, J W},
isbn = {0804705968},
journal = {Contributions to Probability and Statistics Essays in Honor of Harold Hotelling},
title = {{A Survey of Sampling from Contaminated Distributions}},
year = {1960}
}
@book{Weber1909,
abstract = {This article focuses on the factors that determine location of industries. The enterprise is started in order to boom the town, to give work to the unemployed, to utilize some plot or site otherwise unusable, to confer value on adjoining real estate or to give safe employment to capital under the watchful eye of the owner. The remaining causes are rational and economic; that is, the selected locality is deemed to offer certain advantages in production or marketing over any other equally available point. To review the economic forces that determines the location of migration of industries, and to describe their mode of operating. First in importance in fixing the home of certain industries is the presence of natural deposits or supplies. This determines imperiously the location of mines, quarries, oil or gas wells, fisheries, lumber and fur industries, and the collecting of nitrates, borax, sponges, pearls, buffalo horns. Climate is not only decisive for vegetal products, but appears to play no small role in locating manufactures.},
author = {Weber, Alfred},
booktitle = {Teil 1: Reine Theorie des Standorts},
isbn = {5880850846},
title = {{{\"{U}}ber den Standort der Industrien: Teil 1: Reine Theorie des Standorts}},
year = {1909}
}
@article{Huber1964,
abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1,⋯,xnx{\_}1, $\backslash$cdots, x{\_}n be independent random variables with common distribution function F(t−$\xi$)F(t - $\backslash$xi). The problem is to estimate the location parameter $\xi$$\backslash$xi, but with the complication that the prototype distribution F(t)F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F=(1−ϵ)$\Phi$+ϵHF = (1 - $\backslash$epsilon)$\backslash$Phi + $\backslash$epsilon H, where 0≦ϵ{\textless}10 $\backslash$leqq $\backslash$epsilon {\textless} 1 is a known number, $\Phi$(t)=(2$\pi$)−12∫t−∞exp(−12s2)ds$\backslash$Phi(t) = (2$\backslash$pi){\^{}}{\{}-$\backslash$frac{\{}1{\}}{\{}2{\}}{\}} $\backslash$int{\^{}}t{\_}{\{}-$\backslash$infty{\}} $\backslash$exp(-$\backslash$frac{\{}1{\}}{\{}2{\}}s{\^{}}2) ds is the standard normal cumulative and HH is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction ϵ$\backslash$epsilon of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., supt|F(t)−$\Phi$(t)|≦ϵ$\backslash$sup{\_}t |F(t) - $\backslash$Phi(t)| $\backslash$leqq $\backslash$epsilon. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed ϵ$\backslash$epsilon, there will be several values of $\xi$$\backslash$xi and $\sigma$$\backslash$sigma such that supt|F(t)−$\Phi$((t−$\xi$)/$\sigma$)|≦ϵ$\backslash$sup{\_}t|F(t) - $\backslash$Phi((t - $\backslash$xi)/$\backslash$sigma)| $\backslash$leqq $\backslash$epsilon, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if ϵ$\backslash$epsilon is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for $\xi$$\backslash$xi but not for $\sigma$$\backslash$sigma); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n→∞n $\backslash$rightarrow $\backslash$infty; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of FF). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression ∑i(xi−T)2$\backslash$sum{\_}i (x{\_}i - T){\^{}}2; this is of course achieved by the sample mean T=∑ixi/nT = $\backslash$sum{\_}i x{\_}i/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T=Tn(x1,⋯,xn)minimizes∑i$\rho$(xi−T),T = T{\_}n(x{\_}1, $\backslash$cdots, x{\_}n) minimizes $\backslash$sum{\_}i $\backslash$rho(x{\_}i - T), where$\rho$isanon−constantfunction.(M)$\backslash$begin{\{}equation*{\}} $\backslash$tag{\{}M{\}} where $\backslash$rho is a non-constant function. $\backslash$end{\{}equation*{\}} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean ($\rho$(t)=t2)($\backslash$rho(t) = t{\^{}}2), (ii) the sample median ($\rho$(t)=|t|)($\backslash$rho(t) = |t|), and more generally, (iii) all maximum likelihood estimators ($\rho$(t)=−logf(t)($\backslash$rho(t) = -$\backslash$log f(t), where ff is the assumed density of the untranslated distribution). These (MM)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x)=Tn(x1,⋯,xn)T{\_}n(x) = T{\_}n(x{\_}1, $\backslash$cdots, x{\_}n)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n→∞)(n $\backslash$rightarrow $\backslash$infty) when FF ranges over some suitable set of underlying distributions, in particular over the set of all F=(1−ϵ)$\Phi$+ϵHF = (1 - $\backslash$epsilon)$\backslash$Phi + $\backslash$epsilon H for fixed ϵ$\backslash$epsilon and symmetric HH. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of nn it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of HH, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (MM)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following $\rho$:$\rho$(t)=12t2$\backslash$rho:$\backslash$rho(t) = $\backslash$frac{\{}1{\}}{\{}2{\}}t{\^{}}2 for |t|{\textless}k,$\rho$(t)=k|t|−12k2|t| {\textless} k, $\backslash$rho(t) = k|t| - $\backslash$frac{\{}1{\}}{\{}2{\}}k{\^{}}2 for |t|≧k|t| $\backslash$geqq k, with kk depending on ϵ$\backslash$epsilon. This estimator is most robust even among all translation invariant estimators. Sample mean (k=∞)(k = $\backslash$infty) and sample median (k=0)(k = 0) are limiting cases corresponding to ϵ=0$\backslash$epsilon = 0 and ϵ=1$\backslash$epsilon = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1≦x2≦⋯≦xnx{\_}1 $\backslash$leqq x{\_}2 $\backslash$leqq $\backslash$cdots $\backslash$leqq x{\_}n, then the statistic T=n−1(gxg+1+xg+1+xg+2+⋯+xn−h+hxn−h)T = n{\^{}}{\{}-1{\}}(gx{\_}{\{}g + 1{\}} + x{\_}{\{}g + 1{\}} + x{\_}{\{}g + 2{\}} + $\backslash$cdots + x{\_}{\{}n - h{\}} + hx{\_}{\{}n - h{\}}) is called the Winsorized mean, obtained by Winsorizing the gg leftmost and the hh rightmost observations. The above most robust (MM)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg+1x{\_}{\{}g + 1{\}} and xn−hx{\_}{\{}n - h{\}} have to be replaced by some numbers u,vu, v satisfying xg≦u≦xg+1x{\_}g $\backslash$leqq u $\backslash$leqq x{\_}{\{}g + 1{\}} and xn−h≦v≦xn−h+1x{\_}{\{}n - h{\}} $\backslash$leqq v $\backslash$leqq x{\_}{\{}n - h + 1{\}}, respectively; g,h,ug, h, u and vv depend on the sample. In fact, this (MM)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0F{\_}0 with density f0(t)=(1−ϵ)(2$\pi$)−12e−$\rho$(t)f{\_}0(t) = (1 - $\backslash$epsilon)(2$\backslash$pi){\^{}}{\{}-$\backslash$frac{\{}1{\}}{\{}2{\}}{\}}e{\^{}}{\{}-$\backslash$rho(t){\}}. This f0f{\_}0 behaves like a normal density for small tt, like an exponential density for large tt. At least for me, this was rather surprising--I would have expected an f0f{\_}0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that FF belongs to some convex set CC of distribution functions. Then the most robust (MM)-estimator for the set CC coincides with the maximum likelihood estimator for the unique F0$\epsilon$CF{\_}0 $\backslash$varepsilon C which has the smallest Fisher information number I(F)=∫(f′/f)2fdtI(F) = $\backslash$int (f'/f){\^{}}2f dt among all F$\epsilon$CF $\backslash$varepsilon C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy supt|F(t)−$\Phi$(t)|≦ϵ$\backslash$sup{\_}t|F(t) - $\backslash$Phi(t)| $\backslash$leqq $\backslash$epsilon; robust estimation of a scale parameter; how to estimate location, if scale and ϵ$\backslash$epsilon are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing ∑i{\textless}j$\rho$(xi−T,xj−T)$\backslash$sum{\_}{\{}i {\textless} j{\}} $\backslash$rho(x{\_}i - T, x{\_}j - T), where $\rho$$\backslash$rho is a function of two arguments. Questions of small sample size theory will not be touched in this paper.},
author = {Huber, Peter J.},
doi = {10.1214/aoms/1177703732},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
title = {{Robust Estimation of a Location Parameter}},
year = {1964}
}
@article{Hubert2005,
abstract = {We introduce a new method for robust principal component analysis (PCA). Classical PCA is based on the empirical covariance matrix of the data and hence is highly sensitive to outlying observations. Two robust approaches have been developed to date. The first approach is based on the eigenvectors of a robust scatter matrix such as the minimum covariance determinant or an S-estimator and is limited to relatively low-dimensional data. The second approach is based on projection pursuit and can handle high-dimensional data. Here we propose the ROBPCA approach, which combines projection pursuit ideas with robust scatter matrix estimation. ROBPCA yields more accurate estimates at noncontaminated datasets and more robust estimates at contaminated data. ROBPCA can be computed rapidly, and is able to detect exact-fit situations. As a by-product, ROBPCA produces a diagnostic plot that displays and classifies the outliers. We apply the algorithm to several datasets from chemometrics and engineering.},
author = {Hubert, Mia and Rousseeuw, Peter J. and {Vanden Branden}, Karlien},
doi = {10.1198/004017004000000563},
issn = {00401706},
journal = {Technometrics},
keywords = {High-dimensional data,Principal component analysis,Projection pursuit,Robust methods},
title = {{ROBPCA: A new approach to robust principal component analysis}},
year = {2005}
}
@incollection{Rousseeuw1985,
abstract = {The succinic acid producer Mannheimia succiniciproducens can efficiently utilize sucrose as a carbon source, but its metabolism has not been understood. This study revealed that M. succiniciproducens uses a sucrose phosphotransferase system (PTS), sucrose 6-phosphate hydrolase, and a fructose PTS for the transport and utilization of sucrose.},
author = {Rousseeuw, Peter},
booktitle = {Mathematical Statistics and Applications},
doi = {10.1007/978-94-009-5438-0_20},
title = {{Multivariate Estimation with High Breakdown Point}},
year = {1985}
}
@article{Gentle1991,
abstract = {Un livre general sur le clustering. 2005},
author = {Gentle, J. E. and Kaufman, L. and Rousseuw, P. J.},
doi = {10.2307/2532178},
issn = {0006341X},
journal = {Biometrics},
title = {{Finding Groups in Data: An Introduction to Cluster Analysis.}},
year = {1991}
}
@inproceedings{Okamoto2008,
abstract = {Closeness centrality is an important concept in social network analysis. In a graph representing a social network, closeness centrality measures how close a vertex is to all other vertices in the graph. In this paper, we combine existing methods on calculating exact values and approximate values of closeness centrality and present new algorithms to rank the top-k vertices with the highest closeness centrality. We show that under certain conditions, our algorithm is more efficient than the algorithm that calculates the closeness-centralities of all vertices. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Okamoto, Kazuya and Chen, Wei and Li, Xiang Yang},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-69311-6_21},
isbn = {3540693106},
issn = {03029743},
title = {{Ranking of closeness centrality for large-scale social networks}},
year = {2008}
}
@article{Rousseeuw1984,
abstract = {Classical least squares regression consists of minimizing the sum of the squared residuals. Many authors have pro-duced more robust versions of this estimator by replacing the square by something else, such as the absolute value. In this article a different approach is introduced in which the sum is replaced by the median of the squared resid-uals. The resulting estimator can resist the effect of nearly 50{\%} of contamination in the data. In the special case of simple regression, it corresponds to finding the narrowest strip covering half of the observations. Generalizations are possible to multivariate location, orthogonal regres-sion, and hypothesis testing in linear models.},
author = {Rousseeuw, Peter J.},
doi = {10.2307/2288718},
issn = {01621459},
journal = {Journal of the American Statistical Association},
title = {{Least Median of Squares Regression}},
year = {1984}
}
@article{Har-Peled2007,
author = {Har-Peled, Sariel and Kushal, Akash},
doi = {https://doi.org/10.1007/s00454-006-1271-x},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Har-Peled, Kushal - 2007 - Smaller Coresets for k -Median and k -Means Clustering.pdf:pdf},
journal = {Discrete Comput Geom},
pages = {3--19},
title = {{Smaller Coresets for k -Median and k -Means Clustering}},
volume = {37},
year = {2007}
}
@article{Chandrasekaran1989,
abstract = {The Fermat-Weber location problem is to find a point in ℝn that minimizes the sum of the weighted Euclidean distances from m given points in ℝn. A popular iterative solution method for this problem was first introduced by Weiszfeld in 1937. In 1973 Kuhn claimed that if the m given points are not collinear then for all but a denumerable number of starting points the sequence of iterates generated by Weiszfeld's scheme converges to the unique optimal solution. We demonstrate that Kuhn's convergence theorem is not always correct. We then conjecture that if this algorithm is initiated at the affine subspace spanned by the m given points, the convergence is ensured for all but a denumerable number of starting points. {\textcopyright} 1989 The Mathematical Programming Society, Inc.},
author = {Chandrasekaran, R. and Tamir, A.},
doi = {10.1007/BF01587094},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Location theory,The Fermat-Weber location problem,Weiszfeld's iterative algorithm},
title = {{Open questions concerning Weiszfeld's algorithm for the Fermat-Weber location problem}},
year = {1989}
}
@article{Newling2017,
abstract = {We present a new algorithm trimed for obtaining the medoid of a set, that is the element of the set which minimises the mean distance to all other elements. The algorithm is shown to have, under certain assumptions, expected run time O(N (Formula presented.) ) in ℝd where N is the set size, making it the first sub-quadratic exact medoid algorithm for d {\textgreater} 1. Experiments show that it performs very well on spatial network data, frequently requiring two orders of magnitude fewer distance calculations than state-of-the-art approximate algorithms. As an application, we show how trimed can be used as a component in an accelerated K-medoids algorithm, and then how it can be relaxed to obtain further computational gains with only a minor loss in cluster quality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1605.06950v4},
author = {Newling, James and Fleuret, Fran{\c{c}}ois},
eprint = {arXiv:1605.06950v4},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Newling, Fleuret - 2017 - A sub-quadratic exact medoid algorithm.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
title = {{A sub-quadratic exact medoid algorithm}},
volume = {54},
year = {2017}
}
@article{Chin2011,
abstract = {We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems. these problems are motivated by the lasso framework and have applications in machine learning and computer vision. Our work shows a close connection between these problems and core questions in algorithmic graph theory. While this connection demonstrates the difficulties of obtaining runtime guarantees, it also suggests an approach of using techniques originally developed for graph algorithms. We then show that most of these problems can be formulated as a grouped least squares problem, and give efficient algorithms for this formulation. Our algorithms rely on routines for solving quadratic minimization problems, which in turn are equivalent to solving linear systems. Finally we present some experimental results on applying our approximation algorithm to image processing problems.},
archivePrefix = {arXiv},
arxivId = {1110.1358},
author = {Chin, Hui Han and Madry, Aleksander and Miller, Gary and Peng, Richard},
doi = {10.1145/2422436.2422469},
eprint = {1110.1358},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Chin et al. - 2011 - Runtime Guarantees for Regression Problems.pdf:pdf},
journal = {ITCS 2013 - 2013 ACM Conference on Innovations in Theoretical Computer Science},
title = {{Runtime Guarantees for Regression Problems}},
url = {http://arxiv.org/abs/1110.1358},
year = {2011}
}
@article{Parrilo2001,
abstract = {We compare algorithms for global optimization of polynomial functions in many variables. It is demonstrated that existing algebraic methods (Gr$\backslash$"obner bases, resultants, homotopy methods) are dramatically outperformed by a relaxation technique, due to N.Z. Shor and the first author, which involves sums of squares and semidefinite programming. This opens up the possibility of using semidefinite programming relaxations arising from the Positivstellensatz for a wide range of computational problems in real algebraic geometry. This paper was presented at the Workshop on Algorithmic and Quantitative Aspects of Real Algebraic Geometry in Mathematics and Computer Science, held at DIMACS, Rutgers University, March 12-16, 2001.},
archivePrefix = {arXiv},
arxivId = {math/0103170},
author = {Parrilo, Pablo and Sturmfels, Bernd},
doi = {10.1090/dimacs/060/08},
eprint = {0103170},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Parrilo, Sturmfels - 2001 - Minimizing polynomial functions.pdf:pdf},
journal = {DIMACS Workshop on Algorithmic and Quantitative Aspects of Real Algebraic Geometry in Mathematics and Computer Science},
keywords = {and phrases,authors was made possible,global optimization,gr,inite programming,obner bases,polynomials,positivstellensatz,real algebra,semidef-,sums of squares,the center,the collaboration between the,through a grant from},
pages = {83--100},
primaryClass = {math},
title = {{Minimizing polynomial functions}},
volume = {0000},
year = {2001}
}
@article{Feldman2011,
abstract = {Given a set F of n positive functions over a ground set X, we consider the problem of computing x* that minimizes the expression $\Sigma$ f ∈ Ff(x), over x ∈ X. A typical application is shape fitting, where we wish to approximate a set P of n elements (say, points) by a shape x from a (possibly infinite) family X of shapes. Here, each point p ∈ P corresponds to a function f such that f(x) is the distance from p to x, and we seek a shape x that minimizes the sum of distances from each point in P. In the k-clustering variant, each x$\backslash$in X is a tuple of k shapes, and f(x) is the distance from p to its closest shape in x. Our main result is a unified framework for constructing coresets and approximate clustering for such general sets of functions. To achieve our results, we forge a link between the classic and well defined notion of $\epsilon$-approximations from the theory of PAC Learning and VC dimension, to the relatively new (and not so consistent) paradigm of coresets, which are some kind of "compressed representation" of the input set F. Using traditional techniques, a coreset usually implies an LTAS (linear time approximation scheme) for the corresponding optimization problem, which can be computed in parallel, via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model). For several function families F for which coresets are known not to exist, or the corresponding (approximate) optimization problems are hard, our framework yields bicriteria approximations, or coresets that are large, but contained in a low-dimensional space. We demonstrate our unified framework by applying it on projective clustering problems. We obtain new coreset constructions and significantly smaller coresets, over the ones that appeared in the literature during the past years, for problems such as: k-Median [Har-Peled and Mazumdar,STOC'04], [Chen, SODA'06], [Langberg and Schulman, SODA'10]; k-Line median [Feldman, Fiat and Sharir, FOCS'06], [Deshpande and Varadarajan, STOC'07]; Projective clustering [Deshpande et al., SODA'06] [Deshpande and Varadarajan, STOC'07]; Linear l p regression [Clarkson, Woodruff, STOC'09 ]; Low-rank approximation [Sarlos, FOCS'06]; Subspace approximation [Shyamalkumar and Varadarajan, SODA'07], [Feldman, Monemizadeh, Sohler and Woodruff, SODA'10], [Deshpande, Tulsiani, and Vishnoi, SODA'11]. The running times of the corresponding optimization problems are also significantly improved. We show how to generalize the results of our framework for squared distances (as in k-mean), distances to the qth power, and deterministic constructions. {\textcopyright} 2011 ACM.},
archivePrefix = {arXiv},
arxivId = {1106.1379},
author = {Feldman, Dan and Langberg, Michael},
doi = {10.1145/1993636.1993712},
eprint = {1106.1379},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Feldman, Langberg - 2011 - A unified framework for approximating and clustering data.pdf:pdf},
isbn = {9781450306911},
issn = {07378017},
journal = {Proceedings of the Symposium on Theory of Computing},
keywords = {approximating,clustering,coresets,cur,epsilon-approximation,epsilon-nets,k-means,k-median,pac-learning,pca,regression,svd},
pages = {569--578},
title = {{A unified framework for approximating and clustering data}},
year = {2011}
}
@phdthesis{IndykPiotrStanfordUniversity2000,
author = {{Indyk Piotr}},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Indyk Piotr - 2001 - High-Dimensional Computational Geometry.pdf:pdf},
number = {September},
school = {Stanford University},
title = {{High-Dimensional Computational Geometry}},
year = {2001}
}
@article{Badoiu2002,
abstract = {In this paper, we show that for several clustering problems one can extract a small set of points, so that using those core-sets enable us to perform approximate clustering efficiently. The surprising property of those core-sets is that their size is independent of the dimension. Using those, we present a (1 + $\epsilon$)-approximation algorithms for the k-center clustering and k-median clustering problems in Euclidean space. The running time of the new algorithms has linear or near linear dependency on the number of points and the dimension, and exponential dependency on 1/$\epsilon$ and k. As such, our results are a substantial improvement over what was previously known. We also present some other clustering results including (1 + $\epsilon$)-approximate 1-cylinder clustering, and k-center clustering with outliers.},
author = {Bǎdoiu, Mihai and Har-Peled, Sariel and Indyk, Piotr},
doi = {10.1145/509943.509947},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Bǎdoiu, Har-Peled, Indyk - 2002 - Approximate clustering via core-sets.pdf:pdf},
issn = {07349025},
journal = {Annual ACM Symposium on Theory of Computing},
pages = {250--257},
title = {{Approximate clustering via core-sets}},
year = {2002}
}
@article{Bao2012,
abstract = {In this paper, we address the error correction problem, that is, to uncover the low-dimensional subspace structure from high-dimensional observations, which are possibly corrupted by errors. When the errors are of Gaussian distribution, principal component analysis (PCA) can find the optimal (in terms of least-square error) low-rank approximation to high-dimensional data. However, the canonical PCA method is known to be extremely fragile to the presence of gross corruptions. Recently, Wright established a so-called robust principal component analysis (RPCA) method, which can well handle the grossly corrupted data. However, RPCA is a transductive method and does not handle well the new samples, which are not involved in the training procedure. Given a new datum, RPCA essentially needs to recalculate over all the data, resulting in high computational cost. So, RPCA is inappropriate for the applications that require fast online computation. To overcome this limitation, in this paper, we propose an inductive robust principal component analysis (IRPCA) method. Given a set of training data, unlike RPCA that targets on recovering the original data matrix, IRPCA aims at learning the underlying projection matrix, which can be used to efficiently remove the possible corruptions in any datum. The learning is done by solving a nuclear-norm regularized minimization problem, which is convex and can be solved in polynomial time. Extensive experiments on a benchmark human face dataset and two video surveillance datasets show that IRPCA cannot only be robust to gross corruptions, but also handle the new data well and in an efficient way. {\textcopyright} 1992-2012 IEEE.},
archivePrefix = {arXiv},
arxivId = {arXiv:0912.3599v1},
author = {Bao, Bing Kun and Liu, Guangcan and Xu, Changsheng and Yan, Shuicheng},
doi = {10.1109/TIP.2012.2192742},
eprint = {arXiv:0912.3599v1},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Bao et al. - 2012 - Inductive robust principal component analysis.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error correction,robust principal component analysis (PCA),subspace learning},
number = {8},
pages = {3794--3800},
title = {{Inductive robust principal component analysis}},
volume = {21},
year = {2012}
}
@article{Akhtar2018,
abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
archivePrefix = {arXiv},
arxivId = {1801.00553},
author = {Akhtar, Naveed and Mian, Ajmal},
doi = {10.1109/ACCESS.2018.2807385},
eprint = {1801.00553},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Akhtar, Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in Computer Vision A Survey.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep learning,adversarial learning,adversarial perturbation,black-box attack,perturbation detection,white-box attack},
pages = {14410--14430},
publisher = {IEEE},
title = {{Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey}},
volume = {6},
year = {2018}
}
@article{Hammond2011,
abstract = {We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian L. Given a wavelet generating kernel g and a scale parameter t, we define the scaled wavelet operator Tgt=g(tL). The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on g, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing L. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains. {\textcopyright} 2010 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {0912.3848},
author = {Hammond, David K. and Vandergheynst, Pierre and Gribonval, R{\'{e}}mi},
doi = {10.1016/j.acha.2010.04.005},
eprint = {0912.3848},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Hammond, Vandergheynst, Gribonval - 2011 - Wavelets on graphs via spectral graph theory.pdf:pdf},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {Graph theory,Overcomplete wavelet frames,Spectral graph theory,Wavelets},
number = {2},
pages = {129--150},
title = {{Wavelets on graphs via spectral graph theory}},
volume = {30},
year = {2011}
}
@article{Zuegner2020,
author = {Z{\"{u}}gner, Daniel and G{\"{u}}nnemann, Stephan},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Z{\"{u}}gner, G{\"{u}}nnemann - 2020 - Certifiable Robustness of Graph Convolutional Networks under Structure Perturbations.pdf:pdf},
journal = {International Conference on Knowledge Discovery and Data Mining, KDD},
pages = {1656--1665},
title = {{Certifiable Robustness of Graph Convolutional Networks under Structure Perturbations}},
year = {2020}
}
@article{Gao2017,
abstract = {In this paper, we utilize results from convex analysis and monotone operator theory to derive additional properties of the softmax function that have not yet been covered in the existing literature. In particular, we show that the softmax function is the monotone gradient map of the log-sum-exp function. By exploiting this connection, we show that the inverse temperature parameter determines the Lipschitz and co-coercivity properties of the softmax function. We then demonstrate the usefulness of these properties through an application in game-theoretic reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1704.00805},
author = {Gao, Bolin and Pavel, Lacra},
eprint = {1704.00805},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Gao, Pavel - 2017 - On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning(2).pdf:pdf},
number = {August},
title = {{On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.00805},
year = {2017}
}
@article{Neumann2018,
abstract = {As machine learning moves from the lab into the real world, reliability is often of paramount importance. The clearest example are safety-critical applications such as pedestrian detection in autonomous driving. Since algorithms can never be expected to be perfect in all cases, managing reliability becomes crucial. To this end, in this paper we investigate the problem of learning in an end-to-end manner object detectors that are accurate while providing an unbiased estimate of the reliablity of their own predictions. We do so by proposing a modification of the standard softmax layer where a probabilistic confidence score is explicitly pre-multiplied into the incoming activations to modulate confidence. We adopt a rigorous assessment protocol based on reliability diagrams to evaluate the quality of the resulting calibration and show excellent results in pedestrian detection on two challenging public benchmarks.},
author = {Neumann, Lukas and Vedaldi, Andrea and Zisserman, Andrew},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Neumann, Vedaldi, Zisserman - 2018 - Relaxed Softmax Efficient Confidence Auto-Calibration for Safe Pedestrian Detection.pdf:pdf},
journal = {NeurIPS Workshop},
pages = {1--10},
title = {{Relaxed Softmax : Efficient Confidence Auto-Calibration for Safe Pedestrian Detection}},
year = {2018}
}
@article{Description2008,
abstract = {The prelims comprise:$\backslash$n$\backslash$n$\backslash$n* $\backslash$nShort Description of the Method$\backslash$n$\backslash$n$\backslash$n* $\backslash$nHow to Use the Program PAM$\backslash$n$\backslash$n$\backslash$n* $\backslash$nExamples$\backslash$n$\backslash$n$\backslash$n* $\backslash$nMore on the Algorithm and the Program$\backslash$n$\backslash$n$\backslash$n* $\backslash$nRelated Methods and References$\backslash$n$\backslash$n},
author = {Description, Short and The, O F},
doi = {10.1002/9780470316801.ch2},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Description, The - 2008 - Partitioning Around Medoids (Program PAM).pdf:pdf},
pages = {68--125},
title = {{Partitioning Around Medoids (Program PAM)}},
year = {2008}
}
@article{Chen2017,
author = {Chen, Yuxin},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Chen - 2017 - Robust Principal Component Analysis Disentangling sparse and low-rank matrices.pdf:pdf},
title = {{Robust Principal Component Analysis Disentangling sparse and low-rank matrices}},
year = {2017}
}
@article{VictoriaFeser2012,
abstract = {Robust statistics, as a concept, probably dates back to the prehistory of statistics. It has, however, been formalized in the sixties by the pioneering work of Huber and Hampel. Robust statistics is an extension of classical statistics, which takes into account the fact that models assumed to have generated the data at hand are only approximate. It provides tools to investigate the robustness properties of a statistic T (such as estimators, test statistics) as well as robust estimators and robust testing procedures.},
author = {Victoria-Feser, Maria-Pia},
doi = {10.2139/ssrn.1762749},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Victoria-Feser - 2012 - Robust Statistics for Multivariate Methods.pdf:pdf},
journal = {SSRN Electronic Journal},
number = {February},
title = {{Robust Statistics for Multivariate Methods}},
year = {2012}
}
@article{Bojchevski2020,
author = {Bojchevski, Aleksandar and Klicpera, Johannes and G{\"{u}}nnemann, Stephan},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Anonymous et al. - 2020 - Efficient Robustness Certificates for Graph Neural Networks via Sparsity-Aware Randomized Smoothing (see URL f.pdf:pdf},
journal = {37th International Conference on Machine Learning, ICML},
title = {{Efficient Robustness Certificates for Graph Neural Networks via Sparsity-Aware Randomized Smoothing}},
url = {https://figshare.com/s/51cd5b404863eaf763d9},
year = {2020}
}
@article{Croux2012,
abstract = {The L1-median is a robust estimator of multivariate location with good statistical properties. Several algorithms for computing the L1-median are available. Problem specific algorithms can be used, but also general optimization routines. The aim is to compare different algorithms with respect to their precision and runtime. This is possible because all considered algorithms have been implemented in a standardized manner in the open source environment R. In most situations, the algorithm based on the optimization routine NLM (non-linear minimization) clearly outperforms other approaches. Its low computation time makes applications for large and high-dimensional data feasible.},
author = {Croux, Christophe and Filzmoser, Peter and Fritz, Heinrich},
doi = {10.2139/ssrn.1690502},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Croux, Filzmoser, Fritz - 2012 - A Comparison of Algorithms for the Multivariate L1-Median.pdf:pdf},
journal = {SSRN Electronic Journal},
title = {{A Comparison of Algorithms for the Multivariate L1-Median}},
year = {2012}
}
@article{Townsend2016,
abstract = {1 The low rank case Let A be an m × n matrix of rank k ≤ min(m, n). Then we may decompose A as A = USV , where U is m × k, S is k × k diagonal, V is n × k and the matrices U and V satisfy the relation U U = V V = I k. (1) In this case the differential of A may be expressed as dA = dUSV + UdSV + USdV. (2) The constraint (1) implies that the diffentials dU and dV are also constrained: focussing on U for a moment, taking the differential of (1) gives dU U + U dU = 0. (3) So the matrix dΩ U = U dU is skew-symmetric. In fact, if we fix an m × (m − k) matrix U ⊥ such that U U ⊥ is an orthogonal matrix (this could be computed using the Gram-Schmidt process) then we may expand dU as dU = UdΩ U + U ⊥ dK U (4) where dK U is an unconstrained (m − k) × k matrix. Similarly we may expand dV as dV = VdΩ V + V ⊥ dK V (5) where dΩ V = V dV is k × k skew-symmetric and dK V is an (n − k) × k matrix. See [1] for more detail. Left-multiplying (2) by U and right-multiplying by V gives U dAV = dΩ U S + dS + SdΩ V. (6) Since dΩ U and dΩ V are skew-symmetric, they have zero diagonal and thus the products dΩ U S and and SdΩ V must also have zero diagonal. This means that we can split (6) into two components as follows. Letting dP := U dAV and using • to denote the Hadamard product, the diagonal component of (6) is dS = I k • dP (7) 1},
author = {Townsend, James},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Townsend - 2016 - Differentiating the Singular Value Decomposition.pdf:pdf},
number = {6},
title = {{Differentiating the Singular Value Decomposition}},
volume = {1},
year = {2016}
}
@article{Mortensen2001,
abstract = {It is not uncommon to find literacy figured as "toxic" in discussions of its power to regulate and discipline social behavior. The author's aim in this article is to move from metaphor to material as he explores the toxicity inherent in the manufacturing processes that make print available for mass consumption. He argues that over the past century, the demand for print in certain regions of the United States, primarily the North and West, spurred the growth of commercial papermaking-and the spread of devastating mill pollution-in the South, where demand for print has historically lagged. He suggests that one result of this pollution has been the weakening of social institutions that typically promote and value normative forms of literate activity. With the industries that enable the mass circulation of print now going global, this pattern of uneven and unjust literacy development may well be repeated. {\textcopyright} 2001 Sage Publications.},
author = {Mortensen, Peter},
doi = {10.1177/0741088301018004001},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Mortensen - 2001 - Reading material(2).pdf:pdf},
issn = {15528472},
journal = {Written Communication},
number = {4},
pages = {395--439},
title = {{Reading material}},
volume = {18},
year = {2001}
}
@article{Analytics2019,
author = {Analytics, Data},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Analytics - 2019 - Roadmap.pdf:pdf},
title = {{Roadmap}},
year = {2019}
}
@article{Mortensen2001a,
abstract = {It is not uncommon to find literacy figured as "toxic" in discussions of its power to regulate and discipline social behavior. The author's aim in this article is to move from metaphor to material as he explores the toxicity inherent in the manufacturing processes that make print available for mass consumption. He argues that over the past century, the demand for print in certain regions of the United States, primarily the North and West, spurred the growth of commercial papermaking-and the spread of devastating mill pollution-in the South, where demand for print has historically lagged. He suggests that one result of this pollution has been the weakening of social institutions that typically promote and value normative forms of literate activity. With the industries that enable the mass circulation of print now going global, this pattern of uneven and unjust literacy development may well be repeated. {\textcopyright} 2001 Sage Publications.},
author = {Mortensen, Peter},
doi = {10.1177/0741088301018004001},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Mortensen - 2001 - Reading material.pdf:pdf},
issn = {15528472},
journal = {Written Communication},
number = {4},
pages = {395--439},
title = {{Reading material}},
volume = {18},
year = {2001}
}
@article{Boyarshinov1997,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Boyarshinov, Victor},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Boyarshinov - 1997 - Machine Learning Machine Learning.pdf:pdf},
isbn = {0071154671},
journal = {Computer},
number = {April},
pages = {414},
title = {{Machine Learning Machine Learning}},
url = {https://books.google.ca/books?id=EoYBngEACAAJ{\&}dq=mitchell+machine+learning+1997{\&}hl=en{\&}sa=X{\&}ved=0ahUKEwiomdqfj8TkAhWGslkKHRCbAtoQ6AEIKjAA},
volume = {2005},
year = {1997}
}
@article{Buchnik2018a,
abstract = {Graph-based semi-supervised learning (SSL) algorithms predict labels for all nodes based on provided labels of a small set of seed nodes. Classic methods capture the graph structure through some underlying diffusion process that propagates through the graph edges. Spectral diffusion, which includes personalized page rank and label propagation, propagates through random walks. Social diffusion propagates through shortest paths. These diffusions are linear in the sense of not distinguishing between contributions of few “strong” relations or many “weak” relations. Recent methods such as node embeddings and graph convolutional networks (GCN) attained significant gains in quality for SSL tasks. These methods vary on how the graph structure, seed label information, and other features are used, but do share a common thread of nonlinearity that suppresses weak relations and re-enforces stronger ones. Aiming for quality gain with more scalable methods, we revisit classic linear diffusion methods and place them in a self-training framework. The resulting bootstrapped diffusions are nonlinear in that they re-enforce stronger relations, as with the more complex methods. Surprisingly, we observe that SSL with bootstrapped diffusions not only significantly improves over the respective non-bootstrapped baselines but also outperform state-of-the-art SSL methods. Moreover, since the self-training wrapper retains the scalability of the base method, we obtain both higher quality and better scalability.},
archivePrefix = {arXiv},
arxivId = {1703.02618},
author = {Buchnik, Eliav and Cohen, Edith},
doi = {10.1145/3219617.3219621},
eprint = {1703.02618},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Buchnik, Cohen - 2018 - Bootstrapped graph diffusions Exposing the power of nonlinearity(2).pdf:pdf},
isbn = {9781450358460},
journal = {SIGMETRICS 2018 - Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems},
keywords = {Bootstrapping,Graph-based semi-supervised learning,Label propagation},
number = {1},
pages = {8--10},
title = {{Bootstrapped graph diffusions: Exposing the power of nonlinearity}},
volume = {2},
year = {2018}
}
@article{Carlini2017a,
abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1145/3128572.3140444},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Carlini, Wagner - 2017 - Adversarial examples are not easily detected Bypassing ten detection methods.pdf:pdf},
isbn = {9781450352024},
journal = {ACM Workshop on Artificial Intelligence and Security, AISec 2017},
pages = {3--14},
title = {{Adversarial examples are not easily detected: Bypassing ten detection methods}},
year = {2017}
}
@article{Zhu2019,
abstract = {Graph Convolutional Networks (GCNs) are an emerging type of neural network model on graphs which have achieved state-ofthe-art performance in the task of node classification. However, recent studies show that GCNs are vulnerable to adversarial attacks, i.e. small deliberate perturbations in graph structures and node attributes, which poses great challenges for applying GCNs to real world applications. How to enhance the robustness of GCNs remains a critical open problem. To address this problem, we propose Robust GCN (RGCN), a novel model that “fortifies” GCNs against adversarial attacks. Specifically, instead of representing nodes as vectors, our method adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer. In this way, when the graph is attacked, our model can automatically absorb the effects of adversarial changes in the variances of the Gaussian distributions. Moreover, to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances when performing convolutions. Extensive experimental results demonstrate that our proposed method can effectively improve the robustness of GCNs. On three benchmark graphs, our RGCN consistently shows a substantial gain in node classification accuracy compared with state-of-the-art GCNs against various adversarial attack strategies.},
author = {Zhu, Dingyuan and Cui, Peng and Zhang, Ziwei and Zhu, Wenwu},
doi = {10.1145/3292500.3330851},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2019 - Robust graph convolutional networks against adversarial attacks.pdf:pdf},
isbn = {9781450362016},
journal = {International Conference on Knowledge Discovery and Data Mining, KDD},
keywords = {Adversarial Attacks,Deep Learning,Graph Convolutional Networks,Robustness},
pages = {1399--1407},
title = {{Robust graph convolutional networks against adversarial attacks}},
year = {2019}
}
@article{Biggio2018,
abstract = {Deep neural networks and machine-learning algorithms are pervasively used in several applications, ranging from computer vision to computer security. In most of these applications, the learning algorithm has to face intelligent and adaptive attackers who can carefully manipulate data to purposely subvert the learning process. As these algorithms have not been originally designed under such premises, they have been shown to be vulnerable to well-crafted, sophisticated attacks, including training-time poisoning and test-time evasion attacks (also known as adversarial examples). The problem of countering these threats and learning secure classifiers in adversarial settings has thus become the subject of an emerging, relevant research field known as adversarial machine learning. The purposes of this tutorial are: (a) to introduce the fundamentals of adversarial machine learning to the security community; (b) to illustrate the design cycle of a learning-based pattern recognition system for adversarial tasks; (c) to present novel techniques that have been recently proposed to assess performance of pattern classifiers and deep learning algorithms under attack, evaluate their vulnerabilities, and implement defense strategies that make learning algorithms more robust to attacks; and (d) to show some applications of adversarial machine learning to pattern recognition tasks like object recognition in images, biometric identity recognition, spam and malware detection.},
author = {Biggio, Battista and Roli, Fabio},
doi = {10.1145/3243734.3264418},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Biggio, Roli - 2018 - Wild patterns Ten years after the rise of adversarial machine learning half-day tutorial.pdf:pdf},
isbn = {9781450356930},
issn = {15437221},
journal = {ACM Conference on Computer and Communications Security},
keywords = {Adversarial examples,Adversarial machine learning,Deep learning,Evasion attacks,Training data poisoning},
pages = {2154--2156},
title = {{Wild patterns: Ten years after the rise of adversarial machine learning half-day tutorial}},
year = {2018}
}
@article{Entezari2020,
abstract = {Recent studies have demonstrated that machine learning approaches like deep learning methods are easily fooled by adversarial attacks. Recently, a highly-influential study examined the impact of adversarial attacks on graph data and demonstrated that graph embedding techniques are also vulnerable to adversarial attacks. Fake users on social media and fake product reviews are examples of perturbations in graph data that are realistic counterparts of the adversarial models proposed. Graphs are widely used in a variety of domains and it is highly important to develop graph analysis techniques that are robust to adversarial attacks. One of the recent studies on generating adversarial attacks for graph data is Nettack. The Nettack model has shown to be very successful in deceiving the Graph Convolutional Network (GCN) model. Nettack is also transferable to other node classification approaches e.g. node embeddings. In this paper, we explore the properties of Nettack perturbations, in search for effective defenses against them. Our first finding is that Nettack demonstrates a very specific behavior in the spectrum of the graph: only high-rank (low-valued) singular components of the graph are affected. Following that insight, we show that a low-rank approximation of the graph, that uses only the top singular components for its reconstruction, can greatly reduce the effects of Nettack and boost the performance of GCN when facing adversarial attacks. Indicatively, on the CiteSeer dataset, our proposed defense mechanism is able to reduce the success rate of Nettack from 98{\%} to 36{\%}. Furthermore, we show that tensor-based node embeddings, which by default project the graph into a low-rank subspace, are robust against Nettack perturbations. Lastly, we propose LowBlow, a low-rank adversarial attack which is able to affect the classification performance of both GCN and tensor-based node embeddings and we show that the low-rank attack is noticeable and making it unnoticeable results in a high-rank attack.},
author = {Entezari, Negin and Al-Sayouri, Saba A. and Darvishzadeh, Amirali and Papalexakis, Evangelos E.},
doi = {10.1145/3336191.3371789},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Entezari et al. - 2020 - All you need is Low (rank) Defending against adversarial attacks on graphs.pdf:pdf},
isbn = {9781450368223},
journal = {International Conference on Web Search and Data Mining, WSDM},
keywords = {Adversarial machine learning,Graph convolutional networks,Graph mining,Graph representation learning,Tensors},
pages = {169--177},
title = {{All you need is Low (rank): Defending against adversarial attacks on graphs}},
year = {2020}
}
@article{Cheng2019,
abstract = {We study the fundamental problem of high-dimensional mean estimation in a robust model where a constant fraction of the samples are adversarially corrupted. Recent work gave the first polynomial time algorithms for this problem with dimension-independent error guarantees for several families of structured distributions. In this work, we give the first nearly-linear time algorithms for high-dimensional robust mean estimation. Specifically, we focus on distributions with (i) known covariance and sub-gaussian tails, and (ii) unknown bounded covariance. Given N samples on Rd, an -fraction of which may be arbitrarily corrupted, our algorithms run in time Oe(Nd)/poly() and approximate the true mean within the information-theoretically optimal error, up to constant factors. Previous robust algorithms with comparable error guarantees have running times $\Omega$(eNd2), for = $\Omega$(1). Our algorithms rely on a natural family of SDPs parameterized by our current guess $\nu$ for the unknown mean µ?. We give a win-win analysis establishing the following: either a near-optimal solution to the primal SDP yields a good candidate for µ?— independent of our current guess $\nu$ — or a near-optimal solution to the dual SDP yields a new guess $\nu$0whose distance from µ?is smaller by a constant factor. We exploit the special structure of the corresponding SDPs to show that they are approximately solvable in nearly-linear time. Our approach is quite general, and we believe it can also be applied to obtain nearly-linear time algorithms for other high-dimensional robust learning problems.},
author = {Cheng, Yu and Diakonikolas, Ilias and Ge, Rong},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Cheng, Diakonikolas, Ge - 2019 - High-dimensional robust mean estimation in nearly-linear time.pdf:pdf},
journal = {Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {2755--2771},
title = {{High-dimensional robust mean estimation in nearly-linear time}},
year = {2019}
}
@article{Zhang2019a,
abstract = {Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classification and matrix completion. Although the performance has been impressive, the current implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other edges may be missing between nodes that have very strong relationships. In this paper we adopt a Bayesian approach, viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the joint posterior of the random graph parameters and the node (or graph) labels. We present the Bayesian GCNN framework and develop an iterative learning procedure for the case of assortative mixed-membership stochastic block models. We present the results of experiments that demonstrate that the Bayesian formulation can provide better performance when there are very few labels available during the training process.},
archivePrefix = {arXiv},
arxivId = {1811.11103},
author = {Zhang, Yingxue and Pal, Soumyasundar and Coates, Mark and Ustebay, Deniz},
doi = {10.1609/aaai.v33i01.33015829},
eprint = {1811.11103},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Bayesian Graph Convolutional Neural Networks for Semi-Supervised Classification.pdf:pdf},
issn = {2159-5399},
journal = {AAAI Conference on Artificial Intelligence},
pages = {5829--5836},
title = {{Bayesian Graph Convolutional Neural Networks for Semi-Supervised Classification}},
volume = {33},
year = {2019}
}
@article{Morris2019,
abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
archivePrefix = {arXiv},
arxivId = {1810.02244},
author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
doi = {10.1609/aaai.v33i01.33014602},
eprint = {1810.02244},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Morris et al. - 2019 - Weisfeiler and Leman Go Neural Higher-Order Graph Neural Networks.pdf:pdf},
issn = {2159-5399},
journal = {AAAI Conference on Artificial Intelligence},
pages = {4602--4609},
title = {{Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks}},
volume = {33},
year = {2019}
}
@article{EngstromEtAl2019,
abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread ex- istence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
author = {{Engstrom, Logan and Gilmer, Justin and Goh, Gabriel and Hendrycks, Dan and Ilyas, Andrew and Madry, Aleksander and Nakano, Reiichiro and Nakkiran, Preetum and Santurkar, Shibani and Tran, Brandon and Tsipras, Dimitris and Wallace}, Eric},
doi = {10.23915/distill.00019},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Engstrom, Logan and Gilmer, Justin and Goh, Gabriel and Hendrycks, Dan and Ilyas, Andrew and Madry, Aleksander and Nakano, Reiichiro a.pdf:pdf},
journal = {Distill},
number = {NeurIPS},
title = {{Adversarial Examples Are Not Bugs, They Are Features – gradient science}},
url = {https://gradientscience.org/adv/},
year = {2019}
}
@article{Cuturi2019,
abstract = {Sorting an array is a fundamental routine in machine learning, one that is used to compute rank-based statistics, cumulative distribution functions (CDFs), quantiles, or to select closest neighbors and labels. The sorting function is however piece-wise constant (the sorting permutation of a vector does not change if the entries of that vector are infinitesimally perturbed) and therefore has no gradient information to back-propagate. We propose a framework to sort elements that is algorithmically differentiable. We leverage the fact that sorting can be seen as a particular instance of the optimal transport (OT) problem on {\$}\backslashmathbb{\{}R{\}}{\$}, from input values to a predefined array of sorted values (e.g. {\$}1,2,\backslashdots,n{\$} if the input array has {\$}n{\$} elements). Building upon this link , we propose generalized CDFs and quantile operators by varying the size and weights of the target presorted array. Because this amounts to using the so-called Kantorovich formulation of OT, we call these quantities K-sorts, K-CDFs and K-quantiles. We recover differentiable algorithms by adding to the OT problem an entropic regularization, and approximate it using a few Sinkhorn iterations. We call these operators S-sorts, S-CDFs and S-quantiles, and use them in various learning settings: we benchmark them against the recently proposed neuralsort [Grover et al. 2019], propose applications to quantile regression and introduce differentiable formulations of the top-k accuracy that deliver state-of-the art performance.},
archivePrefix = {arXiv},
arxivId = {1905.11885},
author = {Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe},
eprint = {1905.11885},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Cuturi, Teboul, Vert - 2019 - Differentiable Ranks and Sorting using Optimal Transport.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
title = {{Differentiable Ranks and Sorting using Optimal Transport}},
url = {http://arxiv.org/abs/1905.11885},
year = {2019}
}
@article{Rabanser2018,
abstract = {We might hope that when faced with unexpected inputs, well-designed software systems would fire off warnings. Machine learning (ML) systems, however, which depend strongly on properties of their inputs (e.g. the i.i.d. assumption), tend to fail silently. This paper explores the problem of building ML systems that fail loudly, investigating methods for detecting dataset shift, identifying exemplars that most typify the shift, and quantifying shift malignancy. We focus on several datasets and various perturbations to both covariates and label distributions with varying magnitudes and fractions of data affected. Interestingly, we show that across the dataset shifts that we explore, a two-sample-testing-based approach, using pre-trained classifiers for dimensionality reduction, performs best. Moreover, we demonstrate that domain-discriminating approaches tend to be helpful for characterizing shifts qualitatively and determining if they are harmful.},
archivePrefix = {arXiv},
arxivId = {1810.11953},
author = {Rabanser, Stephan and G{\"{u}}nnemann, Stephan and Lipton, Zachary C.},
eprint = {1810.11953},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Rabanser, G{\"{u}}nnemann, Lipton - 2018 - Failing Loudly An Empirical Study of Methods for Detecting Dataset Shift.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
title = {{Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift}},
url = {http://arxiv.org/abs/1810.11953},
year = {2019}
}
@article{Maron2019,
abstract = {Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressive power of graph neural networks (GNN). It was shown that the popular message passing GNN cannot distinguish between graphs that are indistinguishable by the 1-WL test (Morris et al. 2018; Xu et al. 2019). Unfortunately, many simple instances of graphs are indistinguishable by the 1-WL test. In search for more expressive graph learning models we build upon the recent k-order invariant and equivariant graph neural networks (Maron et al. 2019a,b) and present two results: First, we show that such k-order networks can distinguish between non-isomorphic graphs as good as the k-WL tests, which are provably stronger than the 1-WL test for k{\textgreater}2. This makes these models strictly stronger than message passing models. Unfortunately, the higher expressiveness of these models comes with a computational cost of processing high order tensors. Second, setting our goal at building a provably stronger, simple and scalable model we show that a reduced 2-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication) has a provable 3-WL expressive power. Differently put, we suggest a simple model that interleaves applications of standard Multilayer-Perceptron (MLP) applied to the feature dimension and matrix multiplication. We validate this model by presenting state of the art results on popular graph classification and regression tasks. To the best of our knowledge, this is the first practical invariant/equivariant model with guaranteed 3-WL expressiveness, strictly stronger than message passing models.},
archivePrefix = {arXiv},
arxivId = {1905.11136},
author = {Maron, Haggai and Ben-Hamu, Heli and Serviansky, Hadar and Lipman, Yaron},
eprint = {1905.11136},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Maron et al. - 2019 - Provably Powerful Graph Networks.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
title = {{Provably Powerful Graph Networks}},
url = {http://arxiv.org/abs/1905.11136},
year = {2019}
}
@article{Li2018,
abstract = {The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defensive models has been considered, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.},
archivePrefix = {arXiv},
arxivId = {1809.03113},
author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
eprint = {1809.03113},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - Certified Adversarial Robustness with Additive Noise.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
title = {{Certified Adversarial Robustness with Additive Noise}},
url = {http://arxiv.org/abs/1809.03113},
year = {2018}
}
@article{Small1990,
abstract = {In this paper we survey a sequence of papers whose primary aim is the generalization of the concept of the median into higher dimensional settings. While a variety of distinct definitions of the median of a multivariate data set are possible these definitions have the common property of producing the usual definition when applied to univariate data or a univariate distribution. Some common ideas of equivariance, symmetry and breakdown are discussed as well as computational convenience for each definition. The extension of these ideas to directional statistics is also discussed. /// L'auteur revoit une s{\'{e}}rie d'articles qui g{\'{e}}n{\'{e}}ralisent de plusieurs mani{\`{e}}res le concept de m{\'{e}}diane dans les espaces multidimensionnels. Ces diff{\'{e}}rentes mesures reproduisent la definition usuelle de la m{\'{e}}diane dans le cas de loi unidimensionnelle. Il examine en outre les concepts d'{\'{e}}quivariance, de symm{\'{e}}trie, de rupture ainsi que la difficult{\'{e}} de calcul de la m{\'{e}}diane pour chaque definition. Enfin, il g{\'{e}}n{\'{e}}ralise ces id{\'{e}}es pour les lois directionnelles.},
author = {Small, Christopher G.},
doi = {10.2307/1403809},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Small - 1990 - A Survey of Multidimensional Medians.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
keywords = {1 background and history,a natural robu,affine,as a nonparametric and,breakdown,data set arises chiefly,directional statistics,equivariance,estimate for the center,in nonparametric problems as,invariance,m,median,of a distribution,robust estimate for,the median of a},
number = {3},
pages = {263},
title = {{A Survey of Multidimensional Medians}},
volume = {58},
year = {1990}
}
@article{Zugner2019a,
abstract = {Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.},
archivePrefix = {arXiv},
arxivId = {1902.08412},
author = {Z{\"{u}}gner, Daniel and G{\"{u}}nnemann, Stephan},
eprint = {1902.08412},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Z{\"{u}}gner, G{\"{u}}nnemann - 2019 - Adversarial attacks on graph neural networks via meta learning.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR},
pages = {1--15},
title = {{Adversarial attacks on graph neural networks via meta learning}},
year = {2019}
}
@article{Hudaib2018,
abstract = {Clustering is the process of grouping a set of patterns into different disjoint clusters where each cluster contains the alike patterns. Many algorithms had been proposed before for clustering. K-medoid is a variant of k-mean that use an actual point in the cluster to represent it instead of the mean in the k-mean algorithm to get the outliers and reduce noise in the cluster. In order to enhance performance of k-medoid algorithm and get more accurate clusters, a hybrid algorithm is proposed which use CRO algorithm along with k-medoid. In this method, CRO is used to expand searching for the optimal medoid and enhance clustering by getting more precise results. The performance of the new algorithm is evaluated by comparing its results with five clustering algorithms, k-mean, k-medoid, DB/rand/1/bin, CRO based clustering algorithm and hybrid CRO-k-mean by using four real world datasets: Lung cancer, Iris, Breast cancer Wisconsin and Haberman's survival from UCI machine learning data repository. The results were conducted and compared base on different metrics and show that proposed algorithm enhanced clustering technique by giving more accurate results.},
author = {Hudaib, Amjad and Khanafseh, Mohammad and Surakhi, Ola},
doi = {10.5539/mas.v12n2p116},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Hudaib, Khanafseh, Surakhi - 2018 - An Improved Version of K-medoid Algorithm using CRO.pdf:pdf},
issn = {1913-1844},
journal = {Modern Applied Science},
keywords = {chemical reaction optimization,clustering,k-mean,k-medoid},
number = {2},
pages = {116},
title = {{An Improved Version of K-medoid Algorithm using CRO}},
volume = {12},
year = {2018}
}
@article{Lopuhaa1991,
abstract = {Finiote-sample replacement breakdown points are derived...},
author = {Lopuha{\"{a}}, Hendrik and Rousseeuw, Peter},
doi = {10.1214/aos/1176347978},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Lopuha{\"{a}}, Rousseeuw - 1991 - Breakdown Points of Affine Equivariant Estimators of Multivariate Location and Covariance Matrices.pdf:pdf},
journal = {Annals of Statistics},
title = {{Breakdown Points of Affine Equivariant Estimators of Multivariate Location and Covariance Matrices}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {19},
year = {1991}
}
@article{Teitz1968,
abstract = {The generalized vertex median of a weighted graph may be found by complete enumeration or by some heuristic method. This paper investigates alternatives and proposes a method that seems to perform well in comparison with others found in the literature.},
author = {Teitz, Michael B. and Bart, Polly},
doi = {10.1287/opre.16.5.955},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Teitz, Bart - 1968 - Heuristic Methods for Estimating the Generalized Vertex Median of a Weighted Graph.pdf:pdf},
issn = {0030-364X},
journal = {Operations Research},
number = {5},
pages = {955--961},
title = {{Heuristic Methods for Estimating the Generalized Vertex Median of a Weighted Graph}},
volume = {16},
year = {1968}
}
@article{Hubert2008,
abstract = {When applying a statistical method in practice it often occurs that some observations deviate from the usual assumptions. However, many classical methods are sensitive to outliers. The goal of robust statistics is to develop methods that are robust against the possibility that one or several unannounced outliers may occur anywhere in the data. These methods then allow to detect outlying observations by their residuals from a robust fit. We focus on high-breakdown methods, which can deal with a substantial fraction of outliers in the data. We give an overview of recent high-breakdown robust methods for multivariate settings such as covariance estimation, multiple and multivariate regression, discriminant analysis, principal components and multivariate calibration. {\textcopyright} Institute of Mathematical Statistics, 2008.},
author = {Hubert, Mia and Rousseeuw, Peter J. and {Van Aelst}, Stefan},
doi = {10.1214/088342307000000087},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Hubert, Rousseeuw, Van Aelst - 2008 - High-breakdown robust multivariate methods.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Breakdown value,Influence function,Multivariate statistics,Outliers,Partial least squares,Principal components,Regression,Robustness},
number = {1},
pages = {92--119},
title = {{High-breakdown robust multivariate methods}},
volume = {23},
year = {2008}
}
@article{Vertrag2015,
author = {Vertrag, Schriftlicher},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Vertrag - 2015 - Info. 12.pdf:pdf},
title = {{Info. 12}},
year = {2015}
}
@article{Vardi2000,
author = {Vardi, Yehuda and Zhang, Cun-hui},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Vardi, Zhang - 2000 - The multivariate L 1 -median and associated data depth.pdf:pdf},
number = {4},
pages = {1423--1426},
title = {{The multivariate L 1 -median and associated data depth}},
volume = {97},
year = {2000}
}
@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - 2015 - Explaining and harnessing adversarial examples.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR},
pages = {1--11},
title = {{Explaining and harnessing adversarial examples}},
year = {2015}
}
@article{Diakonikolas2019,
abstract = {We study high-dimensional distribution learning in an agnostic setting where an adversary is allowed to arbitrarily corrupt an ϵ-fraction of the samples. Such questions have a rich history spanning statistics, machine learning, and theoretical computer science. Even in the most basic settings, the only known approaches are either computat ionally inefficient or lose dimensiondependent factors in their error guarantees. This raises the following question: Is high-dimensional agnostic distribution learning even possible, algorithmically? In this work, we obtain the first computationally efficient algorithms with dimension-independent error guarantees for agnostically learning several fundamental classes of high-dimensional distributions: (1) a single Gaussian, (2) a product distribution on the hypercube, (3) mixtures of two product distributions (under a natural balancedness condition), and (4) mixtures of spherical Gaussians. Our a lgorithms achieve error that is independent of the dimension, and in many cases scales nearly linearly with the fraction of adversarially corrupted samples. Moreover, we develop a general recipe for detecting and correcting corruptions in high-dimensions that may be applicable to many other problems.},
archivePrefix = {arXiv},
arxivId = {1604.06443},
author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
doi = {10.1137/17M1126680},
eprint = {1604.06443},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas et al. - 2019 - Robust estimators in high-dimensions without the computational intractability.pdf:pdf},
issn = {10957111},
journal = {SIAM Journal on Computing},
keywords = {Gaussian distribution,High-dimensions,Mixture models,Product distributions,Robust learning},
number = {2},
pages = {742--864},
title = {{Robust estimators in high-dimensions without the computational intractability}},
volume = {48},
year = {2019}
}
@article{Cohen2016,
abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural general-ization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups gen-erated by translations, reflections and rotations. G-CNNs achieve state of the art results on CI- FAR10 and rotated MNIST.},
archivePrefix = {arXiv},
arxivId = {1602.07576},
author = {Cohen, Taco S. and Welling, Max},
eprint = {1602.07576},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Welling - 2016 - Group equivariant convolutional networks.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML},
keywords = {()},
pages = {4375--4386},
title = {{Group equivariant convolutional networks}},
volume = {6},
year = {2016}
}
@article{Kipf2017,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N. and Welling, Max},
eprint = {1609.02907},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Kipf, Welling - 2017 - Semi-supervised classification with graph convolutional networks.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR},
pages = {1--14},
title = {{Semi-supervised classification with graph convolutional networks}},
year = {2017}
}
@article{Cohen2016a,
abstract = {In this paper we provide faster algorithms for solving the geometric median problem: given n points in R compute a point that minimizes the sum of Euclidean distances to the points. This is one of the oldest non-trivial problems in computational geometry yet despite a long history of research the previous fastest running times for computing a (1 + ϵ)-approximate geometric median were O(d{\textperiodcentered}n4/3ϵ-8/3) by Chin et. al, {\~{O}}(d exp ϵ-4 log ϵ-1) by Badoiu et. al, O(nd+ poly(d, ϵ-1)) by Feldman and Langberg, and the polynomial running time of O((nd)O(1) log1/ϵ) by Parrilo and Sturmfels and Xue and Ye. In this paper we show how to compute such an approximate geometric median in time O(nd log3n/ϵ) and O(dϵ-2). While our O(dϵ-2) is a fairly straightforward application of stochastic subgradient descent, our O(nd log3n/ϵ) time algorithm is a novel long step interior point method. We start with a simple O((nd)O(1)log 1/ϵ) time interior point method and show how to improve it, ultimately building an algorithm that is quite non-standard from the perspective of interior point literature. Our result is one of few cases of outperforming standard interior point theory. Furthermore, it is the only case we know of where interior point methods yield a nearly linear time algorithm for a canonical optimization problem that traditionally requires superlinear time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.05225v1},
author = {Cohen, Michael B. and Lee, Yin Tat and Miller, Gary and Pachocki, Jakub and Sidford, Aaron},
doi = {10.1145/2897518.2897647},
eprint = {arXiv:1606.05225v1},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Cohen et al. - 2016 - Geometric median in nearly linear time.pdf:pdf},
isbn = {9781450341325},
issn = {07378017},
journal = {Annual ACM Symposium on Theory of Computing},
keywords = {Geometric median,Interior point methods,Stochastic gradient descent},
number = {1},
pages = {9--21},
title = {{Geometric median in nearly linear time}},
volume = {19-21-June},
year = {2016}
}
@article{Carlini2017,
abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95{\%} to 0.5{\%}.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100{\%} probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1608.04644},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1109/SP.2017.49},
eprint = {1608.04644},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Carlini, Wagner - 2017 - Towards Evaluating the Robustness of Neural Networks.pdf:pdf},
isbn = {9781509055326},
issn = {10816011},
journal = {IEEE Symposium on Security and Privacy},
pages = {39--57},
title = {{Towards Evaluating the Robustness of Neural Networks}},
year = {2017}
}
@article{Kipf2016,
abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1611.07308},
author = {Kipf, Thomas N. and Welling, Max},
eprint = {1611.07308},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Kipf, Welling - 2016 - Variational Graph Auto-Encoders.pdf:pdf},
number = {2},
pages = {1--3},
title = {{Variational Graph Auto-Encoders}},
url = {http://arxiv.org/abs/1611.07308},
year = {2016}
}
@article{Diakonikolas2017,
abstract = {Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00893v4},
author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M. and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
eprint = {arXiv:1703.00893v4},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas et al. - 2017 - Being robust (in high dimensions) can be practical.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML},
pages = {1659--1689},
title = {{Being robust (in high dimensions) can be practical}},
volume = {3},
year = {2017}
}
@article{Boudt2020,
abstract = {The minimum covariance determinant (MCD) approach estimates the location and scatter matrix using the subset of given size with lowest sample covariance determinant. Its main drawback is that it cannot be applied when the dimension exceeds the subset size. We propose the minimum regularized covariance determinant (MRCD) approach, which differs from the MCD in that the scatter matrix is a convex combination of a target matrix and the sample covariance matrix of the subset. A data-driven procedure sets the weight of the target matrix, so that the regularization is only used when needed. The MRCD estimator is defined in any dimension, is well-conditioned by construction and preserves the good robustness properties of the MCD. We prove that so-called concentration steps can be performed to reduce the MRCD objective function, and we exploit this fact to construct a fast algorithm. We verify the accuracy and robustness of the MRCD estimator in a simulation study and illustrate its practical use for outlier detection and regression analysis on real-life high-dimensional data sets in chemistry and criminology.},
archivePrefix = {arXiv},
arxivId = {1701.07086},
author = {Boudt, Kris and Rousseeuw, Peter J. and Vanduffel, Steven and Verdonck, Tim},
doi = {10.1007/s11222-019-09869-x},
eprint = {1701.07086},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Boudt et al. - 2020 - The minimum regularized covariance determinant estimator.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Breakdown value,High-dimensional data,Regularization,Robust covariance estimation},
number = {1},
pages = {113--128},
title = {{The minimum regularized covariance determinant estimator}},
volume = {30},
year = {2020}
}
@article{Diehl2019,
abstract = {Graph Neural Networks (GNNs) research has concentrated on improving convolutional layers, with little attention paid to developing graph pooling layers. Yet pooling layers can enable GNNs to reason over abstracted groups of nodes instead of single nodes, thus increasing their generalization potential. To close this gap, we propose a graph pooling layer relying on the notion of edge contraction: EdgePool learns a localized and sparse pooling transform. We evaluate it on four datasets, finding that it increases performance on the three largest. We also show that EdgePool can be integrated in existing GNN architectures without adding any additional losses or regularization.},
author = {Diehl, Frederik and Brunner, Thomas and Le, Michael Truong and Knoll, Alois},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Diehl et al. - 2019 - Towards Graph Pooling by Edge Contraction.pdf:pdf},
journal = {ICML 2019 Workshop: Learning and Reasoning with Graph-Structured Representations},
number = {2018},
title = {{Towards Graph Pooling by Edge Contraction}},
year = {2019}
}
@article{Gilmer2017,
abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
archivePrefix = {arXiv},
arxivId = {1704.01212},
author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
eprint = {1704.01212},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Gilmer et al. - 2017 - Neural message passing for quantum chemistry.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML},
pages = {2053--2070},
title = {{Neural message passing for quantum chemistry}},
volume = {3},
year = {2017}
}
@article{Buchnik2018,
abstract = {Graph-based semi-supervised learning (SSL) algorithms predict labels for all nodes based on provided labels of a small set of seed nodes. Classic methods capture the graph structure through some underlying diffusion process that propagates through the graph edges. Spectral diffusion, which includes personalized page rank and label propagation, propagates through random walks. Social diffusion propagates through shortest paths. These diffusions are linear in the sense of not distinguishing between contributions of few “strong” relations or many “weak” relations. Recent methods such as node embeddings and graph convolutional networks (GCN) attained significant gains in quality for SSL tasks. These methods vary on how the graph structure, seed label information, and other features are used, but do share a common thread of nonlinearity that suppresses weak relations and re-enforces stronger ones. Aiming for quality gain with more scalable methods, we revisit classic linear diffusion methods and place them in a self-training framework. The resulting bootstrapped diffusions are nonlinear in that they re-enforce stronger relations, as with the more complex methods. Surprisingly, we observe that SSL with bootstrapped diffusions not only significantly improves over the respective non-bootstrapped baselines but also outperform state-of-the-art SSL methods. Moreover, since the self-training wrapper retains the scalability of the base method, we obtain both higher quality and better scalability.},
archivePrefix = {arXiv},
arxivId = {1703.02618},
author = {Buchnik, Eliav and Cohen, Edith},
doi = {10.1145/3219617.3219621},
eprint = {1703.02618},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Buchnik, Cohen - 2018 - Bootstrapped graph diffusions Exposing the power of nonlinearity.pdf:pdf},
isbn = {9781450358460},
journal = {SIGMETRICS 2018 - Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems},
keywords = {Bootstrapping,Graph-based semi-supervised learning,Label propagation},
pages = {8--10},
title = {{Bootstrapped graph diffusions: Exposing the power of nonlinearity}},
year = {2018}
}
@article{Madry2018,
abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
archivePrefix = {arXiv},
arxivId = {1706.06083},
author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
eprint = {1706.06083},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Madry et al. - 2018 - Towards deep learning models resistant to adversarial attacks.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR},
pages = {1--28},
title = {{Towards deep learning models resistant to adversarial attacks}},
year = {2018}
}
@article{Velickovic2018,
abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
archivePrefix = {arXiv},
arxivId = {1710.10903},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Casanova, Arantxa and Li{\`{o}}, Pietro and Cucurull, Guillem and Romero, Adriana and Bengio, Yoshua},
eprint = {1710.10903},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Veli{\v{c}}kovi{\'{c}} et al. - 2018 - Graph attention networks.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR},
pages = {1--12},
title = {{Graph attention networks}},
year = {2018}
}
@article{Yuan2019,
abstract = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed.},
archivePrefix = {arXiv},
arxivId = {1712.07107},
author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
doi = {10.1109/TNNLS.2018.2886017},
eprint = {1712.07107},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Yuan et al. - 2019 - Adversarial Examples Attacks and Defenses for Deep Learning.pdf:pdf},
issn = {21622388},
journal = {IEEE transactions on neural networks and learning systems},
number = {9},
pages = {2805--2824},
title = {{Adversarial Examples: Attacks and Defenses for Deep Learning}},
volume = {30},
year = {2019}
}
@article{Ax,
author = {Ax, All Vectors},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Ax - Unknown - Problem Sets Related to Lectures and Readings The Column Space of A Contains.pdf:pdf},
pages = {1--36},
title = {{Problem Sets Related to Lectures and Readings The Column Space of A Contains}}
}
@article{Weng2018,
abstract = {The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide a theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the 2 and ∞ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifier.},
archivePrefix = {arXiv},
arxivId = {1801.10578},
author = {Weng, Tsui Wei and Zhang, Huan and Chen, Pin Yu and Yi, Jinfeng and Su, Dong and Gao, Yupeng and Hsieh, Cho Jui and Daniel, Luca},
eprint = {1801.10578},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Weng et al. - 2018 - Evaluating the robustness of neural networks An extreme value theory approach.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR},
pages = {1--18},
title = {{Evaluating the robustness of neural networks: An extreme value theory approach}},
year = {2018}
}
@article{Pan2018,
abstract = {Graph embedding is an effective method to represent graph data in a low dimensional space for graph analytics. Most existing embedding algorithms typically focus on preserving the topological structure or minimizing the reconstruction errors of graph data, but they have mostly ignored the data distribution of the latent codes from the graphs, which often results in inferior embedding in real-world graph data. In this paper, we propose a novel adversarial graph embedding framework for graph data. The framework encodes the topological structure and node content in a graph to a compact representation, on which a decoder is trained to reconstruct the graph structure. Furthermore, the latent representation is enforced to match a prior distribution via an adversarial training scheme. To learn a robust embedding, two variants of adversarial approaches, adversarially regularized graph autoencoder (ARGA) and adversarially regularized variational graph autoencoder (ARVGA), are developed. Experimental studies on real-world graphs validate our design and demonstrate that our algorithms outperform baselines by a wide margin in link prediction, graph clustering, and graph visualization tasks.},
archivePrefix = {arXiv},
arxivId = {1802.04407},
author = {Pan, Shirui and Hu, Ruiqi and Long, Guodong and Jiang, Jing and Yao, Lina and Zhang, Chengqi},
doi = {10.24963/ijcai.2018/362},
eprint = {1802.04407},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Pan et al. - 2018 - Adversarially regularized graph autoencoder for graph embedding.pdf:pdf},
isbn = {9780999241127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {2609--2615},
title = {{Adversarially regularized graph autoencoder for graph embedding}},
volume = {2018-July},
year = {2018}
}
@article{AbuElHaija2019,
abstract = {Graph Convolutional Networks (GCNs) have shown significant improvements in semi-supervised learning on graph-structured data. Concurrently, unsupervised learning of graph embeddings has benefited from the information contained in random walks. In this paper, we propose a model: Network of GCNs (NGCN), which marries these two lines of work. At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective. Our experiments show that our proposed N-GCN model improves state-of-the-art baselines on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.1},
archivePrefix = {arXiv},
arxivId = {1802.08888},
author = {Abu-El-Haija, Sami and Kapoor, Amol and Perozzi, Bryan and Lee, Joonseok},
eprint = {1802.08888},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Abu-El-Haija et al. - 2019 - N-GCN Multi-scale graph convolution for semi-supervised node classification.pdf:pdf},
journal = {35th Conference on Uncertainty in Artificial Intelligence, UAI 2019},
keywords = {convolution,deep,graph,semi-supervised learning,spectral},
title = {{N-GCN: Multi-scale graph convolution for semi-supervised node classification}},
year = {2019}
}
@article{Ren2018,
abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
archivePrefix = {arXiv},
arxivId = {1803.09050},
author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
eprint = {1803.09050},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Ren et al. - 2018 - Learning to reweight examples for robust deep learning.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML},
pages = {6900--6909},
title = {{Learning to reweight examples for robust deep learning}},
volume = {10},
year = {2018}
}
@article{Diakonikolas2019a,
abstract = {In high dimensions, most machine learning methods arc brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, SEVER, possesses strong theoretical guarantees yet is also highly scalable—beyond running the base learner itself, it only requires computing the top singular vector of a certain n x d matrix. We apply SEVER on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines.},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.02815v2},
author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M. and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
eprint = {arXiv:1803.02815v2},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas et al. - 2019 - Sever A robust meta-aigorithm for stochastic optimization.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
pages = {2867--2898},
title = {{Sever: A robust meta-aigorithm for stochastic optimization}},
volume = {2019-June},
year = {2019}
}
@article{Monti2018,
abstract = {In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.},
archivePrefix = {arXiv},
arxivId = {1806.00770},
author = {Monti, Federico and Shchur, Oleksandr and Bojchevski, Aleksandar and Litany, Or and G{\"{u}}nnemann, Stephan and Bronstein, Michael M.},
eprint = {1806.00770},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Monti et al. - 2018 - Dual-Primal Graph Convolutional Networks.pdf:pdf},
pages = {1--11},
title = {{Dual-Primal Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1806.00770},
year = {2018}
}
@article{Zugner2018,
abstract = {Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.},
archivePrefix = {arXiv},
arxivId = {1805.07984},
author = {Z{\"{u}}gner, Daniel and Akbarnejad, Amir and G{\"{u}}nnemann, Stephan},
doi = {10.1145/3219819.3220078},
eprint = {1805.07984},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Z{\"{u}}gner, Akbarnejad, G{\"{u}}nnemann - 2018 - Adversarial attacks on neural networks for graph data.pdf:pdf},
isbn = {9781450355520},
journal = {International Conference on Knowledge Discovery and Data Mining, KDD},
keywords = {Adversarial machine learning,Graph convolutional networks,Graph mining,Network mining,Semi-supervised learning},
pages = {2847--2856},
title = {{Adversarial attacks on neural networks for graph data}},
year = {2018}
}
@article{Xu2018,
abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture-jumping knowledge (JK) networks - that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.},
archivePrefix = {arXiv},
arxivId = {1806.03536},
author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken Ichi and Jegelka, Stefanie},
eprint = {1806.03536},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2018 - Representation learning on graphs with jumping knowledge networks.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML},
pages = {8676--8685},
title = {{Representation learning on graphs with jumping knowledge networks}},
volume = {12},
year = {2018}
}
@article{Dai2018,
abstract = {Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool deep learning models by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. We further propose attack methods based on genetic algorithms and gradient descent in the scenario where additional prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.},
archivePrefix = {arXiv},
arxivId = {1806.02371},
author = {Dai, Hanjun and Li, Hui and Tian, Tian and Xin, Huang and Wang, Lin and Jun, Zhu and Le, Song},
eprint = {1806.02371},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Dai et al. - 2018 - Adversarial attack on graph structured data.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML},
pages = {1799--1808},
title = {{Adversarial attack on graph structured data}},
volume = {3},
year = {2018}
}
@article{Bojchevski2019,
abstract = {The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.},
archivePrefix = {arXiv},
arxivId = {1809.01093},
author = {Bojchevski, Aleksandar and G{\"{u}}nnemann, Stephan},
eprint = {1809.01093},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Bojchevski, G{\"{u}}nnemann - 2019 - Adversarial attacks on node embeddings via graph poisoning.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML},
pages = {1112--1123},
title = {{Adversarial attacks on node embeddings via graph poisoning}},
volume = {2019-June},
year = {2019}
}
@article{Ying2018,
abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs-a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DIFFPOOL, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DIFFPOOL learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DIFFPOOL yields an average improvement of 5-10{\%} accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1806.08804},
author = {Ying, Rex and Morris, Christopher and Hamilton, William L. and You, Jiaxuan and Ren, Xiang and Leskovec, Jure},
eprint = {1806.08804},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Ying et al. - 2018 - Hierarchical graph representation learning with differentiable pooling.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {4800--4810},
title = {{Hierarchical graph representation learning with differentiable pooling}},
volume = {2018-Decem},
year = {2018}
}
@article{Gao2019,
abstract = {Robust estimation under Huber's -contamination model has become an important topic in statistics and theoretical computer science. Statistically optimal procedures such as Tukey's median and other estimators based on depth functions are impractical because of their computational intractability. In this paper, we establish an intriguing connection between f-GANs and various depth functions through the lens of f-Learning. Similar to the derivation of f-GANs, we show that these depth functions that lead to statistically optimal robust estimators can all be viewed as variational lower bounds of the total variation distance in the framework of f-Learning. This connection opens the door of computing robust estimators using tools developed for training GANs. In particular, we show in both theory and experiments that some appropriate structures of discriminator networks with hidden layers in GANs lead to statistically optimal robust location estimators for both Gaussian distribution and general elliptical distributions where first moment may not exist.},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.02030v3},
author = {Gao, Chao and Yao, Yuan and Zhu, Weizhi and Liu, Jiyi},
eprint = {arXiv:1810.02030v3},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Gao et al. - 2019 - Robust estimation and generative adversarial networks.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR},
keywords = {contami-,data depth,gan,minimax rate,nation model,neural networks,robust statistics,tukey median},
number = {1},
pages = {1--38},
title = {{Robust estimation and generative adversarial networks}},
year = {2019}
}
@article{Xu2019,
abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.00826v3},
author = {Xu, Keyulu and Jegelka, Stefanie and Hu, Weihua and Leskovec, Jure},
eprint = {arXiv:1810.00826v3},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2019 - How powerful are graph neural networks.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR},
pages = {1--17},
title = {{How powerful are graph neural networks?}},
year = {2019}
}
@article{Martinez2020,
abstract = {Debugging is an important skill for novice programmers to master, but many students struggle to learn how to debug due in part to difficulty with program syntax. Block-based environments provide an alternative to traditional textual programming that reduces syntax errors, and recently hybrid block-based/textual environments have become more common. This poster presents preliminary research to understand how novice debugging strategies differ between blockbased and hybrid environments. We assigned seven participants to debug four programs within one of the two environments and conducted interviews about their debugging approaches. Thematic analysis of interview responses suggest that students adjusted their strategies based on their prior experience with textual environments. By understanding novice programmers' strategies in these environments, the field can move toward more effectively supporting productive strategies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.10751v3},
author = {Martinez, Phoebe and Lopez, John and Rodriguez, Fernando J. and Wiggins, Joseph B. and Boyer, Kristy Elizabeth},
doi = {10.1145/1122445.1122456},
eprint = {arXiv:1810.10751v3},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Martinez et al. - 2020 - Novice debugging in block-based and hybrid environments.pdf:pdf},
isbn = {9781450367936},
issn = {1942647X},
journal = {Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
keywords = {Block-based programming,Debugging,Hybrid environment},
pages = {1291},
title = {{Fake Node Attacks on Graph Convolutional Networks Xiaoyun}},
year = {2018}
}
@article{Cohen2019,
abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the ℓ2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between ℓ2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49{\%} under adversarial perturbations with ℓ2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified ℓ2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification. Code and models arc available at http://github.com/locuslab/smoothing.},
archivePrefix = {arXiv},
arxivId = {1902.02918},
author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, J. Zico},
eprint = {1902.02918},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Rosenfeld, Kolter - 2019 - Certified adversarial robustness via randomized smoothing.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML},
pages = {2323--2356},
title = {{Certified adversarial robustness via randomized smoothing}},
year = {2019}
}
@article{Klicpera2019,
abstract = {Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.},
archivePrefix = {arXiv},
arxivId = {1810.05997},
author = {Klicpera, Johannes and Bojchevski, Aleksandar and G{\"{u}}nnemann, Stephan},
eprint = {1810.05997},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Klicpera, Bojchevski, G{\"{u}}nnemann - 2019 - Predict then propagate Graph neural networks meet personalized PageRank.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR},
pages = {1--15},
title = {{Predict then propagate: Graph neural networks meet personalized PageRank}},
year = {2019}
}
@article{Li2019,
abstract = {Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7{\%} mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.},
archivePrefix = {arXiv},
arxivId = {1904.03751},
author = {Li, Guohao and Muller, Matthias and Thabet, Ali and Ghanem, Bernard},
doi = {10.1109/ICCV.2019.00936},
eprint = {1904.03751},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2019 - DeepGCNs Can GCNs go as deep as CNNs.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {IEEE International Conference on Computer Vision},
pages = {9266--9275},
title = {{DeepGCNs: Can GCNs go as deep as CNNs?}},
volume = {2019-Octob},
year = {2019}
}
@article{Gao2019a,
abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and un-pooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
archivePrefix = {arXiv},
arxivId = {1905.05178},
author = {Gao, Hongyang and Ji, Shuiwang},
eprint = {1905.05178},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Gao, Ji - 2019 - Graph U-nets.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML},
pages = {3651--3660},
title = {{Graph U-nets}},
volume = {2019-June},
year = {2019}
}
@article{Xu2019a,
abstract = {Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrificing classification accuracy on original graph.},
archivePrefix = {arXiv},
arxivId = {1906.04214},
author = {Xu, Kaidi and Chen, Hongge and Liu, Sijia and Chen, Pin Yu and Weng, Tsui Wei and Hong, Mingyi and Lin, Xue},
doi = {10.24963/ijcai.2019/550},
eprint = {1906.04214},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2019 - Topology attack and defense for graph neural networks An optimization perspective.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {3961--3967},
title = {{Topology attack and defense for graph neural networks: An optimization perspective}},
volume = {2019-Augus},
year = {2019}
}
@article{Lee2019,
abstract = {Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying down-sampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1904.08082v4},
author = {Lee, Junhyun and Lee, Inyeop and Kang, Jaewoo},
eprint = {arXiv:1904.08082v4},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Lee, Kang - 2019 - Self-attention graph pooling.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML},
pages = {6661--6670},
title = {{Self-attention graph pooling}},
volume = {2019-June},
year = {2019}
}
@article{Zhang2019,
abstract = {Traditional set prediction models can struggle with simple datasets due to an issue we call the responsibility problem. We introduce a pooling method for sets of feature vectors based on sorting features across elements of the set. This can be used to construct a permutation-equivariant auto-encoder that avoids this responsibility problem. On a toy dataset of polygons and a set version of MNIST, we show that such an auto-encoder produces considerably better reconstructions and representations. Replacing the pooling function in existing set encoders with FSPool improves accuracy and convergence speed on a variety of datasets.},
archivePrefix = {arXiv},
arxivId = {1906.02795},
author = {Zhang, Yan and Hare, Jonathon and Pr{\"{u}}gel-Bennett, Adam},
eprint = {1906.02795},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Hare, Pr{\"{u}}gel-Bennett - 2019 - FSPool Learning Set Representations with Featurewise Sort Pooling.pdf:pdf},
number = {section 3},
title = {{FSPool: Learning Set Representations with Featurewise Sort Pooling}},
url = {http://arxiv.org/abs/1906.02795},
year = {2019}
}
@article{Rong2019,
abstract = {$\backslash$emph{\{}Over-fitting{\}} and $\backslash$emph{\{}over-smoothing{\}} are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on{\~{}}$\backslash$url{\{}https://github.com/DropEdge/DropEdge{\}}.},
archivePrefix = {arXiv},
arxivId = {1907.10903},
author = {Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
eprint = {1907.10903},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Rong et al. - 2019 - DropEdge Towards Deep Graph Convolutional Networks on Node Classification.pdf:pdf},
number = {2019},
pages = {1--18},
title = {{DropEdge: Towards Deep Graph Convolutional Networks on Node Classification}},
url = {http://arxiv.org/abs/1907.10903},
year = {2019}
}
@article{Lee2019a,
abstract = {Strong theoretical guarantees of robustness can be given for ensembles of classifiers generated by input randomization. Specifically, an {\$}\backslashell{\_}2{\$} bounded adversary cannot alter the ensemble prediction generated by an additive isotropic Gaussian noise, where the radius for the adversary depends on both the variance of the distribution as well as the ensemble margin at the point of interest. We build on and considerably expand this work across broad classes of distributions. In particular, we offer adversarial robustness guarantees and associated algorithms for the discrete case where the adversary is {\$}\backslashell{\_}0{\$} bounded. Moreover, we exemplify how the guarantees can be tightened with specific assumptions about the function class of the classifier such as a decision tree. We empirically illustrate these results with and without functional restrictions across image and molecule datasets.},
archivePrefix = {arXiv},
arxivId = {1906.04948},
author = {Lee, Guang-He and Yuan, Yang and Chang, Shiyu and Jaakkola, Tommi S.},
eprint = {1906.04948},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2019 - Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers.pdf:pdf},
number = {NeurIPS},
pages = {1--19},
title = {{Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers}},
url = {http://arxiv.org/abs/1906.04948},
year = {2019}
}
@article{Zugner2019,
abstract = {Recent works show that Graph Neural Networks (GNNs) are highly non-robust with respect to adversarial attacks on both the graph structure and the node attributes, making their outcomes unreliable. We propose the first method for certifiable (non-)robustness of graph convolutional networks with respect to perturbations of the node attributes1. We consider the case of binary node attributes (e.g. bag-of-words) and perturbations that are L0-bounded. If a node has been certified with our method, it is guaranteed to be robust under any possible perturbation given the attack model. Likewise, we can certify non-robustness. Finally, we propose a robust semi-supervised training procedure that treats the labeled and unlabeled nodes jointly. As shown in our experimental evaluation, our method significantly improves the robustness of the GNN with only minimal effect on the predictive accuracy.},
archivePrefix = {arXiv},
arxivId = {1906.12269},
author = {Z{\"{u}}gner, Daniel and G{\"{u}}nnemann, Stephan},
doi = {10.1145/3292500.3330905},
eprint = {1906.12269},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Z{\"{u}}gner, G{\"{u}}nnemann - 2019 - Certifiable robustness and robust training for graph convolutional networks.pdf:pdf},
isbn = {9781450362016},
journal = {International Conference on Knowledge Discovery and Data Mining, KDD},
pages = {246--256},
title = {{Certifiable robustness and robust training for graph convolutional networks}},
year = {2019}
}
@article{Nguyen2019,
abstract = {Deep neural network (DNN) based salient object detection in images based on high-quality labels is expensive. Alternative unsupervised approaches rely on careful selection of multiple handcrafted saliency methods to generate noisy pseudo-ground-truth labels. In this work, we propose a two-stage mechanism for robust unsupervised object saliency prediction, where the first stage involves refinement of the noisy pseudo labels generated from different handcrafted methods. Each handcrafted method is substituted by a deep network that learns to generate the pseudo labels. These labels are refined incrementally in multiple iterations via our proposed self-supervision technique. In the second stage, the refined labels produced from multiple networks representing multiple saliency methods are used to train the actual saliency detection network. We show that this self-learning procedure outperforms all the existing unsupervised methods over different datasets. Results are even comparable to those of fully-supervised state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {1909.13055},
author = {Nguyen, Duc Tam and Dax, Maximilian and Mummadi, Chaithanya Kumar and Ngo, Thi Phuong Nhung and Nguyen, Thi Hoai Phuong and Lou, Zhongyu and Brox, Thomas},
eprint = {1909.13055},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2019 - DeepUSPS Deep Robust Unsupervised Saliency Prediction With Self-Supervision.pdf:pdf},
number = {NeurIPS},
title = {{DeepUSPS: Deep Robust Unsupervised Saliency Prediction With Self-Supervision}},
url = {http://arxiv.org/abs/1909.13055},
year = {2019}
}
@article{Bojchevski2019a,
abstract = {Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.},
archivePrefix = {arXiv},
arxivId = {1910.14356},
author = {Bojchevski, Aleksandar and G{\"{u}}nnemann, Stephan},
eprint = {1910.14356},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Bojchevski, G{\"{u}}nnemann - 2019 - Certifiable Robustness to Graph Perturbations.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
title = {{Certifiable Robustness to Graph Perturbations}},
url = {http://arxiv.org/abs/1910.14356},
year = {2019}
}
@article{Klicpera2019a,
abstract = {Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.},
archivePrefix = {arXiv},
arxivId = {1911.05485},
author = {Klicpera, Johannes and Wei{\ss}enberger, Stefan and G{\"{u}}nnemann, Stephan},
eprint = {1911.05485},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Klicpera, Wei{\ss}enberger, G{\"{u}}nnemann - 2019 - Diffusion Improves Graph Learning.pdf:pdf},
journal = {Neural Information Processing Systems, NeurIPS},
title = {{Diffusion Improves Graph Learning}},
url = {http://arxiv.org/abs/1911.05485},
year = {2019}
}
@article{Diakonikolas2019b,
abstract = {Learning in the presence of outliers is a fundamental problem in statistics. Until recently, all known efficient unsupervised learning algorithms were very sensitive to outliers in high dimensions. In particular, even for the task of robust mean estimation under natural distributional assumptions, no efficient algorithm was known. Recent work in theoretical computer science gave the first efficient robust estimators for a number of fundamental statistical tasks, including mean and covariance estimation. Since then, there has been a flurry of research activity on algorithmic high-dimensional robust estimation in a range of settings. In this survey article, we introduce the core ideas and algorithmic techniques in the emerging area of algorithmic high-dimensional robust statistics with a focus on robust mean estimation. We also provide an overview of the approaches that have led to computationally efficient robust estimators for a range of broader statistical tasks and discuss new directions and opportunities for future work.},
archivePrefix = {arXiv},
arxivId = {1911.05911},
author = {Diakonikolas, Ilias and Kane, Daniel M.},
eprint = {1911.05911},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Diakonikolas, Kane - 2019 - Recent Advances in Algorithmic High-Dimensional Robust Statistics.pdf:pdf},
journal = {arXiv preprint arXiv:1911.05911},
title = {{Recent Advances in Algorithmic High-Dimensional Robust Statistics}},
url = {http://arxiv.org/abs/1911.05911},
year = {2019}
}
@article{Baluta2020,
abstract = {Verifying security properties of deep neural networks (DNNs) is becoming increasingly important. This paper introduces a new quantitative verification framework for DNNs that can decide, with user-specified confidence, whether a given logical property {\{}$\backslash$psi{\}} defined over the space of inputs of the given DNN holds for less than a user-specified threshold, {\{}$\backslash$theta{\}}. We present new algorithms that are scalable to large real-world models as well as proven to be sound. Our approach requires only black-box access to the models. Further, it certifies properties of both deterministic and non-deterministic DNNs. We implement our approach in a tool called PROVERO. We apply PROVERO to the problem of certifying adversarial robustness. In this context, PROVERO provides an attack-agnostic measure of robustness for a given DNN and a test input. First, we find that this metric has a strong statistical correlation with perturbation bounds reported by 2 of the most prominent white-box attack strategies today. Second, we show that PROVERO can quantitatively certify robustness with high confidence in cases where the state-of-the-art qualitative verification tool (ERAN) fails to produce conclusive results. Thus, quantitative verification scales easily to large DNNs.},
archivePrefix = {arXiv},
arxivId = {2002.06864},
author = {Baluta, Teodora and Chua, Zheng Leong and Meel, Kuldeep S. and Saxena, Prateek},
eprint = {2002.06864},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Baluta et al. - 2020 - Scalable Quantitative Verification For Deep Neural Networks.pdf:pdf},
title = {{Scalable Quantitative Verification For Deep Neural Networks}},
url = {http://arxiv.org/abs/2002.06864},
year = {2020}
}
@book{Goos1999,
author = {Goos, G. and Hartmanis, J. and Leeuwen, J.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Goos, Hartmanis, Leeuwen - 1999 - Lecture Notes in Computer Science.pdf:pdf},
isbn = {3540666664},
issn = {16113349},
pages = {437--445},
title = {{Lecture Notes in Computer Science}},
volume = {1716},
year = {1999}
}
@article{Bahmani2020,
abstract = {We propose an estimator for the mean of random variables in separable real Banach spaces using the empirical characteristic function. Assuming that the covariance operator of the random variable is bounded in a precise sense, we show that the proposed estimator achieves a nearly optimal rate. Furthermore, we show robustness of the estimator against adversarial contamination.},
archivePrefix = {arXiv},
arxivId = {2004.02287},
author = {Bahmani, Sohail},
eprint = {2004.02287},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Bahmani - 2020 - Nearly Optimal Robust Mean Estimation via Empirical Characteristic Function.pdf:pdf},
pages = {1--17},
title = {{Nearly Optimal Robust Mean Estimation via Empirical Characteristic Function}},
url = {http://arxiv.org/abs/2004.02287},
year = {2020}
}
@article{Gal2018,
author = {Gal, Yarin},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Gal - 2018 - Bayesian Deep Learning Pillar I Deep learning.pdf:pdf},
journal = {Slide},
pages = {18},
title = {{Bayesian Deep Learning Pillar I : Deep learning}},
year = {2018}
}
@article{Klicpera2020,
abstract = {Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76{\%} on MD17 and by 31{\%} on QM9. Our implementation is available online.},
archivePrefix = {arXiv},
arxivId = {2003.03123},
author = {Klicpera, Johannes and Gro{\ss}, Janek and G{\"{u}}nnemann, Stephan},
eprint = {2003.03123},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Klicpera, Gro{\ss}, G{\"{u}}nnemann - 2020 - Directional Message Passing for Molecular Graphs.pdf:pdf},
pages = {1--13},
title = {{Directional Message Passing for Molecular Graphs}},
url = {http://arxiv.org/abs/2003.03123},
year = {2020}
}
@article{Corso2020,
abstract = {Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this setting. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a benchmark containing multiple tasks taken from classical graph theory, which demonstrates the capacity of our model.},
archivePrefix = {arXiv},
arxivId = {2004.05718},
author = {Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Li{\`{o}}, Pietro and Veli{\v{c}}kovi{\'{c}}, Petar},
eprint = {2004.05718},
file = {:Users/simon/Library/Application Support/Mendeley Desktop/Downloaded/Corso et al. - 2020 - Principal Neighbourhood Aggregation for Graph Nets.pdf:pdf},
title = {{Principal Neighbourhood Aggregation for Graph Nets}},
url = {http://arxiv.org/abs/2004.05718},
year = {2020}
}
