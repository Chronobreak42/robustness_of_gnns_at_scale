{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigeisler/robustness_of_gnns_at_scale/blob/notebook/Quick_start_robustness_gnns_at_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixdVBV5PugOh"
      },
      "source": [
        "# Robustness of Graph Neural Networks at Scale - Quick Start\n",
        "\n",
        "This notebook can be run in google colab and serves as a quick introduction to the [Robustness of Graph Neural Networks at Scale](https://github.com/sigeisler/robustness_of_gnns_at_scale) repository.\n",
        "\n",
        "## 0. Setup\n",
        "\n",
        "First, let's get the code and install requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "srl7gR-Gy5gX",
        "outputId": "b2ba3f32-f5b8-4527-f6ba-0b05e61771b9"
      },
      "outputs": [],
      "source": [
        "# clone package repository\n",
        "!git clone https://github.com/sigeisler/robustness_of_gnns_at_scale.git\n",
        "\n",
        "# navigate to the repository\n",
        "%cd robustness_of_gnns_at_scale\n",
        "\n",
        "# install package requirements\n",
        "# !pip install -r requirements.txt # colab already has these installed\n",
        "!pip install -r requirements-dev.txt\n",
        "\n",
        "# install package\n",
        "# !python setup.py install\n",
        "!pip install --use-feature=in-tree-build .\n",
        "\n",
        "# build kernels\n",
        "!pip install --use-feature=in-tree-build ./kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb2y6lUbdYi_"
      },
      "source": [
        "## 1. Training\n",
        "\n",
        "For the training and evaluation code we decided to provide Sacred experiments which make it very easy to run the same code from the command line or on your cluster. To train or attack the models you can use the `script_execute_experiment` script and simply specify the respective configuration or execute the experiment directly passing the desired configuration in [experiments/experiment_train.py](https://github.com/sigeisler/robustness_of_gnns_at_scale/blob/main/experiments/experiment_train.py#L74).\n",
        "\n",
        "In the example below, we train a `GCN` on `Cora ML`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ac11939e90534d398eb3f256ea174853",
            "295ff31ab4e6450d8cd0502c69d97288",
            "0250afcb7080447cb1420d3257c01891",
            "e5183a9fe79140bca1bf699b1fa8c553",
            "cad60df04ed544e6a09e20558a244976",
            "2c1b6ca2211c422f97aff856fd04baaa",
            "7761eeede57e40b7999ec8df737fad3f",
            "a1d310d3b5704d09a6736ff7a4f02074",
            "0d234a4a2e564ef4b661abd09a579d67",
            "09354c2e30c24e73a7296291d61e7c4c",
            "13b730ce7edd4674aa3f105cfe7bc3cd"
          ]
        },
        "id": "6mga1GGvdbLp",
        "outputId": "6498cbff-72df-4f99-ca47-2bab5d7856d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-11 15:31:58 (INFO): {'dataset': 'cora_ml', 'model_params': {'label': 'Vanilla GCN', 'model': 'GCN', 'do_cache_adj_prep': True, 'n_filters': 64, 'dropout': 0.5, 'svd_params': None, 'jaccard_params': None, 'gdc_params': {'alpha': 0.15, 'k': 64}}, 'train_params': {'lr': 0.01, 'weight_decay': 0.001, 'patience': 300, 'max_epochs': 3000}, 'binary_attr': False, 'make_undirected': True, 'seed': 0, 'artifact_dir': 'cache', 'model_storage_type': 'demo', 'ppr_cache_params': {}, 'device': 0, 'display_steps': 100, 'data_device': 0}\n",
            "2022-04-11 15:32:01 (INFO): Training set size: 140\n",
            "2022-04-11 15:32:01 (INFO): Validation set size: 140\n",
            "2022-04-11 15:32:01 (INFO): Test set size: 2530\n",
            "2022-04-11 15:32:01 (INFO): Memory Usage after loading the dataset:\n",
            "2022-04-11 15:32:01 (INFO): 3.087627410888672\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac11939e90534d398eb3f256ea174853",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training...:   0%|          | 0/3000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-11 15:32:01 (INFO): \n",
            "Epoch    0: loss_train: 1.94546, loss_val: 1.94581, acc_train: 0.12857, acc_val: 0.15714 \n",
            "2022-04-11 15:32:02 (INFO): \n",
            "Epoch  100: loss_train: 0.08831, loss_val: 0.39982, acc_train: 1.00000, acc_val: 0.90714 \n",
            "2022-04-11 15:32:03 (INFO): \n",
            "Epoch  200: loss_train: 0.07764, loss_val: 0.37613, acc_train: 1.00000, acc_val: 0.90000 \n",
            "2022-04-11 15:32:04 (INFO): \n",
            "Epoch  300: loss_train: 0.07414, loss_val: 0.39645, acc_train: 1.00000, acc_val: 0.88571 \n",
            "2022-04-11 15:32:05 (INFO): \n",
            "Epoch  400: loss_train: 0.06669, loss_val: 0.37830, acc_train: 1.00000, acc_val: 0.91429 \n",
            "2022-04-11 15:32:06 (INFO): \n",
            "Epoch  500: loss_train: 0.07280, loss_val: 0.38037, acc_train: 1.00000, acc_val: 0.90714 \n",
            "2022-04-11 15:32:06 (INFO): Test accuracy is 0.8185770511627197 with seed 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8185770511627197,\n",
              " 'model_path': 'cache/demo/demo_1.pt',\n",
              " 'trace_train': [1.9454624652862549,\n",
              "  1.8765193223953247,\n",
              "  1.7774474620819092,\n",
              "  1.6563345193862915,\n",
              "  1.536136507987976,\n",
              "  1.4086090326309204,\n",
              "  1.2841672897338867,\n",
              "  1.1469098329544067,\n",
              "  1.0158731937408447,\n",
              "  0.910151481628418,\n",
              "  0.7819910049438477,\n",
              "  0.6642155051231384,\n",
              "  0.6038099527359009,\n",
              "  0.545272946357727,\n",
              "  0.4568222463130951,\n",
              "  0.4078209102153778,\n",
              "  0.3527106046676636,\n",
              "  0.34389984607696533,\n",
              "  0.28623950481414795,\n",
              "  0.26793235540390015,\n",
              "  0.25208890438079834,\n",
              "  0.2332736700773239,\n",
              "  0.22999583184719086,\n",
              "  0.20495042204856873,\n",
              "  0.19868922233581543,\n",
              "  0.19470176100730896,\n",
              "  0.18154895305633545,\n",
              "  0.1853218674659729,\n",
              "  0.18129600584506989,\n",
              "  0.1832646280527115,\n",
              "  0.1607210785150528,\n",
              "  0.1759190857410431,\n",
              "  0.1606225222349167,\n",
              "  0.17753200232982635,\n",
              "  0.17196036875247955,\n",
              "  0.17054834961891174,\n",
              "  0.17432887852191925,\n",
              "  0.16151244938373566,\n",
              "  0.17313575744628906,\n",
              "  0.15916259586811066,\n",
              "  0.15914678573608398,\n",
              "  0.16137467324733734,\n",
              "  0.1435081958770752,\n",
              "  0.1584894061088562,\n",
              "  0.15368112921714783,\n",
              "  0.1442224532365799,\n",
              "  0.14095990359783173,\n",
              "  0.14580146968364716,\n",
              "  0.13953302800655365,\n",
              "  0.13623549044132233,\n",
              "  0.13352873921394348,\n",
              "  0.1325882375240326,\n",
              "  0.13281208276748657,\n",
              "  0.13602599501609802,\n",
              "  0.13410137593746185,\n",
              "  0.1278298944234848,\n",
              "  0.12045950442552567,\n",
              "  0.11548545211553574,\n",
              "  0.12650488317012787,\n",
              "  0.11120671033859253,\n",
              "  0.11290527135133743,\n",
              "  0.1101432517170906,\n",
              "  0.12298240512609482,\n",
              "  0.11479354649782181,\n",
              "  0.10997384786605835,\n",
              "  0.10979470610618591,\n",
              "  0.11014271527528763,\n",
              "  0.11066227406263351,\n",
              "  0.10922015458345413,\n",
              "  0.11671212315559387,\n",
              "  0.11630456149578094,\n",
              "  0.09910757839679718,\n",
              "  0.11409700661897659,\n",
              "  0.1075548529624939,\n",
              "  0.11335631459951401,\n",
              "  0.10807591676712036,\n",
              "  0.11999574303627014,\n",
              "  0.10622256249189377,\n",
              "  0.10520853847265244,\n",
              "  0.10463797301054001,\n",
              "  0.1022256463766098,\n",
              "  0.09990998357534409,\n",
              "  0.09312403202056885,\n",
              "  0.10852690786123276,\n",
              "  0.10062839835882187,\n",
              "  0.101137675344944,\n",
              "  0.09818650037050247,\n",
              "  0.09255743026733398,\n",
              "  0.09483841806650162,\n",
              "  0.09656927734613419,\n",
              "  0.1076381579041481,\n",
              "  0.09355910122394562,\n",
              "  0.09444035589694977,\n",
              "  0.10766557604074478,\n",
              "  0.08932018280029297,\n",
              "  0.09067957103252411,\n",
              "  0.10573992878198624,\n",
              "  0.09527070820331573,\n",
              "  0.08591015636920929,\n",
              "  0.09086459875106812,\n",
              "  0.08831144124269485,\n",
              "  0.09529493749141693,\n",
              "  0.08763248473405838,\n",
              "  0.10522740334272385,\n",
              "  0.09032468497753143,\n",
              "  0.08377087116241455,\n",
              "  0.09593658149242401,\n",
              "  0.08524572104215622,\n",
              "  0.09495316445827484,\n",
              "  0.08273616433143616,\n",
              "  0.09104061871767044,\n",
              "  0.09952601045370102,\n",
              "  0.09217376261949539,\n",
              "  0.09030833840370178,\n",
              "  0.08511007577180862,\n",
              "  0.09215347468852997,\n",
              "  0.09653395414352417,\n",
              "  0.08362765610218048,\n",
              "  0.09537725150585175,\n",
              "  0.09117544442415237,\n",
              "  0.09232236444950104,\n",
              "  0.08155611157417297,\n",
              "  0.08144865185022354,\n",
              "  0.0831625908613205,\n",
              "  0.08503154665231705,\n",
              "  0.08070649206638336,\n",
              "  0.08265364170074463,\n",
              "  0.0801684632897377,\n",
              "  0.08761996030807495,\n",
              "  0.08795752376317978,\n",
              "  0.07808081060647964,\n",
              "  0.08049744367599487,\n",
              "  0.08643175661563873,\n",
              "  0.0896553099155426,\n",
              "  0.08395235985517502,\n",
              "  0.08158736675977707,\n",
              "  0.08887969702482224,\n",
              "  0.07763232290744781,\n",
              "  0.0846264511346817,\n",
              "  0.08520349860191345,\n",
              "  0.0818156972527504,\n",
              "  0.08240552246570587,\n",
              "  0.07893750071525574,\n",
              "  0.08188491314649582,\n",
              "  0.08709950000047684,\n",
              "  0.0767858475446701,\n",
              "  0.0798293873667717,\n",
              "  0.07851400971412659,\n",
              "  0.08466262370347977,\n",
              "  0.07794567942619324,\n",
              "  0.0737026110291481,\n",
              "  0.08350660651922226,\n",
              "  0.07928808778524399,\n",
              "  0.07711523026227951,\n",
              "  0.07621517777442932,\n",
              "  0.08051528036594391,\n",
              "  0.0771523043513298,\n",
              "  0.07713542878627777,\n",
              "  0.07707583904266357,\n",
              "  0.07690031826496124,\n",
              "  0.07576137781143188,\n",
              "  0.07461893558502197,\n",
              "  0.07869041711091995,\n",
              "  0.07748046517372131,\n",
              "  0.08487940579652786,\n",
              "  0.08249688148498535,\n",
              "  0.0786413922905922,\n",
              "  0.08377847075462341,\n",
              "  0.07768649607896805,\n",
              "  0.07493269443511963,\n",
              "  0.07727725803852081,\n",
              "  0.07829191535711288,\n",
              "  0.08558938652276993,\n",
              "  0.07840380072593689,\n",
              "  0.07967446744441986,\n",
              "  0.07077636569738388,\n",
              "  0.08282405138015747,\n",
              "  0.07384126633405685,\n",
              "  0.07937047630548477,\n",
              "  0.07733709365129471,\n",
              "  0.08134502917528152,\n",
              "  0.08006670325994492,\n",
              "  0.07580161094665527,\n",
              "  0.07738342136144638,\n",
              "  0.0738472267985344,\n",
              "  0.0788361132144928,\n",
              "  0.08407812565565109,\n",
              "  0.07737129926681519,\n",
              "  0.07586336880922318,\n",
              "  0.07039029151201248,\n",
              "  0.07636354118585587,\n",
              "  0.07789288461208344,\n",
              "  0.07367588579654694,\n",
              "  0.07518306374549866,\n",
              "  0.07362978160381317,\n",
              "  0.07304046303033829,\n",
              "  0.07388054579496384,\n",
              "  0.08194093406200409,\n",
              "  0.08051576465368271,\n",
              "  0.08401905745267868,\n",
              "  0.07763823866844177,\n",
              "  0.07570686936378479,\n",
              "  0.0769566148519516,\n",
              "  0.08398996293544769,\n",
              "  0.07111790031194687,\n",
              "  0.06728065758943558,\n",
              "  0.06879151612520218,\n",
              "  0.0758417546749115,\n",
              "  0.07204801589250565,\n",
              "  0.07957389950752258,\n",
              "  0.07196909189224243,\n",
              "  0.08058952540159225,\n",
              "  0.0753682404756546,\n",
              "  0.07657365500926971,\n",
              "  0.07690723985433578,\n",
              "  0.07426050305366516,\n",
              "  0.07812266051769257,\n",
              "  0.07929809391498566,\n",
              "  0.07532280683517456,\n",
              "  0.07527928054332733,\n",
              "  0.06899525225162506,\n",
              "  0.07486473023891449,\n",
              "  0.07345186918973923,\n",
              "  0.07294358313083649,\n",
              "  0.0818217322230339,\n",
              "  0.07212524861097336,\n",
              "  0.0738687664270401,\n",
              "  0.071187824010849,\n",
              "  0.07435037940740585,\n",
              "  0.07187630236148834,\n",
              "  0.06800747662782669,\n",
              "  0.07801736146211624,\n",
              "  0.08072804659605026,\n",
              "  0.07205942273139954,\n",
              "  0.07267690449953079,\n",
              "  0.07891114056110382,\n",
              "  0.07806283980607986,\n",
              "  0.07397490739822388,\n",
              "  0.07242061197757721,\n",
              "  0.06766543537378311,\n",
              "  0.07112132757902145,\n",
              "  0.07195796072483063,\n",
              "  0.0739990621805191,\n",
              "  0.06748563796281815,\n",
              "  0.07306589931249619,\n",
              "  0.0753890722990036,\n",
              "  0.08540141582489014,\n",
              "  0.06763164699077606,\n",
              "  0.07729250937700272,\n",
              "  0.07847108691930771,\n",
              "  0.0742059126496315,\n",
              "  0.08072364330291748,\n",
              "  0.07293923199176788,\n",
              "  0.07004181295633316,\n",
              "  0.07164599746465683,\n",
              "  0.07403608411550522,\n",
              "  0.07322467863559723,\n",
              "  0.07310831546783447,\n",
              "  0.07330770045518875,\n",
              "  0.07460492104291916,\n",
              "  0.06726079434156418,\n",
              "  0.07604452222585678,\n",
              "  0.0737617090344429,\n",
              "  0.07372152805328369,\n",
              "  0.08259514719247818,\n",
              "  0.07502197474241257,\n",
              "  0.07266423106193542,\n",
              "  0.07803486287593842,\n",
              "  0.06688709557056427,\n",
              "  0.06939739733934402,\n",
              "  0.0707789808511734,\n",
              "  0.07106918841600418,\n",
              "  0.07046601176261902,\n",
              "  0.07100382447242737,\n",
              "  0.08388391137123108,\n",
              "  0.06990614533424377,\n",
              "  0.07033002376556396,\n",
              "  0.06975814700126648,\n",
              "  0.07445579767227173,\n",
              "  0.06971868127584457,\n",
              "  0.07374773174524307,\n",
              "  0.06878868490457535,\n",
              "  0.07566611468791962,\n",
              "  0.07307013124227524,\n",
              "  0.06695210933685303,\n",
              "  0.06978923082351685,\n",
              "  0.07469354569911957,\n",
              "  0.07358498871326447,\n",
              "  0.07493890821933746,\n",
              "  0.07522055506706238,\n",
              "  0.07439439743757248,\n",
              "  0.07325567305088043,\n",
              "  0.06835433095693588,\n",
              "  0.06957685947418213,\n",
              "  0.06863974034786224,\n",
              "  0.07589084655046463,\n",
              "  0.0768352746963501,\n",
              "  0.08346335589885712,\n",
              "  0.07497764378786087,\n",
              "  0.07419546693563461,\n",
              "  0.07414127141237259,\n",
              "  0.07181386649608612,\n",
              "  0.07151369005441666,\n",
              "  0.0704316720366478,\n",
              "  0.06886349618434906,\n",
              "  0.07392474263906479,\n",
              "  0.0660652369260788,\n",
              "  0.0704478919506073,\n",
              "  0.07604644447565079,\n",
              "  0.06278172135353088,\n",
              "  0.0738469660282135,\n",
              "  0.07136350125074387,\n",
              "  0.07125648111104965,\n",
              "  0.07299483567476273,\n",
              "  0.07491063326597214,\n",
              "  0.0768868550658226,\n",
              "  0.07833603769540787,\n",
              "  0.0663604587316513,\n",
              "  0.07419463247060776,\n",
              "  0.07328619807958603,\n",
              "  0.06720153987407684,\n",
              "  0.07475204765796661,\n",
              "  0.06879179179668427,\n",
              "  0.06555420160293579,\n",
              "  0.07413772493600845,\n",
              "  0.07235153764486313,\n",
              "  0.06878612190485,\n",
              "  0.067410409450531,\n",
              "  0.07396092265844345,\n",
              "  0.07348872721195221,\n",
              "  0.07428189367055893,\n",
              "  0.07700775563716888,\n",
              "  0.06720873713493347,\n",
              "  0.07208046317100525,\n",
              "  0.07375974208116531,\n",
              "  0.07563642412424088,\n",
              "  0.06580270826816559,\n",
              "  0.07702070474624634,\n",
              "  0.0701683983206749,\n",
              "  0.06415438652038574,\n",
              "  0.0705319419503212,\n",
              "  0.0669446513056755,\n",
              "  0.0719376802444458,\n",
              "  0.07811425626277924,\n",
              "  0.07350952178239822,\n",
              "  0.0690566748380661,\n",
              "  0.07218679785728455,\n",
              "  0.06868413090705872,\n",
              "  0.06729129701852798,\n",
              "  0.06915543973445892,\n",
              "  0.07717482000589371,\n",
              "  0.06926341354846954,\n",
              "  0.07367929816246033,\n",
              "  0.06965163350105286,\n",
              "  0.06804157048463821,\n",
              "  0.0685725137591362,\n",
              "  0.07999638468027115,\n",
              "  0.06471160799264908,\n",
              "  0.07518293708562851,\n",
              "  0.07887306064367294,\n",
              "  0.07579910010099411,\n",
              "  0.0698947012424469,\n",
              "  0.06937352567911148,\n",
              "  0.07280207425355911,\n",
              "  0.0688510313630104,\n",
              "  0.0733644962310791,\n",
              "  0.06639429181814194,\n",
              "  0.06884007155895233,\n",
              "  0.07685387879610062,\n",
              "  0.0682227835059166,\n",
              "  0.06996419280767441,\n",
              "  0.07153909653425217,\n",
              "  0.07255470752716064,\n",
              "  0.06656882166862488,\n",
              "  0.07226891815662384,\n",
              "  0.07149847596883774,\n",
              "  0.07471442222595215,\n",
              "  0.0707818791270256,\n",
              "  0.06492859870195389,\n",
              "  0.07231838256120682,\n",
              "  0.07584042847156525,\n",
              "  0.06963127106428146,\n",
              "  0.07318224012851715,\n",
              "  0.0710441842675209,\n",
              "  0.08094454556703568,\n",
              "  0.07367002964019775,\n",
              "  0.0737631693482399,\n",
              "  0.07559448480606079,\n",
              "  0.06839693337678909,\n",
              "  0.06498828530311584,\n",
              "  0.06819392740726471,\n",
              "  0.07366976141929626,\n",
              "  0.07515370100736618,\n",
              "  0.07555466145277023,\n",
              "  0.06805684417486191,\n",
              "  0.06878481805324554,\n",
              "  0.07155568897724152,\n",
              "  0.0662366971373558,\n",
              "  0.0713069811463356,\n",
              "  0.06849706918001175,\n",
              "  0.06668685376644135,\n",
              "  0.07049056142568588,\n",
              "  0.07010304927825928,\n",
              "  0.06879657506942749,\n",
              "  0.07602161169052124,\n",
              "  0.07139775156974792,\n",
              "  0.08663618564605713,\n",
              "  0.07704844325780869,\n",
              "  0.06943262368440628,\n",
              "  0.07178135216236115,\n",
              "  0.07552161067724228,\n",
              "  0.07184851169586182,\n",
              "  0.06676580756902695,\n",
              "  0.07083873450756073,\n",
              "  0.06508956104516983,\n",
              "  0.06793946027755737,\n",
              "  0.07164820283651352,\n",
              "  0.07549329847097397,\n",
              "  0.0773543044924736,\n",
              "  0.07469898462295532,\n",
              "  0.07214291393756866,\n",
              "  0.07292602211236954,\n",
              "  0.07318102568387985,\n",
              "  0.07529836893081665,\n",
              "  0.06847263127565384,\n",
              "  0.07070738822221756,\n",
              "  0.06606148928403854,\n",
              "  0.07309854030609131,\n",
              "  0.06884445995092392,\n",
              "  0.07455025613307953,\n",
              "  0.07086513936519623,\n",
              "  0.07541806995868683,\n",
              "  0.06685905903577805,\n",
              "  0.06778871268033981,\n",
              "  0.07530378550291061,\n",
              "  0.07014582306146622,\n",
              "  0.07416532188653946,\n",
              "  0.07017481327056885,\n",
              "  0.07505416125059128,\n",
              "  0.07664867490530014,\n",
              "  0.06639375537633896,\n",
              "  0.07234811782836914,\n",
              "  0.07089018076658249,\n",
              "  0.07101357728242874,\n",
              "  0.06985421478748322,\n",
              "  0.08041688799858093,\n",
              "  0.0726538673043251,\n",
              "  0.0779852420091629,\n",
              "  0.07375345379114151,\n",
              "  0.07110985368490219,\n",
              "  0.06866304576396942,\n",
              "  0.08168499171733856,\n",
              "  0.07345100492238998,\n",
              "  0.07078295946121216,\n",
              "  0.06319686025381088,\n",
              "  0.06897776573896408,\n",
              "  0.07061818242073059,\n",
              "  0.07267383486032486,\n",
              "  0.06957611441612244,\n",
              "  0.0718003585934639,\n",
              "  0.07634159177541733,\n",
              "  0.07342886179685593,\n",
              "  0.0750800222158432,\n",
              "  0.06616267561912537,\n",
              "  0.07559145987033844,\n",
              "  0.0653897374868393,\n",
              "  0.07820971310138702,\n",
              "  0.06745504587888718,\n",
              "  0.07968752831220627,\n",
              "  0.06916297972202301,\n",
              "  0.07847058027982712,\n",
              "  0.07989541441202164,\n",
              "  0.06797834485769272,\n",
              "  0.0690954178571701,\n",
              "  0.06775835901498795,\n",
              "  0.07343748956918716,\n",
              "  0.067483089864254,\n",
              "  0.06815756112337112,\n",
              "  0.06779622286558151,\n",
              "  0.07259123772382736,\n",
              "  0.0732329711318016,\n",
              "  0.06733450293540955,\n",
              "  0.07647238671779633,\n",
              "  0.07034260034561157,\n",
              "  0.07510723173618317,\n",
              "  0.07273433357477188,\n",
              "  0.06637857109308243,\n",
              "  0.07495111972093582,\n",
              "  0.07084683328866959,\n",
              "  0.06811260432004929,\n",
              "  0.06426391750574112,\n",
              "  0.07577698677778244,\n",
              "  0.07279229164123535,\n",
              "  0.07557054609060287,\n",
              "  0.06659217923879623,\n",
              "  0.06816435605287552,\n",
              "  0.07412617653608322,\n",
              "  0.06617465615272522,\n",
              "  0.07106118649244308,\n",
              "  0.07378647476434708,\n",
              "  0.07280251383781433,\n",
              "  0.07336103916168213,\n",
              "  0.07042042911052704,\n",
              "  0.06547398120164871,\n",
              "  0.07051335275173187,\n",
              "  0.07209205627441406,\n",
              "  0.07376475632190704,\n",
              "  0.0704282894730568,\n",
              "  0.07092975825071335,\n",
              "  0.06760700792074203,\n",
              "  0.08015414327383041,\n",
              "  0.06561852246522903,\n",
              "  0.07105165719985962,\n",
              "  0.0750579833984375,\n",
              "  0.06850704550743103,\n",
              "  0.0705743059515953,\n",
              "  0.0707627460360527,\n",
              "  0.06583721190690994,\n",
              "  0.0681033581495285,\n",
              "  0.07065474987030029,\n",
              "  0.08150304853916168,\n",
              "  0.06556231528520584,\n",
              "  0.07232894748449326,\n",
              "  0.071319580078125,\n",
              "  0.07026871293783188,\n",
              "  0.07015526294708252,\n",
              "  0.0669829249382019,\n",
              "  0.06582657247781754,\n",
              "  0.07769040763378143,\n",
              "  0.07657786458730698,\n",
              "  0.06664837151765823,\n",
              "  0.06528663635253906,\n",
              "  0.06858279556035995,\n",
              "  0.07666850090026855,\n",
              "  0.06621811538934708,\n",
              "  0.06855981796979904],\n",
              " 'trace_val': [1.9458086490631104,\n",
              "  1.8983572721481323,\n",
              "  1.8273985385894775,\n",
              "  1.7385255098342896,\n",
              "  1.6504135131835938,\n",
              "  1.55681312084198,\n",
              "  1.457169771194458,\n",
              "  1.3693373203277588,\n",
              "  1.2457658052444458,\n",
              "  1.1692532300949097,\n",
              "  1.0975494384765625,\n",
              "  0.9934638142585754,\n",
              "  0.920744776725769,\n",
              "  0.8867924809455872,\n",
              "  0.8259952068328857,\n",
              "  0.7557573318481445,\n",
              "  0.714472770690918,\n",
              "  0.6564303636550903,\n",
              "  0.633428156375885,\n",
              "  0.6104797124862671,\n",
              "  0.5896329879760742,\n",
              "  0.5639485120773315,\n",
              "  0.5323152542114258,\n",
              "  0.5352728366851807,\n",
              "  0.5481207966804504,\n",
              "  0.5233671069145203,\n",
              "  0.5269015431404114,\n",
              "  0.5579335689544678,\n",
              "  0.5172232985496521,\n",
              "  0.4948172867298126,\n",
              "  0.4971565902233124,\n",
              "  0.5277129411697388,\n",
              "  0.4869767427444458,\n",
              "  0.48411130905151367,\n",
              "  0.4558480381965637,\n",
              "  0.4921707808971405,\n",
              "  0.4849965274333954,\n",
              "  0.4571562111377716,\n",
              "  0.47464659810066223,\n",
              "  0.47583329677581787,\n",
              "  0.4703189432621002,\n",
              "  0.46517765522003174,\n",
              "  0.4697255790233612,\n",
              "  0.4537728428840637,\n",
              "  0.4953461289405823,\n",
              "  0.49740901589393616,\n",
              "  0.4352671802043915,\n",
              "  0.47139450907707214,\n",
              "  0.4381427466869354,\n",
              "  0.45526742935180664,\n",
              "  0.44689786434173584,\n",
              "  0.4429704546928406,\n",
              "  0.4492012858390808,\n",
              "  0.42258110642433167,\n",
              "  0.4391893148422241,\n",
              "  0.43025222420692444,\n",
              "  0.4279213547706604,\n",
              "  0.4170669913291931,\n",
              "  0.4314036965370178,\n",
              "  0.4053283631801605,\n",
              "  0.4525439739227295,\n",
              "  0.4313931167125702,\n",
              "  0.4391247034072876,\n",
              "  0.4336811602115631,\n",
              "  0.4027527868747711,\n",
              "  0.41125667095184326,\n",
              "  0.39467376470565796,\n",
              "  0.4255686402320862,\n",
              "  0.4247288703918457,\n",
              "  0.42858070135116577,\n",
              "  0.4172200858592987,\n",
              "  0.44425052404403687,\n",
              "  0.41459420323371887,\n",
              "  0.39446452260017395,\n",
              "  0.42157119512557983,\n",
              "  0.44516289234161377,\n",
              "  0.42002493143081665,\n",
              "  0.4149439334869385,\n",
              "  0.41417020559310913,\n",
              "  0.3927859663963318,\n",
              "  0.4208071529865265,\n",
              "  0.4327845871448517,\n",
              "  0.403705358505249,\n",
              "  0.40655115246772766,\n",
              "  0.4205333888530731,\n",
              "  0.3957173824310303,\n",
              "  0.3959771394729614,\n",
              "  0.4239743947982788,\n",
              "  0.42161068320274353,\n",
              "  0.4108070433139801,\n",
              "  0.4394359886646271,\n",
              "  0.4410351812839508,\n",
              "  0.4340229630470276,\n",
              "  0.4115619659423828,\n",
              "  0.40628597140312195,\n",
              "  0.416265606880188,\n",
              "  0.42391207814216614,\n",
              "  0.41739267110824585,\n",
              "  0.3993520438671112,\n",
              "  0.3939777612686157,\n",
              "  0.3998223841190338,\n",
              "  0.43555501103401184,\n",
              "  0.41394174098968506,\n",
              "  0.39452052116394043,\n",
              "  0.3889000415802002,\n",
              "  0.42838606238365173,\n",
              "  0.4040893018245697,\n",
              "  0.37141624093055725,\n",
              "  0.40477633476257324,\n",
              "  0.4215286672115326,\n",
              "  0.41339364647865295,\n",
              "  0.4112156331539154,\n",
              "  0.44423216581344604,\n",
              "  0.41944053769111633,\n",
              "  0.39267516136169434,\n",
              "  0.41417837142944336,\n",
              "  0.38785216212272644,\n",
              "  0.4012841284275055,\n",
              "  0.42843344807624817,\n",
              "  0.41818180680274963,\n",
              "  0.39457249641418457,\n",
              "  0.37973785400390625,\n",
              "  0.38893622159957886,\n",
              "  0.3879025876522064,\n",
              "  0.37616685032844543,\n",
              "  0.3772009313106537,\n",
              "  0.39936691522598267,\n",
              "  0.4126855731010437,\n",
              "  0.43382728099823,\n",
              "  0.40590012073516846,\n",
              "  0.4266124367713928,\n",
              "  0.43212082982063293,\n",
              "  0.392751544713974,\n",
              "  0.4001312851905823,\n",
              "  0.39762818813323975,\n",
              "  0.40781885385513306,\n",
              "  0.3972882032394409,\n",
              "  0.35705533623695374,\n",
              "  0.4199467599391937,\n",
              "  0.42681685090065,\n",
              "  0.38762521743774414,\n",
              "  0.4062913656234741,\n",
              "  0.3889169991016388,\n",
              "  0.40149959921836853,\n",
              "  0.4345124363899231,\n",
              "  0.43114128708839417,\n",
              "  0.37753769755363464,\n",
              "  0.3811464011669159,\n",
              "  0.3890610337257385,\n",
              "  0.3513208031654358,\n",
              "  0.4177996814250946,\n",
              "  0.37450727820396423,\n",
              "  0.3975191116333008,\n",
              "  0.4105489253997803,\n",
              "  0.3528960943222046,\n",
              "  0.4029442369937897,\n",
              "  0.38859954476356506,\n",
              "  0.36654138565063477,\n",
              "  0.4112900197505951,\n",
              "  0.39400869607925415,\n",
              "  0.3696107864379883,\n",
              "  0.4350522458553314,\n",
              "  0.4079478979110718,\n",
              "  0.40219876170158386,\n",
              "  0.39806798100471497,\n",
              "  0.3859691321849823,\n",
              "  0.3954431116580963,\n",
              "  0.3923104405403137,\n",
              "  0.3959227502346039,\n",
              "  0.3894766867160797,\n",
              "  0.3804193437099457,\n",
              "  0.4408196210861206,\n",
              "  0.42405739426612854,\n",
              "  0.3841691017150879,\n",
              "  0.42549192905426025,\n",
              "  0.40094226598739624,\n",
              "  0.39867183566093445,\n",
              "  0.3632373511791229,\n",
              "  0.3864394724369049,\n",
              "  0.3696194589138031,\n",
              "  0.4047972559928894,\n",
              "  0.4091225862503052,\n",
              "  0.38373130559921265,\n",
              "  0.4149555265903473,\n",
              "  0.387429416179657,\n",
              "  0.3742521405220032,\n",
              "  0.39919763803482056,\n",
              "  0.38617029786109924,\n",
              "  0.4184466004371643,\n",
              "  0.4269067943096161,\n",
              "  0.4147152304649353,\n",
              "  0.39318299293518066,\n",
              "  0.41324588656425476,\n",
              "  0.39852985739707947,\n",
              "  0.3930976688861847,\n",
              "  0.3608098030090332,\n",
              "  0.40273237228393555,\n",
              "  0.40533512830734253,\n",
              "  0.3930821716785431,\n",
              "  0.4051331877708435,\n",
              "  0.37612974643707275,\n",
              "  0.39961209893226624,\n",
              "  0.3896925449371338,\n",
              "  0.3975021541118622,\n",
              "  0.3808801472187042,\n",
              "  0.40258732438087463,\n",
              "  0.3961772918701172,\n",
              "  0.405321329832077,\n",
              "  0.42826035618782043,\n",
              "  0.3782132565975189,\n",
              "  0.38170480728149414,\n",
              "  0.3619426190853119,\n",
              "  0.36559024453163147,\n",
              "  0.3650161027908325,\n",
              "  0.39267265796661377,\n",
              "  0.41162410378456116,\n",
              "  0.39161309599876404,\n",
              "  0.4059183597564697,\n",
              "  0.36549437046051025,\n",
              "  0.3938738703727722,\n",
              "  0.4217061996459961,\n",
              "  0.35467517375946045,\n",
              "  0.404829740524292,\n",
              "  0.4017525017261505,\n",
              "  0.38011786341667175,\n",
              "  0.36961039900779724,\n",
              "  0.3760843575000763,\n",
              "  0.4011533558368683,\n",
              "  0.40065836906433105,\n",
              "  0.40768271684646606,\n",
              "  0.36411556601524353,\n",
              "  0.39629825949668884,\n",
              "  0.4100399613380432,\n",
              "  0.40216654539108276,\n",
              "  0.37412720918655396,\n",
              "  0.34221404790878296,\n",
              "  0.40405288338661194,\n",
              "  0.3734041452407837,\n",
              "  0.39689281582832336,\n",
              "  0.36215829849243164,\n",
              "  0.3725495934486389,\n",
              "  0.3855791985988617,\n",
              "  0.37463387846946716,\n",
              "  0.38524389266967773,\n",
              "  0.3828681707382202,\n",
              "  0.3728848993778229,\n",
              "  0.4099005162715912,\n",
              "  0.3683489263057709,\n",
              "  0.3888567388057709,\n",
              "  0.393731027841568,\n",
              "  0.3917013704776764,\n",
              "  0.39297837018966675,\n",
              "  0.38733410835266113,\n",
              "  0.3835935890674591,\n",
              "  0.3744681477546692,\n",
              "  0.3979516327381134,\n",
              "  0.377372145652771,\n",
              "  0.39944055676460266,\n",
              "  0.4286138415336609,\n",
              "  0.4106096923351288,\n",
              "  0.3930230140686035,\n",
              "  0.3735085129737854,\n",
              "  0.3972712457180023,\n",
              "  0.39233261346817017,\n",
              "  0.39788132905960083,\n",
              "  0.3870972990989685,\n",
              "  0.40333667397499084,\n",
              "  0.370221346616745,\n",
              "  0.3844008147716522,\n",
              "  0.3932136297225952,\n",
              "  0.3852345645427704,\n",
              "  0.4077465236186981,\n",
              "  0.3801037073135376,\n",
              "  0.4232177734375,\n",
              "  0.38086339831352234,\n",
              "  0.3798537254333496,\n",
              "  0.3908158838748932,\n",
              "  0.37570425868034363,\n",
              "  0.38455653190612793,\n",
              "  0.40246376395225525,\n",
              "  0.39771419763565063,\n",
              "  0.4120163917541504,\n",
              "  0.401796817779541,\n",
              "  0.3587551712989807,\n",
              "  0.4320191442966461,\n",
              "  0.4017936885356903,\n",
              "  0.3918331265449524,\n",
              "  0.3958694338798523,\n",
              "  0.3645104169845581,\n",
              "  0.3504353165626526,\n",
              "  0.3810950517654419,\n",
              "  0.37675416469573975,\n",
              "  0.39419281482696533,\n",
              "  0.3856479227542877,\n",
              "  0.395812451839447,\n",
              "  0.388553649187088,\n",
              "  0.37621206045150757,\n",
              "  0.38149240612983704,\n",
              "  0.3779938220977783,\n",
              "  0.39089199900627136,\n",
              "  0.39644819498062134,\n",
              "  0.3821323812007904,\n",
              "  0.3985336124897003,\n",
              "  0.3836668133735657,\n",
              "  0.39810627698898315,\n",
              "  0.40283259749412537,\n",
              "  0.38940078020095825,\n",
              "  0.3866797983646393,\n",
              "  0.426530122756958,\n",
              "  0.39491304755210876,\n",
              "  0.3963121175765991,\n",
              "  0.40884479880332947,\n",
              "  0.37000587582588196,\n",
              "  0.4001438319683075,\n",
              "  0.42656975984573364,\n",
              "  0.40668484568595886,\n",
              "  0.39401763677597046,\n",
              "  0.37066882848739624,\n",
              "  0.4014788269996643,\n",
              "  0.41410377621650696,\n",
              "  0.4067825973033905,\n",
              "  0.3778471052646637,\n",
              "  0.3975267708301544,\n",
              "  0.4116717278957367,\n",
              "  0.39601442217826843,\n",
              "  0.3792162537574768,\n",
              "  0.3791540861129761,\n",
              "  0.4034317135810852,\n",
              "  0.3839220106601715,\n",
              "  0.3984687328338623,\n",
              "  0.391137957572937,\n",
              "  0.3968223035335541,\n",
              "  0.40846750140190125,\n",
              "  0.39734625816345215,\n",
              "  0.41689759492874146,\n",
              "  0.3744485080242157,\n",
              "  0.3760519027709961,\n",
              "  0.3928150236606598,\n",
              "  0.38994085788726807,\n",
              "  0.39401260018348694,\n",
              "  0.42830774188041687,\n",
              "  0.39298731088638306,\n",
              "  0.4029644727706909,\n",
              "  0.35957449674606323,\n",
              "  0.4166794419288635,\n",
              "  0.38185223937034607,\n",
              "  0.3979300856590271,\n",
              "  0.3910173773765564,\n",
              "  0.38581669330596924,\n",
              "  0.37289223074913025,\n",
              "  0.3954262435436249,\n",
              "  0.39621061086654663,\n",
              "  0.4239944517612457,\n",
              "  0.37477752566337585,\n",
              "  0.37351545691490173,\n",
              "  0.4028468132019043,\n",
              "  0.396322101354599,\n",
              "  0.3654296100139618,\n",
              "  0.38578662276268005,\n",
              "  0.4164990782737732,\n",
              "  0.4098794162273407,\n",
              "  0.42454805970191956,\n",
              "  0.3867379128932953,\n",
              "  0.39748576283454895,\n",
              "  0.3941291570663452,\n",
              "  0.3891606032848358,\n",
              "  0.3989705443382263,\n",
              "  0.38307392597198486,\n",
              "  0.36559006571769714,\n",
              "  0.37197455763816833,\n",
              "  0.4097214937210083,\n",
              "  0.36508485674858093,\n",
              "  0.40383970737457275,\n",
              "  0.38570326566696167,\n",
              "  0.40672436356544495,\n",
              "  0.3979814350605011,\n",
              "  0.40578386187553406,\n",
              "  0.3671250343322754,\n",
              "  0.3688664436340332,\n",
              "  0.39365774393081665,\n",
              "  0.4025857150554657,\n",
              "  0.3880380690097809,\n",
              "  0.3846967816352844,\n",
              "  0.39645838737487793,\n",
              "  0.4039440453052521,\n",
              "  0.40738269686698914,\n",
              "  0.3683193027973175,\n",
              "  0.3978255093097687,\n",
              "  0.38373732566833496,\n",
              "  0.39579761028289795,\n",
              "  0.4364181458950043,\n",
              "  0.42840495705604553,\n",
              "  0.41863319277763367,\n",
              "  0.3713933527469635,\n",
              "  0.3816690742969513,\n",
              "  0.392429918050766,\n",
              "  0.3665054738521576,\n",
              "  0.3777081072330475,\n",
              "  0.3873533308506012,\n",
              "  0.4128887355327606,\n",
              "  0.37829673290252686,\n",
              "  0.35991570353507996,\n",
              "  0.3724766969680786,\n",
              "  0.39398831129074097,\n",
              "  0.4112522602081299,\n",
              "  0.3997328281402588,\n",
              "  0.4008118510246277,\n",
              "  0.3946913480758667,\n",
              "  0.37426528334617615,\n",
              "  0.37300506234169006,\n",
              "  0.3805524706840515,\n",
              "  0.3621336817741394,\n",
              "  0.38141700625419617,\n",
              "  0.3970183730125427,\n",
              "  0.40450525283813477,\n",
              "  0.36492830514907837,\n",
              "  0.39918026328086853,\n",
              "  0.415545791387558,\n",
              "  0.3912319540977478,\n",
              "  0.3989170789718628,\n",
              "  0.3714345693588257,\n",
              "  0.3988308608531952,\n",
              "  0.4094535708427429,\n",
              "  0.400316447019577,\n",
              "  0.3731636703014374,\n",
              "  0.36285799741744995,\n",
              "  0.4087425768375397,\n",
              "  0.38359785079956055,\n",
              "  0.38715559244155884,\n",
              "  0.377874493598938,\n",
              "  0.416248619556427,\n",
              "  0.3733210265636444,\n",
              "  0.40489768981933594,\n",
              "  0.36798015236854553,\n",
              "  0.4001193344593048,\n",
              "  0.3916413187980652,\n",
              "  0.3838350176811218,\n",
              "  0.38369086384773254,\n",
              "  0.4070010185241699,\n",
              "  0.3949730396270752,\n",
              "  0.4100433588027954,\n",
              "  0.4013398587703705,\n",
              "  0.43855708837509155,\n",
              "  0.3990630805492401,\n",
              "  0.39760953187942505,\n",
              "  0.3807286322116852,\n",
              "  0.37222760915756226,\n",
              "  0.42886990308761597,\n",
              "  0.40481311082839966,\n",
              "  0.369436651468277,\n",
              "  0.3886192739009857,\n",
              "  0.38248535990715027,\n",
              "  0.3931952714920044,\n",
              "  0.42311742901802063,\n",
              "  0.41020044684410095,\n",
              "  0.40556690096855164,\n",
              "  0.39382392168045044,\n",
              "  0.39341750741004944,\n",
              "  0.38589200377464294,\n",
              "  0.3635130822658539,\n",
              "  0.40073853731155396,\n",
              "  0.38094601035118103,\n",
              "  0.38607078790664673,\n",
              "  0.390256404876709,\n",
              "  0.39575815200805664,\n",
              "  0.3887658417224884,\n",
              "  0.4204023480415344,\n",
              "  0.3795934319496155,\n",
              "  0.3650466799736023,\n",
              "  0.3838615417480469,\n",
              "  0.3595256507396698,\n",
              "  0.41042977571487427,\n",
              "  0.40320223569869995,\n",
              "  0.37598031759262085,\n",
              "  0.397885262966156,\n",
              "  0.37748652696609497,\n",
              "  0.36393722891807556,\n",
              "  0.4319354295730591,\n",
              "  0.4157254993915558,\n",
              "  0.4155743718147278,\n",
              "  0.3905608654022217,\n",
              "  0.37727150321006775,\n",
              "  0.4044599235057831,\n",
              "  0.39248737692832947,\n",
              "  0.3903142213821411,\n",
              "  0.40374431014060974,\n",
              "  0.3935806155204773,\n",
              "  0.38874197006225586,\n",
              "  0.41183146834373474,\n",
              "  0.38755592703819275,\n",
              "  0.4210144281387329,\n",
              "  0.40813177824020386,\n",
              "  0.38100436329841614,\n",
              "  0.37627744674682617,\n",
              "  0.365994930267334,\n",
              "  0.4062098264694214,\n",
              "  0.3804323077201843,\n",
              "  0.37659838795661926,\n",
              "  0.3664206564426422,\n",
              "  0.42783358693122864,\n",
              "  0.3803738057613373,\n",
              "  0.3968173563480377,\n",
              "  0.3878633677959442,\n",
              "  0.3845966160297394,\n",
              "  0.37624591588974,\n",
              "  0.4213649034500122,\n",
              "  0.39323726296424866,\n",
              "  0.4266013503074646,\n",
              "  0.3910275995731354,\n",
              "  0.390926718711853,\n",
              "  0.35166311264038086,\n",
              "  0.4080929160118103,\n",
              "  0.38888832926750183,\n",
              "  0.3999341130256653,\n",
              "  0.40296027064323425,\n",
              "  0.377753347158432,\n",
              "  0.3997664451599121,\n",
              "  0.384647399187088,\n",
              "  0.3896752893924713,\n",
              "  0.3869929611682892,\n",
              "  0.40089941024780273,\n",
              "  0.390645295381546,\n",
              "  0.3936295509338379,\n",
              "  0.3944956958293915,\n",
              "  0.38757583498954773,\n",
              "  0.4015766978263855,\n",
              "  0.4164515435695648,\n",
              "  0.3659975826740265,\n",
              "  0.3730084300041199,\n",
              "  0.40651026368141174,\n",
              "  0.3778762221336365,\n",
              "  0.4277869760990143,\n",
              "  0.38532355427742004,\n",
              "  0.4256151616573334,\n",
              "  0.3865291476249695,\n",
              "  0.3826952278614044]}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from experiments import experiment_train\n",
        "\n",
        "experiment_train.run(\n",
        "    data_dir = './data',\n",
        "    dataset = 'cora_ml',\n",
        "    model_params = dict(\n",
        "        label=\"Vanilla GCN\", \n",
        "        model=\"GCN\", \n",
        "        do_cache_adj_prep=True, \n",
        "        n_filters=64, \n",
        "        dropout=0.5, \n",
        "        svd_params=None, \n",
        "        jaccard_params=None, \n",
        "        gdc_params={\"alpha\": 0.15, \"k\": 64}),\n",
        "    train_params = dict(\n",
        "        lr=1e-2,\n",
        "        weight_decay=1e-3,\n",
        "        patience=300,\n",
        "        max_epochs=3000),\n",
        "    binary_attr = False,\n",
        "    make_undirected = True,\n",
        "    seed=0,\n",
        "    artifact_dir = 'cache',\n",
        "    model_storage_type = 'demo',\n",
        "    ppr_cache_params = dict(),\n",
        "    device = 0,\n",
        "    data_device = 0,\n",
        "    display_steps = 100,\n",
        "    debug_level = \"info\"     \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_JpK_oh9o4A"
      },
      "source": [
        "As we can see, the model achieved an accuracy of 0.8186."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzCocLocdb2y"
      },
      "source": [
        "## 2. Evaluation\n",
        "\n",
        "For evaluation, we use the locally stored models. Similarly to training, we provide a script that runs the attacks for different seeds for all pretrained models. For all experiments, please check out the [config](https://github.com/sigeisler/robustness_of_gnns_at_scale/tree/main/config) folder.\n",
        "\n",
        "### 2.1 Local PR-BCD Attack\n",
        "We provide an example for a `local PR-BCD` attack on the `Vanilla GCN` model trained previously. First, we create the `config` file for the attack:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tviiVnyi3l4O"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "demo_localprbcd_config = {\n",
        "    'seml': {'name': 'rgnn_at_scale_attack_evasion_local_direct', \n",
        "             'executable': 'experiments/experiment_local_attack_direct.py', \n",
        "             'project_root_dir': '../..', \n",
        "             'output_dir': 'config/attack_evasion_local_direct/output'},\n",
        "    'slurm': {'experiments_per_job': 4, \n",
        "              'sbatch_options': {'gres': 'gpu:1', 'mem': '16G', 'cpus-per-task': 4, 'time': '1-00:00'}}, \n",
        "    'fixed': {'data_dir': 'data/', \n",
        "              'artifact_dir': 'cache', \n",
        "              'nodes': 'None', \n",
        "              'nodes_topk': 4, \n",
        "              'attack_params.epochs': 500, \n",
        "              'attack_params.fine_tune_epochs': 100, \n",
        "              'attack_params.search_space_size': 10000, \n",
        "              'attack_params.ppr_recalc_at_end': True, \n",
        "              'attack_params.loss_type': 'Margin', \n",
        "              'device': 0, \n",
        "              'data_device': 0, \n",
        "              'binary_attr': False},\n",
        "    'grid': {'epsilons': {'type': 'choice', 'options': [[0.5]]}, \n",
        "             'seed': {'type': 'choice', 'options': [0]}, \n",
        "             'dataset': {'type': 'choice', 'options': ['cora_ml']}}, \n",
        "    'localprbcd_gcn': {'fixed': {'attack': 'LocalPRBCD', \n",
        "                                 'model_label': 'Vanilla GCN', \n",
        "                                 'model_storage_type': 'demo', \n",
        "                                 'attack_params': {'lr_factor': 0.05}, \n",
        "                                 'make_undirected': True}}\n",
        "}                      \n",
        "\n",
        "with open(r'/content/robustness_of_gnns_at_scale/config/attack_evasion_local_direct/demo_localprbcd.yaml', 'w') as file:\n",
        "    documents = yaml.dump(demo_localprbcd_config, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItLOPkW-7wzx"
      },
      "source": [
        "Now let's run the attack using this `demo_localprbcd` config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUOYoGHZdefU",
        "outputId": "8484deac-80f1-4c9c-ec92-885dffe23e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-11 16:03:38,585 - root - DEBUG - Namespace(config_file='config/attack_evasion_local_direct/demo_localprbcd.yaml', kwargs={}, output='output')\n",
            "2022-04-11 16:03:41,526 - git.cmd - DEBUG - Popen(['git', 'version'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:03:41,540 - git.cmd - DEBUG - Popen(['git', 'version'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:03:41,552 - git.cmd - DEBUG - Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:03:41,566 - git.cmd - DEBUG - Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:03:41,643 - git.cmd - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=<valid stream>)\n",
            "2022-04-11 16:03:41,659 - git.cmd - DEBUG - Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:03:41,672 - git.cmd - DEBUG - Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:03:41,750 - git.cmd - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=<valid stream>)\n",
            "2022-04-11 16:03:41 (WARNING): Changed type of config entry \"data_device\" from str to int\n",
            "2022-04-11 16:03:41 (WARNING): No observers have been added to this run\n",
            "2022-04-11 16:03:41 (INFO): Running command 'run'\n",
            "2022-04-11 16:03:41 (INFO): Started\n",
            "2022-04-11 16:03:41 (INFO): {'dataset': 'cora_ml', 'attack': 'LocalPRBCD', 'attack_params': {'ppr_cache_params': {'data_artifact_dir': 'cache', 'data_storage_type': 'ppr'}, 'epochs': 500, 'fine_tune_epochs': 100, 'search_space_size': 10000, 'ppr_recalc_at_end': True, 'loss_type': 'Margin', 'lr_factor': 0.05}, 'epsilons': [0.5], 'make_undirected': True, 'binary_attr': False, 'seed': 0, 'artifact_dir': 'cache', 'pert_adj_storage_type': None, 'pert_attr_storage_type': None, 'model_label': 'Vanilla GCN', 'model_storage_type': 'demo', 'device': 0, 'data_device': 0}\n",
            "2022-04-11 16:03:44 (INFO): Found 1 models with label 'Vanilla GCN' to attack.\n",
            "2022-04-11 16:03:44 (INFO): Sample Attack Nodes for model with accuracy 0.8186\n",
            "2022-04-11 16:03:44 (INFO): Found 2093 suitable '2+ degree' nodes out of 2530 candidate nodes to be sampled from for the attack of which 1725 have the correct class label\n",
            "2022-04-11 16:03:44 (INFO): Sample the following attack nodes:\n",
            "[475]\n",
            "[2210]\n",
            "[181 920]\n",
            "2022-04-11 16:03:44 (INFO): Original: Loss: -9.646224021911621 Statstics: {'logit_target': -0.00023326536756940186, 'logit_best_non_target': -9.64645767211914, 'confidence_target': 0.9997667618366811, 'confidence_non_target': 6.465418820320856e-05, 'margin': 0.9997021076484779}\n",
            "\n",
            "  0% 0/500 [00:00<?, ?it/s]2022-04-11 16:03:45 (INFO): Initial: Loss: -9.646224021911621 Statstics: {'logit_target': -0.00023326536756940186, 'logit_best_non_target': -9.64645767211914, 'confidence_target': 0.9997667618366811, 'confidence_non_target': 6.465418820320856e-05, 'margin': 0.9997021076484779}\n",
            "\n",
            "2022-04-11 16:03:45 (INFO): \n",
            "Epoch: 0 Loss: -9.646224021911621 Statstics: {'logit_target': -0.0003034608089365065, 'logit_best_non_target': -9.37145709991455, 'confidence_target': 0.9996965852306375, 'confidence_non_target': 8.511927073381186e-05, 'margin': 0.9996114659599037}\n",
            "\n",
            "2022-04-11 16:03:45 (INFO): Gradient mean 0.37169349193573 std 0.1512388437986374 with base learning rate 0.05\n",
            "2022-04-11 16:03:45 (INFO): Cuda memory 0.03272581100463867\n",
            "  4% 20/500 [00:08<03:25,  2.33it/s]2022-04-11 16:03:53 (INFO): \n",
            "Epoch: 20 Loss: -0.27440762519836426 Statstics: {'logit_target': -0.5957375168800354, 'logit_best_non_target': -0.8700166344642639, 'confidence_target': 0.551155929134266, 'confidence_non_target': 0.4189445802710275, 'margin': 0.13221134886323854}\n",
            "\n",
            "2022-04-11 16:03:53 (INFO): Gradient mean 0.6216849684715271 std 0.27120500802993774 with base learning rate 0.05\n",
            "2022-04-11 16:03:53 (INFO): Cuda memory 0.03272533416748047\n",
            "  8% 40/500 [00:17<03:15,  2.36it/s]2022-04-11 16:04:02 (INFO): \n",
            "Epoch: 40 Loss: -0.27715229988098145 Statstics: {'logit_target': -0.5942661762237549, 'logit_best_non_target': -0.8712961673736572, 'confidence_target': 0.5519674641365689, 'confidence_non_target': 0.41840886969610297, 'margin': 0.13355859444046597}\n",
            "\n",
            "2022-04-11 16:04:02 (INFO): Gradient mean 0.6214743256568909 std 0.2710551619529724 with base learning rate 0.05\n",
            "2022-04-11 16:04:02 (INFO): Cuda memory 0.03272581100463867\n",
            " 12% 60/500 [00:25<03:07,  2.35it/s]2022-04-11 16:04:10 (INFO): \n",
            "Epoch: 60 Loss: -0.2772097587585449 Statstics: {'logit_target': -0.5942561030387878, 'logit_best_non_target': -0.8712930083274841, 'confidence_target': 0.5519730242349349, 'confidence_non_target': 0.41841019147112934, 'margin': 0.13356283276380554}\n",
            "\n",
            "2022-04-11 16:04:10 (INFO): Gradient mean 0.6213957071304321 std 0.27057403326034546 with base learning rate 0.05\n",
            "2022-04-11 16:04:10 (INFO): Cuda memory 0.03272581100463867\n",
            " 16% 80/500 [00:33<02:57,  2.36it/s]2022-04-11 16:04:19 (INFO): \n",
            "Epoch: 80 Loss: -0.2771329879760742 Statstics: {'logit_target': -0.5942193865776062, 'logit_best_non_target': -0.8713378310203552, 'confidence_target': 0.551993291103114, 'confidence_non_target': 0.4183914376199251, 'margin': 0.13360185348318893}\n",
            "\n",
            "2022-04-11 16:04:19 (INFO): Gradient mean 0.6218699216842651 std 0.27175021171569824 with base learning rate 0.05\n",
            "2022-04-11 16:04:19 (INFO): Cuda memory 0.03272533416748047\n",
            " 20% 100/500 [00:42<02:50,  2.35it/s]2022-04-11 16:04:27 (INFO): \n",
            "Epoch: 100 Loss: -0.2772548198699951 Statstics: {'logit_target': -0.5941592454910278, 'logit_best_non_target': -0.8714147806167603, 'confidence_target': 0.5520264895777112, 'confidence_non_target': 0.4183592438063273, 'margin': 0.1336672457713839}\n",
            "\n",
            "2022-04-11 16:04:27 (INFO): Gradient mean 0.6214284300804138 std 0.2703467607498169 with base learning rate 0.05\n",
            "2022-04-11 16:04:27 (INFO): Cuda memory 0.03272581100463867\n",
            " 24% 120/500 [00:50<02:41,  2.35it/s]2022-04-11 16:04:36 (INFO): \n",
            "Epoch: 120 Loss: -0.27724575996398926 Statstics: {'logit_target': -0.5941622853279114, 'logit_best_non_target': -0.8714109063148499, 'confidence_target': 0.5520248115097779, 'confidence_non_target': 0.41836086465948463, 'margin': 0.13366394685029326}\n",
            "\n",
            "2022-04-11 16:04:36 (INFO): Gradient mean 0.6208359003067017 std 0.2714536190032959 with base learning rate 0.05\n",
            "2022-04-11 16:04:36 (INFO): Cuda memory 0.03272581100463867\n",
            " 28% 140/500 [00:59<02:32,  2.36it/s]2022-04-11 16:04:44 (INFO): \n",
            "Epoch: 140 Loss: -0.27724719047546387 Statstics: {'logit_target': -0.5941628217697144, 'logit_best_non_target': -0.8714102506637573, 'confidence_target': 0.5520245153806722, 'confidence_non_target': 0.41836113895833255, 'margin': 0.13366337642233966}\n",
            "\n",
            "2022-04-11 16:04:44 (INFO): Gradient mean 0.6212138533592224 std 0.269907146692276 with base learning rate 0.05\n",
            "2022-04-11 16:04:44 (INFO): Cuda memory 0.03272581100463867\n",
            " 32% 160/500 [01:07<02:23,  2.37it/s]2022-04-11 16:04:53 (INFO): \n",
            "Epoch: 160 Loss: -0.2772483825683594 Statstics: {'logit_target': -0.594162106513977, 'logit_best_non_target': -0.8714112043380737, 'confidence_target': 0.5520249102195152, 'confidence_non_target': 0.4183607399782496, 'margin': 0.1336641702412656}\n",
            "\n",
            "2022-04-11 16:04:53 (INFO): Gradient mean 0.6217824220657349 std 0.2715383470058441 with base learning rate 0.05\n",
            "2022-04-11 16:04:53 (INFO): Cuda memory 0.03272533416748047\n",
            " 36% 180/500 [01:16<02:15,  2.36it/s]2022-04-11 16:05:01 (INFO): \n",
            "Epoch: 180 Loss: -0.27724289894104004 Statstics: {'logit_target': -0.5941646695137024, 'logit_best_non_target': -0.8714080452919006, 'confidence_target': 0.552023495381635, 'confidence_non_target': 0.41836206160123174, 'margin': 0.13366143378040324}\n",
            "\n",
            "2022-04-11 16:05:01 (INFO): Gradient mean 0.6216560006141663 std 0.27184101939201355 with base learning rate 0.05\n",
            "2022-04-11 16:05:01 (INFO): Cuda memory 0.03272581100463867\n",
            " 40% 200/500 [01:24<02:07,  2.35it/s]2022-04-11 16:05:10 (INFO): \n",
            "Epoch: 200 Loss: -0.27724528312683105 Statstics: {'logit_target': -0.5941636562347412, 'logit_best_non_target': -0.8714091777801514, 'confidence_target': 0.5520240547357124, 'confidence_non_target': 0.41836158781138066, 'margin': 0.1336624669243317}\n",
            "\n",
            "2022-04-11 16:05:10 (INFO): Gradient mean 0.6217296123504639 std 0.2718881666660309 with base learning rate 0.05\n",
            "2022-04-11 16:05:10 (INFO): Cuda memory 0.03272533416748047\n",
            " 44% 220/500 [01:33<01:59,  2.35it/s]2022-04-11 16:05:18 (INFO): \n",
            "Epoch: 220 Loss: -0.27724385261535645 Statstics: {'logit_target': -0.5941641330718994, 'logit_best_non_target': -0.8714084625244141, 'confidence_target': 0.5520237915101935, 'confidence_non_target': 0.41836188704701366, 'margin': 0.13366190446317988}\n",
            "\n",
            "2022-04-11 16:05:18 (INFO): Gradient mean 0.6207626461982727 std 0.27149561047554016 with base learning rate 0.05\n",
            "2022-04-11 16:05:18 (INFO): Cuda memory 0.03272533416748047\n",
            " 48% 240/500 [01:41<01:50,  2.35it/s]2022-04-11 16:05:27 (INFO): \n",
            "Epoch: 240 Loss: -0.27724552154541016 Statstics: {'logit_target': -0.5941634774208069, 'logit_best_non_target': -0.8714092373847961, 'confidence_target': 0.5520241534453142, 'confidence_non_target': 0.4183615628750876, 'margin': 0.1336625905702266}\n",
            "\n",
            "2022-04-11 16:05:27 (INFO): Gradient mean 0.6219009757041931 std 0.2717243731021881 with base learning rate 0.05\n",
            "2022-04-11 16:05:27 (INFO): Cuda memory 0.03272581100463867\n",
            " 52% 260/500 [01:50<01:41,  2.36it/s]2022-04-11 16:05:35 (INFO): \n",
            "Epoch: 260 Loss: -0.27724599838256836 Statstics: {'logit_target': -0.5941634178161621, 'logit_best_non_target': -0.8714094161987305, 'confidence_target': 0.5520241863485188, 'confidence_non_target': 0.41836148806621726, 'margin': 0.13366269828230154}\n",
            "\n",
            "2022-04-11 16:05:35 (INFO): Gradient mean 0.6213138699531555 std 0.27078405022621155 with base learning rate 0.05\n",
            "2022-04-11 16:05:35 (INFO): Cuda memory 0.03272581100463867\n",
            " 56% 280/500 [01:58<01:32,  2.38it/s]2022-04-11 16:05:44 (INFO): \n",
            "Epoch: 280 Loss: -0.27724671363830566 Statstics: {'logit_target': -0.5941628217697144, 'logit_best_non_target': -0.8714102506637573, 'confidence_target': 0.5520245153806722, 'confidence_non_target': 0.41836113895833255, 'margin': 0.13366337642233966}\n",
            "\n",
            "2022-04-11 16:05:44 (INFO): Gradient mean 0.6210404634475708 std 0.27080878615379333 with base learning rate 0.05\n",
            "2022-04-11 16:05:44 (INFO): Cuda memory 0.03272581100463867\n",
            " 60% 300/500 [02:07<01:25,  2.35it/s]2022-04-11 16:05:52 (INFO): \n",
            "Epoch: 300 Loss: -0.27724599838256836 Statstics: {'logit_target': -0.5941634774208069, 'logit_best_non_target': -0.8714092373847961, 'confidence_target': 0.5520241534453142, 'confidence_non_target': 0.4183615628750876, 'margin': 0.1336625905702266}\n",
            "\n",
            "2022-04-11 16:05:52 (INFO): Gradient mean 0.621903121471405 std 0.2724526524543762 with base learning rate 0.05\n",
            "2022-04-11 16:05:52 (INFO): Cuda memory 0.03272533416748047\n",
            " 64% 320/500 [02:15<01:16,  2.36it/s]2022-04-11 16:06:01 (INFO): \n",
            "Epoch: 320 Loss: -0.27724242210388184 Statstics: {'logit_target': -0.5941646099090576, 'logit_best_non_target': -0.8714077472686768, 'confidence_target': 0.5520235282848003, 'confidence_non_target': 0.41836218628286065, 'margin': 0.13366134200193963}\n",
            "\n",
            "2022-04-11 16:06:01 (INFO): Gradient mean 0.6214991211891174 std 0.2710627019405365 with base learning rate 0.05\n",
            "2022-04-11 16:06:01 (INFO): Cuda memory 0.03272581100463867\n",
            " 68% 340/500 [02:24<01:07,  2.36it/s]2022-04-11 16:06:09 (INFO): \n",
            "Epoch: 340 Loss: -0.27724432945251465 Statstics: {'logit_target': -0.5941643714904785, 'logit_best_non_target': -0.8714084625244141, 'confidence_target': 0.5520236598974813, 'confidence_non_target': 0.41836188704701366, 'margin': 0.1336617728504676}\n",
            "\n",
            "2022-04-11 16:06:09 (INFO): Gradient mean 0.6212318539619446 std 0.2713199555873871 with base learning rate 0.05\n",
            "2022-04-11 16:06:09 (INFO): Cuda memory 0.03272581100463867\n",
            " 72% 360/500 [02:32<00:59,  2.35it/s]2022-04-11 16:06:17 (INFO): \n",
            "Epoch: 360 Loss: -0.27724313735961914 Statstics: {'logit_target': -0.5941649675369263, 'logit_best_non_target': -0.8714076280593872, 'confidence_target': 0.5520233308658378, 'confidence_non_target': 0.4183622361555226, 'margin': 0.13366109471031518}\n",
            "\n",
            "2022-04-11 16:06:17 (INFO): Gradient mean 0.621460497379303 std 0.2705019414424896 with base learning rate 0.05\n",
            "2022-04-11 16:06:17 (INFO): Cuda memory 0.03272533416748047\n",
            " 76% 380/500 [02:41<00:51,  2.35it/s]2022-04-11 16:06:26 (INFO): \n",
            "Epoch: 380 Loss: -0.2772367000579834 Statstics: {'logit_target': -0.5941678881645203, 'logit_best_non_target': -0.8714038729667664, 'confidence_target': 0.5520217186136195, 'confidence_non_target': 0.41836380714741805, 'margin': 0.13365791146620143}\n",
            "\n",
            "2022-04-11 16:06:26 (INFO): Gradient mean 0.6218119263648987 std 0.27125608921051025 with base learning rate 0.05\n",
            "2022-04-11 16:06:26 (INFO): Cuda memory 0.03272581100463867\n",
            " 80% 399/500 [02:49<00:42,  2.35it/s]2022-04-11 16:06:34 (INFO): Loading search space of epoch 21 (margin=0.13210362529603636) for fine tuning\n",
            "\n",
            " 80% 400/500 [02:49<00:42,  2.36it/s]2022-04-11 16:06:34 (INFO): \n",
            "Epoch: 400 Loss: -0.2740471363067627 Statstics: {'logit_target': -0.5951055884361267, 'logit_best_non_target': -0.8705548644065857, 'confidence_target': 0.5515043303136284, 'confidence_non_target': 0.41871915242520247, 'margin': 0.13278517788842598}\n",
            "\n",
            "2022-04-11 16:06:34 (INFO): Gradient mean 0.6215167045593262 std 0.26896995306015015 with base learning rate 0.05\n",
            "2022-04-11 16:06:34 (INFO): Cuda memory 0.03267097473144531\n",
            " 84% 420/500 [02:58<00:33,  2.37it/s]2022-04-11 16:06:43 (INFO): \n",
            "Epoch: 420 Loss: -0.2756526470184326 Statstics: {'logit_target': -0.5949861407279968, 'logit_best_non_target': -0.8705460429191589, 'confidence_target': 0.551570210176429, 'confidence_non_target': 0.41872284616723304, 'margin': 0.132847364009196}\n",
            "\n",
            "2022-04-11 16:06:43 (INFO): Gradient mean 0.6215524673461914 std 0.2695935070514679 with base learning rate 0.05\n",
            "2022-04-11 16:06:43 (INFO): Cuda memory 0.03267097473144531\n",
            " 88% 440/500 [03:06<00:25,  2.38it/s]2022-04-11 16:06:51 (INFO): \n",
            "Epoch: 440 Loss: -0.27541494369506836 Statstics: {'logit_target': -0.5950331091880798, 'logit_best_non_target': -0.8704466223716736, 'confidence_target': 0.5515443043814118, 'confidence_non_target': 0.4187644778913337, 'margin': 0.13277982649007808}\n",
            "\n",
            "2022-04-11 16:06:51 (INFO): Gradient mean 0.6215912103652954 std 0.2696388065814972 with base learning rate 0.05\n",
            "2022-04-11 16:06:51 (INFO): Cuda memory 0.03267097473144531\n",
            " 92% 460/500 [03:14<00:16,  2.37it/s]2022-04-11 16:07:00 (INFO): \n",
            "Epoch: 460 Loss: -0.2754642963409424 Statstics: {'logit_target': -0.5949828028678894, 'logit_best_non_target': -0.8704909682273865, 'confidence_target': 0.5515720512437027, 'confidence_non_target': 0.41874590783397503, 'margin': 0.13282614340972765}\n",
            "\n",
            "2022-04-11 16:07:00 (INFO): Gradient mean 0.6216335892677307 std 0.26967325806617737 with base learning rate 0.05\n",
            "2022-04-11 16:07:00 (INFO): Cuda memory 0.03267097473144531\n",
            " 96% 480/500 [03:23<00:08,  2.37it/s]2022-04-11 16:07:08 (INFO): \n",
            "Epoch: 480 Loss: -0.2772247791290283 Statstics: {'logit_target': -0.5941929817199707, 'logit_best_non_target': -0.8713994026184082, 'confidence_target': 0.5520078665998125, 'confidence_non_target': 0.41836567738355673, 'margin': 0.13364218921625576}\n",
            "\n",
            "2022-04-11 16:07:08 (INFO): Gradient mean 0.6217324137687683 std 0.26977354288101196 with base learning rate 0.05\n",
            "2022-04-11 16:07:08 (INFO): Cuda memory 0.03267097473144531\n",
            "100% 500/500 [03:31<00:00,  2.36it/s]\n",
            "2022-04-11 16:07:18 (INFO): Cuda Memory before local evaluation on clean adjacency 0.03243398666381836\n",
            "2022-04-11 16:07:18 (INFO): Cuda Memory before local evaluation on perturbed adjacency 0.03250741958618164\n",
            "2022-04-11 16:07:18 (INFO): Evaluated model Vanilla GCN using LocalPRBCD with pert. edges for node 475 and budget 1: \n",
            "2022-04-11 16:07:18 (INFO): {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 1, 'degree': 2, 'logits': [[-0.8856097459793091, -1.534730315208435, 3.812166213989258, -1.9702682495117188, -1.6980338096618652, 2.5939292907714844, -1.3224011659622192]], 'initial_logits': [[-1.4388686418533325, -1.9843776226043701, 8.674994468688965, -2.0514442920684814, -1.4827415943145752, -0.9712307453155518, -1.355878233909607]], 'larget': 2, 'node_id': 475, 'perturbed_edges': [[475], [1244]], 'logit_target': -0.279634028673172, 'logit_best_non_target': -1.497870922088623, 'confidence_target': 0.7560603872535082, 'confidence_non_target': 0.2236057277243299, 'margin': 0.5324546595291784, 'initial_logit_target': -0.00023326536756940186, 'initial_logit_best_non_target': -9.646458625793457, 'initial_confidence_target': 0.9997667618366811, 'initial_confidence_non_target': 6.465412654419923e-05, 'initial_margin': 0.9997021077101369}\n",
            "2022-04-11 16:07:18 (INFO): Completed attack and evaluation of Vanilla GCN using LocalPRBCD with pert. edges for node 475 and budget 1\n",
            "2022-04-11 16:07:19 (INFO): Original: Loss: -0.006889045238494873 Statstics: {'logit_target': -1.3817017078399658, 'logit_best_non_target': -1.3885908126831055, 'confidence_target': 0.25115080391890404, 'confidence_non_target': 0.24942654581609308, 'margin': 0.0017242581028109605}\n",
            "\n",
            "  0% 0/500 [00:00<?, ?it/s]2022-04-11 16:07:19 (INFO): Initial: Loss: -0.0068891048431396484 Statstics: {'logit_target': -1.3817017078399658, 'logit_best_non_target': -1.3885908126831055, 'confidence_target': 0.25115080391890404, 'confidence_non_target': 0.24942654581609308, 'margin': 0.0017242581028109605}\n",
            "\n",
            "2022-04-11 16:07:19 (INFO): \n",
            "Epoch: 0 Loss: -0.0068891048431396484 Statstics: {'logit_target': -1.4223153591156006, 'logit_best_non_target': -1.325409173965454, 'confidence_target': 0.24115500954575997, 'confidence_non_target': 0.26569422168590107, 'margin': -0.024539212140141098}\n",
            "\n",
            "2022-04-11 16:07:19 (INFO): Gradient mean 0.010098437778651714 std 0.00734969275072217 with base learning rate 0.75\n",
            "2022-04-11 16:07:19 (INFO): Cuda memory 0.033168792724609375\n",
            "  4% 20/500 [00:08<03:23,  2.36it/s]2022-04-11 16:07:27 (INFO): \n",
            "Epoch: 20 Loss: 0.29589003324508667 Statstics: {'logit_target': -1.597354531288147, 'logit_best_non_target': -1.2962701320648193, 'confidence_target': 0.2024313360263165, 'confidence_non_target': 0.27355019870883907, 'margin': -0.07111886268252257}\n",
            "\n",
            "2022-04-11 16:07:27 (INFO): Gradient mean 0.008339326828718185 std 0.008448041044175625 with base learning rate 0.75\n",
            "2022-04-11 16:07:27 (INFO): Cuda memory 0.033168792724609375\n",
            "  8% 40/500 [00:16<03:14,  2.36it/s]2022-04-11 16:07:36 (INFO): \n",
            "Epoch: 40 Loss: 0.33655136823654175 Statstics: {'logit_target': -1.6458992958068848, 'logit_best_non_target': -1.3100261688232422, 'confidence_target': 0.1928390654297839, 'confidence_non_target': 0.2698129956037107, 'margin': -0.07697393017392679}\n",
            "\n",
            "2022-04-11 16:07:36 (INFO): Gradient mean 0.008363853208720684 std 0.007699477020651102 with base learning rate 0.75\n",
            "2022-04-11 16:07:36 (INFO): Cuda memory 0.03316974639892578\n",
            " 12% 60/500 [00:25<03:06,  2.36it/s]2022-04-11 16:07:44 (INFO): \n",
            "Epoch: 60 Loss: 0.34291818737983704 Statstics: {'logit_target': -1.6554114818572998, 'logit_best_non_target': -1.3122901916503906, 'confidence_target': 0.19101344096684628, 'confidence_non_target': 0.2692028238046782, 'margin': -0.07818938283783192}\n",
            "\n",
            "2022-04-11 16:07:44 (INFO): Gradient mean 0.00843697227537632 std 0.007815171964466572 with base learning rate 0.75\n",
            "2022-04-11 16:07:44 (INFO): Cuda memory 0.033168792724609375\n",
            " 16% 80/500 [00:33<02:58,  2.36it/s]2022-04-11 16:07:53 (INFO): \n",
            "Epoch: 80 Loss: 0.3399929404258728 Statstics: {'logit_target': -1.6555202007293701, 'logit_best_non_target': -1.315988302230835, 'confidence_target': 0.19099267532982286, 'confidence_non_target': 0.26820912053845397, 'margin': -0.07721644520863111}\n",
            "\n",
            "2022-04-11 16:07:53 (INFO): Gradient mean 0.00837610475718975 std 0.0076619298197329044 with base learning rate 0.75\n",
            "2022-04-11 16:07:53 (INFO): Cuda memory 0.033168792724609375\n",
            " 20% 100/500 [00:42<02:48,  2.38it/s]2022-04-11 16:08:01 (INFO): \n",
            "Epoch: 100 Loss: 0.3405899405479431 Statstics: {'logit_target': -1.654977798461914, 'logit_best_non_target': -1.3049676418304443, 'confidence_target': 0.1910962982901131, 'confidence_non_target': 0.271181309834435, 'margin': -0.0800850115443219}\n",
            "\n",
            "2022-04-11 16:08:01 (INFO): Gradient mean 0.008388380520045757 std 0.007688364014029503 with base learning rate 0.75\n",
            "2022-04-11 16:08:01 (INFO): Cuda memory 0.033168792724609375\n",
            " 24% 120/500 [00:50<02:41,  2.35it/s]2022-04-11 16:08:10 (INFO): \n",
            "Epoch: 120 Loss: 0.34892597794532776 Statstics: {'logit_target': -1.653691053390503, 'logit_best_non_target': -1.3052414655685425, 'confidence_target': 0.1913423487782802, 'confidence_non_target': 0.2711070641200572, 'margin': -0.07976471534177698}\n",
            "\n",
            "2022-04-11 16:08:10 (INFO): Gradient mean 0.00833620224148035 std 0.0072906105779111385 with base learning rate 0.75\n",
            "2022-04-11 16:08:10 (INFO): Cuda memory 0.03316974639892578\n",
            " 28% 140/500 [00:59<02:33,  2.34it/s]2022-04-11 16:08:18 (INFO): \n",
            "Epoch: 140 Loss: 0.34915655851364136 Statstics: {'logit_target': -1.6527138948440552, 'logit_best_non_target': -1.3038829565048218, 'confidence_target': 0.19152941196999995, 'confidence_non_target': 0.2714756158076254, 'margin': -0.07994620383762546}\n",
            "\n",
            "2022-04-11 16:08:18 (INFO): Gradient mean 0.008388749323785305 std 0.007412697654217482 with base learning rate 0.75\n",
            "2022-04-11 16:08:18 (INFO): Cuda memory 0.033168792724609375\n",
            " 32% 160/500 [01:07<02:23,  2.36it/s]2022-04-11 16:08:27 (INFO): \n",
            "Epoch: 160 Loss: 0.34909459948539734 Statstics: {'logit_target': -1.6515605449676514, 'logit_best_non_target': -1.3045313358306885, 'confidence_target': 0.19175043983034973, 'confidence_non_target': 0.27129965368212544, 'margin': -0.07954921385177571}\n",
            "\n",
            "2022-04-11 16:08:27 (INFO): Gradient mean 0.00835452415049076 std 0.007295024115592241 with base learning rate 0.75\n",
            "2022-04-11 16:08:27 (INFO): Cuda memory 0.033168792724609375\n",
            " 36% 180/500 [01:16<02:16,  2.34it/s]2022-04-11 16:08:35 (INFO): \n",
            "Epoch: 180 Loss: 0.3467360734939575 Statstics: {'logit_target': -1.6511757373809814, 'logit_best_non_target': -1.303334355354309, 'confidence_target': 0.19182424105306825, 'confidence_non_target': 0.2716245885023532, 'margin': -0.07980034744928496}\n",
            "\n",
            "2022-04-11 16:08:35 (INFO): Gradient mean 0.008380807936191559 std 0.007456380873918533 with base learning rate 0.75\n",
            "2022-04-11 16:08:35 (INFO): Cuda memory 0.033168792724609375\n",
            " 40% 200/500 [01:24<02:06,  2.37it/s]2022-04-11 16:08:44 (INFO): \n",
            "Epoch: 200 Loss: 0.34801873564720154 Statstics: {'logit_target': -1.6503522396087646, 'logit_best_non_target': -1.303113579750061, 'confidence_target': 0.1919822729487589, 'confidence_non_target': 0.2716845632052405, 'margin': -0.0797022902564816}\n",
            "\n",
            "2022-04-11 16:08:44 (INFO): Gradient mean 0.008379601873457432 std 0.007379118353128433 with base learning rate 0.75\n",
            "2022-04-11 16:08:44 (INFO): Cuda memory 0.033168792724609375\n",
            " 44% 220/500 [01:33<01:59,  2.35it/s]2022-04-11 16:08:52 (INFO): \n",
            "Epoch: 220 Loss: 0.3475065529346466 Statstics: {'logit_target': -1.649875521659851, 'logit_best_non_target': -1.3024144172668457, 'confidence_target': 0.19207381616255967, 'confidence_non_target': 0.27187458127811565, 'margin': -0.07980076511555598}\n",
            "\n",
            "2022-04-11 16:08:52 (INFO): Gradient mean 0.008386163972318172 std 0.007430324796587229 with base learning rate 0.75\n",
            "2022-04-11 16:08:52 (INFO): Cuda memory 0.033168792724609375\n",
            " 48% 240/500 [01:41<01:51,  2.33it/s]2022-04-11 16:09:01 (INFO): \n",
            "Epoch: 240 Loss: 0.368099570274353 Statstics: {'logit_target': -1.6621663570404053, 'logit_best_non_target': -1.269426941871643, 'confidence_target': 0.18972751703220053, 'confidence_non_target': 0.2809926007426861, 'margin': -0.09126508371048558}\n",
            "\n",
            "2022-04-11 16:09:01 (INFO): Gradient mean 0.008450456894934177 std 0.007362966425716877 with base learning rate 0.75\n",
            "2022-04-11 16:09:01 (INFO): Cuda memory 0.033168792724609375\n",
            " 52% 260/500 [01:50<01:43,  2.31it/s]2022-04-11 16:09:09 (INFO): \n",
            "Epoch: 260 Loss: 0.5201839208602905 Statstics: {'logit_target': -1.684597134590149, 'logit_best_non_target': -1.1621513366699219, 'confidence_target': 0.1855191561539421, 'confidence_non_target': 0.31281249149233703, 'margin': -0.12729333533839493}\n",
            "\n",
            "2022-04-11 16:09:09 (INFO): Gradient mean 0.009137669578194618 std 0.007430585566908121 with base learning rate 0.75\n",
            "2022-04-11 16:09:09 (INFO): Cuda memory 0.033168792724609375\n",
            " 56% 280/500 [01:58<01:33,  2.34it/s]2022-04-11 16:09:18 (INFO): \n",
            "Epoch: 280 Loss: 0.5479859709739685 Statstics: {'logit_target': -1.679481863975525, 'logit_best_non_target': -1.1300687789916992, 'confidence_target': 0.18647056813218116, 'confidence_non_target': 0.3230110392846358, 'margin': -0.13654047115245466}\n",
            "\n",
            "2022-04-11 16:09:18 (INFO): Gradient mean 0.009275679476559162 std 0.007040513213723898 with base learning rate 0.75\n",
            "2022-04-11 16:09:18 (INFO): Cuda memory 0.033168792724609375\n",
            " 60% 300/500 [02:07<01:25,  2.35it/s]2022-04-11 16:09:26 (INFO): \n",
            "Epoch: 300 Loss: 0.5533965826034546 Statstics: {'logit_target': -1.6783602237701416, 'logit_best_non_target': -1.1245465278625488, 'confidence_target': 0.18667983835952942, 'confidence_non_target': 0.3247997215919741, 'margin': -0.1381198832324447}\n",
            "\n",
            "2022-04-11 16:09:26 (INFO): Gradient mean 0.00936164241284132 std 0.0069734263233840466 with base learning rate 0.75\n",
            "2022-04-11 16:09:26 (INFO): Cuda memory 0.03316974639892578\n",
            " 64% 320/500 [02:15<01:16,  2.37it/s]2022-04-11 16:09:35 (INFO): \n",
            "Epoch: 320 Loss: 0.5495160818099976 Statstics: {'logit_target': -1.6759973764419556, 'logit_best_non_target': -1.12631356716156, 'confidence_target': 0.18712145584871137, 'confidence_non_target': 0.324226294502921, 'margin': -0.13710483865420964}\n",
            "\n",
            "2022-04-11 16:09:35 (INFO): Gradient mean 0.009321530349552631 std 0.006808486767113209 with base learning rate 0.75\n",
            "2022-04-11 16:09:35 (INFO): Cuda memory 0.03316831588745117\n",
            " 68% 340/500 [02:24<01:07,  2.35it/s]2022-04-11 16:09:43 (INFO): \n",
            "Epoch: 340 Loss: 0.5510122776031494 Statstics: {'logit_target': -1.675385594367981, 'logit_best_non_target': -1.12489914894104, 'confidence_target': 0.187235968425855, 'confidence_non_target': 0.324685210554542, 'margin': -0.13744924212868703}\n",
            "\n",
            "2022-04-11 16:09:43 (INFO): Gradient mean 0.009404012933373451 std 0.006689907982945442 with base learning rate 0.75\n",
            "2022-04-11 16:09:43 (INFO): Cuda memory 0.03316831588745117\n",
            " 72% 360/500 [02:32<00:58,  2.39it/s]2022-04-11 16:09:52 (INFO): \n",
            "Epoch: 360 Loss: 0.5511919856071472 Statstics: {'logit_target': -1.6746447086334229, 'logit_best_non_target': -1.1233930587768555, 'confidence_target': 0.18737474028455534, 'confidence_non_target': 0.3251745841845156, 'margin': -0.13779984389996028}\n",
            "\n",
            "2022-04-11 16:09:52 (INFO): Gradient mean 0.0093570901080966 std 0.006817166227847338 with base learning rate 0.75\n",
            "2022-04-11 16:09:52 (INFO): Cuda memory 0.033168792724609375\n",
            " 76% 380/500 [02:41<00:50,  2.36it/s]2022-04-11 16:10:00 (INFO): \n",
            "Epoch: 380 Loss: 0.5522537231445312 Statstics: {'logit_target': -1.6741031408309937, 'logit_best_non_target': -1.1225028038024902, 'confidence_target': 0.18747624389394443, 'confidence_non_target': 0.3254642013730663, 'margin': -0.13798795747912188}\n",
            "\n",
            "2022-04-11 16:10:00 (INFO): Gradient mean 0.009437748230993748 std 0.006890127435326576 with base learning rate 0.75\n",
            "2022-04-11 16:10:00 (INFO): Cuda memory 0.03316974639892578\n",
            " 80% 399/500 [02:49<00:42,  2.37it/s]2022-04-11 16:10:08 (INFO): Loading search space of epoch 315 (margin=-0.13880522059769612) for fine tuning\n",
            "\n",
            " 80% 400/500 [02:49<00:42,  2.37it/s]2022-04-11 16:10:09 (INFO): \n",
            "Epoch: 400 Loss: 0.5555240511894226 Statstics: {'logit_target': -1.6762477159500122, 'logit_best_non_target': -1.1263939142227173, 'confidence_target': 0.1870746178184567, 'confidence_non_target': 0.32420024491952554, 'margin': -0.13712562710106885}\n",
            "\n",
            "2022-04-11 16:10:09 (INFO): Gradient mean 0.009361917153000832 std 0.006860847119241953 with base learning rate 0.75\n",
            "2022-04-11 16:10:09 (INFO): Cuda memory 0.033113956451416016\n",
            " 84% 420/500 [02:58<00:33,  2.39it/s]2022-04-11 16:10:17 (INFO): \n",
            "Epoch: 420 Loss: 0.5505881309509277 Statstics: {'logit_target': -1.6762235164642334, 'logit_best_non_target': -1.1254576444625854, 'confidence_target': 0.1870791449827875, 'confidence_non_target': 0.3245039259465683, 'margin': -0.1374247809637808}\n",
            "\n",
            "2022-04-11 16:10:17 (INFO): Gradient mean 0.009330974891781807 std 0.0067824372090399265 with base learning rate 0.75\n",
            "2022-04-11 16:10:17 (INFO): Cuda memory 0.033113956451416016\n",
            " 88% 440/500 [03:06<00:25,  2.37it/s]2022-04-11 16:10:26 (INFO): \n",
            "Epoch: 440 Loss: 0.5510849952697754 Statstics: {'logit_target': -1.6760952472686768, 'logit_best_non_target': -1.125075340270996, 'confidence_target': 0.18710314301329098, 'confidence_non_target': 0.32462800887484533, 'margin': -0.13752486586155435}\n",
            "\n",
            "2022-04-11 16:10:26 (INFO): Gradient mean 0.009325440973043442 std 0.006772187538444996 with base learning rate 0.75\n",
            "2022-04-11 16:10:26 (INFO): Cuda memory 0.033113956451416016\n",
            " 92% 460/500 [03:14<00:16,  2.39it/s]2022-04-11 16:10:34 (INFO): \n",
            "Epoch: 460 Loss: 0.5508437156677246 Statstics: {'logit_target': -1.6759891510009766, 'logit_best_non_target': -1.124969482421875, 'confidence_target': 0.1871229950115325, 'confidence_non_target': 0.3246623751165654, 'margin': -0.13753938010503292}\n",
            "\n",
            "2022-04-11 16:10:34 (INFO): Gradient mean 0.009313150309026241 std 0.006773218512535095 with base learning rate 0.75\n",
            "2022-04-11 16:10:34 (INFO): Cuda memory 0.033113956451416016\n",
            " 96% 480/500 [03:23<00:08,  2.39it/s]2022-04-11 16:10:42 (INFO): \n",
            "Epoch: 480 Loss: 0.5511181354522705 Statstics: {'logit_target': -1.6758676767349243, 'logit_best_non_target': -1.1246951818466187, 'confidence_target': 0.18714572702066212, 'confidence_non_target': 0.3247514424078702, 'margin': -0.13760571538720806}\n",
            "\n",
            "2022-04-11 16:10:42 (INFO): Gradient mean 0.009334256872534752 std 0.006792247761040926 with base learning rate 0.75\n",
            "2022-04-11 16:10:42 (INFO): Cuda memory 0.033113956451416016\n",
            "100% 500/500 [03:31<00:00,  2.36it/s]\n",
            "2022-04-11 16:10:52 (INFO): Cuda Memory before local evaluation on clean adjacency 0.032581329345703125\n",
            "2022-04-11 16:10:53 (INFO): Cuda Memory before local evaluation on perturbed adjacency 0.032654762268066406\n",
            "2022-04-11 16:10:53 (INFO): Evaluated model Vanilla GCN using LocalPRBCD with pert. edges for node 2210 and budget 15: \n",
            "2022-04-11 16:10:53 (INFO): {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 15, 'degree': 30, 'logits': [[0.18504709005355835, 0.2727407217025757, -0.6521108150482178, -0.27883657813072205, -0.6258774399757385, 0.7944414615631104, -0.61902916431427]], 'initial_logits': [[-0.0848335325717926, 0.5635748505592346, -0.8179147243499756, 0.0325467586517334, -0.46229881048202515, 0.5566858053207397, -0.7532972097396851]], 'larget': 1, 'node_id': 2210, 'perturbed_edges': [[2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210], [264, 441, 443, 1103, 1198, 1233, 1283, 1294, 1343, 1536, 1654, 1655, 1661, 2206, 2567]], 'logit_target': -1.6846612691879272, 'logit_best_non_target': -1.1629605293273926, 'confidence_target': 0.18550725833901685, 'confidence_non_target': 0.3125594683071146, 'margin': -0.12705220996809777, 'initial_logit_target': -1.3817017078399658, 'initial_logit_best_non_target': -1.3885908126831055, 'initial_confidence_target': 0.25115080391890404, 'initial_confidence_non_target': 0.24942654581609308, 'initial_margin': 0.0017242581028109605}\n",
            "2022-04-11 16:10:53 (INFO): Completed attack and evaluation of Vanilla GCN using LocalPRBCD with pert. edges for node 2210 and budget 15\n",
            "2022-04-11 16:10:53 (INFO): Original: Loss: -6.625917911529541 Statstics: {'logit_target': -0.004901299253106117, 'logit_best_non_target': -6.630819320678711, 'confidence_target': 0.9951106925143317, 'confidence_non_target': 0.001319081892087611, 'margin': 0.9937916106222441}\n",
            "\n",
            "  0% 0/500 [00:00<?, ?it/s]2022-04-11 16:10:53 (INFO): Initial: Loss: -6.625917911529541 Statstics: {'logit_target': -0.004901299253106117, 'logit_best_non_target': -6.630819320678711, 'confidence_target': 0.9951106925143317, 'confidence_non_target': 0.001319081892087611, 'margin': 0.9937916106222441}\n",
            "\n",
            "2022-04-11 16:10:53 (INFO): \n",
            "Epoch: 0 Loss: -6.625917911529541 Statstics: {'logit_target': -0.006641576066613197, 'logit_best_non_target': -6.348412036895752, 'confidence_target': 0.993380430453433, 'confidence_non_target': 0.0017495231097543647, 'margin': 0.9916309073436786}\n",
            "\n",
            "2022-04-11 16:10:53 (INFO): Gradient mean 0.45199552178382874 std 0.10803080350160599 with base learning rate 0.05\n",
            "2022-04-11 16:10:53 (INFO): Cuda memory 0.03317070007324219\n",
            "  4% 20/500 [00:08<03:24,  2.35it/s]2022-04-11 16:11:02 (INFO): \n",
            "Epoch: 20 Loss: -2.888176441192627 Statstics: {'logit_target': -0.1798814833164215, 'logit_best_non_target': -3.064575433731079, 'confidence_target': 0.8353692107330402, 'confidence_non_target': 0.046673653717467505, 'margin': 0.7886955570155727}\n",
            "\n",
            "2022-04-11 16:11:02 (INFO): Gradient mean 0.4345478415489197 std 0.1382906138896942 with base learning rate 0.05\n",
            "2022-04-11 16:11:02 (INFO): Cuda memory 0.03317070007324219\n",
            "  8% 40/500 [00:16<03:14,  2.36it/s]2022-04-11 16:11:10 (INFO): \n",
            "Epoch: 40 Loss: -2.837538719177246 Statstics: {'logit_target': -0.1814514845609665, 'logit_best_non_target': -3.018683433532715, 'confidence_target': 0.8340587090463626, 'confidence_non_target': 0.04886551072296757, 'margin': 0.7851931983233951}\n",
            "\n",
            "2022-04-11 16:11:10 (INFO): Gradient mean 0.4292251169681549 std 0.1363023966550827 with base learning rate 0.05\n",
            "2022-04-11 16:11:10 (INFO): Cuda memory 0.03317070007324219\n",
            " 12% 60/500 [00:25<03:07,  2.34it/s]2022-04-11 16:11:19 (INFO): \n",
            "Epoch: 60 Loss: -2.848086357116699 Statstics: {'logit_target': -0.17900286614894867, 'logit_best_non_target': -3.027158260345459, 'confidence_target': 0.8361035029964252, 'confidence_non_target': 0.048453133861878765, 'margin': 0.7876503691345464}\n",
            "\n",
            "2022-04-11 16:11:19 (INFO): Gradient mean 0.42822521924972534 std 0.12919825315475464 with base learning rate 0.05\n",
            "2022-04-11 16:11:19 (INFO): Cuda memory 0.03317070007324219\n",
            " 16% 80/500 [00:33<02:58,  2.35it/s]2022-04-11 16:11:27 (INFO): \n",
            "Epoch: 80 Loss: -2.848013401031494 Statstics: {'logit_target': -0.17892342805862427, 'logit_best_non_target': -3.0269503593444824, 'confidence_target': 0.8361699241001647, 'confidence_non_target': 0.048463208364122365, 'margin': 0.7877067157360423}\n",
            "\n",
            "2022-04-11 16:11:27 (INFO): Gradient mean 0.4289548397064209 std 0.13281898200511932 with base learning rate 0.05\n",
            "2022-04-11 16:11:27 (INFO): Cuda memory 0.03317117691040039\n",
            " 20% 100/500 [00:42<02:52,  2.33it/s]2022-04-11 16:11:36 (INFO): \n",
            "Epoch: 100 Loss: -2.8480660915374756 Statstics: {'logit_target': -0.17891396582126617, 'logit_best_non_target': -3.0269150733947754, 'confidence_target': 0.8361778361758911, 'confidence_non_target': 0.04846491846462642, 'margin': 0.7877129177112647}\n",
            "\n",
            "2022-04-11 16:11:36 (INFO): Gradient mean 0.42849522829055786 std 0.13200554251670837 with base learning rate 0.05\n",
            "2022-04-11 16:11:36 (INFO): Cuda memory 0.03317070007324219\n",
            " 24% 120/500 [00:51<02:43,  2.32it/s]2022-04-11 16:11:44 (INFO): \n",
            "Epoch: 120 Loss: -2.848085880279541 Statstics: {'logit_target': -0.17890867590904236, 'logit_best_non_target': -3.0269336700439453, 'confidence_target': 0.8361822594949475, 'confidence_non_target': 0.04846401718792107, 'margin': 0.7877182423070265}\n",
            "\n",
            "2022-04-11 16:11:44 (INFO): Gradient mean 0.42808544635772705 std 0.13208241760730743 with base learning rate 0.05\n",
            "2022-04-11 16:11:44 (INFO): Cuda memory 0.033170223236083984\n",
            " 28% 140/500 [00:59<02:32,  2.36it/s]2022-04-11 16:11:53 (INFO): \n",
            "Epoch: 140 Loss: -2.847989797592163 Statstics: {'logit_target': -0.1789148598909378, 'logit_best_non_target': -3.0269017219543457, 'confidence_target': 0.8361770885749819, 'confidence_non_target': 0.048465565545417956, 'margin': 0.7877115230295639}\n",
            "\n",
            "2022-04-11 16:11:53 (INFO): Gradient mean 0.4281481206417084 std 0.1314390003681183 with base learning rate 0.05\n",
            "2022-04-11 16:11:53 (INFO): Cuda memory 0.03317070007324219\n",
            " 32% 160/500 [01:08<02:23,  2.38it/s]2022-04-11 16:12:01 (INFO): \n",
            "Epoch: 160 Loss: -2.8480007648468018 Statstics: {'logit_target': -0.17891237139701843, 'logit_best_non_target': -3.0269153118133545, 'confidence_target': 0.8361791693991714, 'confidence_non_target': 0.048464906909690804, 'margin': 0.7877142624894805}\n",
            "\n",
            "2022-04-11 16:12:01 (INFO): Gradient mean 0.42858654260635376 std 0.13245241343975067 with base learning rate 0.05\n",
            "2022-04-11 16:12:01 (INFO): Cuda memory 0.03317070007324219\n",
            " 36% 180/500 [01:16<02:16,  2.35it/s]2022-04-11 16:12:10 (INFO): \n",
            "Epoch: 180 Loss: -2.848047971725464 Statstics: {'logit_target': -0.17890499532222748, 'logit_best_non_target': -3.0269553661346436, 'confidence_target': 0.8361853371420104, 'confidence_non_target': 0.04846296571961498, 'margin': 0.7877223714223954}\n",
            "\n",
            "2022-04-11 16:12:10 (INFO): Gradient mean 0.4285794496536255 std 0.13393162190914154 with base learning rate 0.05\n",
            "2022-04-11 16:12:10 (INFO): Cuda memory 0.03317070007324219\n",
            " 40% 200/500 [01:25<02:08,  2.33it/s]2022-04-11 16:12:18 (INFO): \n",
            "Epoch: 200 Loss: -2.848072052001953 Statstics: {'logit_target': -0.17891086637973785, 'logit_best_non_target': -3.0269241333007812, 'confidence_target': 0.836180427864218, 'confidence_non_target': 0.04846447937900959, 'margin': 0.7877159484852084}\n",
            "\n",
            "2022-04-11 16:12:18 (INFO): Gradient mean 0.4277574121952057 std 0.12929750978946686 with base learning rate 0.05\n",
            "2022-04-11 16:12:18 (INFO): Cuda memory 0.03317070007324219\n",
            " 44% 220/500 [01:33<01:58,  2.36it/s]2022-04-11 16:12:27 (INFO): \n",
            "Epoch: 220 Loss: -2.84808349609375 Statstics: {'logit_target': -0.17889980971813202, 'logit_best_non_target': -3.0269827842712402, 'confidence_target': 0.836189673279362, 'confidence_non_target': 0.04846163697361695, 'margin': 0.7877280363057451}\n",
            "\n",
            "2022-04-11 16:12:27 (INFO): Gradient mean 0.42861345410346985 std 0.13355088233947754 with base learning rate 0.05\n",
            "2022-04-11 16:12:27 (INFO): Cuda memory 0.03317070007324219\n",
            " 48% 240/500 [01:42<01:50,  2.35it/s]2022-04-11 16:12:35 (INFO): \n",
            "Epoch: 240 Loss: -2.8480868339538574 Statstics: {'logit_target': -0.17890729010105133, 'logit_best_non_target': -3.0269429683685303, 'confidence_target': 0.8361834182838076, 'confidence_non_target': 0.048463566555853634, 'margin': 0.787719851727954}\n",
            "\n",
            "2022-04-11 16:12:35 (INFO): Gradient mean 0.4288348853588104 std 0.13377389311790466 with base learning rate 0.05\n",
            "2022-04-11 16:12:35 (INFO): Cuda memory 0.03317070007324219\n",
            " 52% 260/500 [01:50<01:42,  2.34it/s]2022-04-11 16:12:44 (INFO): \n",
            "Epoch: 260 Loss: -2.8479857444763184 Statstics: {'logit_target': -0.178901806473732, 'logit_best_non_target': -3.0269722938537598, 'confidence_target': 0.8361880036146161, 'confidence_non_target': 0.04846214535908718, 'margin': 0.7877258582555289}\n",
            "\n",
            "2022-04-11 16:12:44 (INFO): Gradient mean 0.4282086193561554 std 0.13116517663002014 with base learning rate 0.05\n",
            "2022-04-11 16:12:44 (INFO): Cuda memory 0.03317117691040039\n",
            " 56% 280/500 [01:59<01:33,  2.35it/s]2022-04-11 16:12:52 (INFO): \n",
            "Epoch: 280 Loss: -2.848085403442383 Statstics: {'logit_target': -0.17889940738677979, 'logit_best_non_target': -3.0269851684570312, 'confidence_target': 0.8361900097047515, 'confidence_non_target': 0.04846152143220841, 'margin': 0.7877284882725432}\n",
            "\n",
            "2022-04-11 16:12:52 (INFO): Gradient mean 0.42838189005851746 std 0.13385094702243805 with base learning rate 0.05\n",
            "2022-04-11 16:12:52 (INFO): Cuda memory 0.03317070007324219\n",
            " 60% 300/500 [02:07<01:25,  2.34it/s]2022-04-11 16:13:01 (INFO): \n",
            "Epoch: 300 Loss: -2.84808611869812 Statstics: {'logit_target': -0.17890718579292297, 'logit_best_non_target': -3.0269436836242676, 'confidence_target': 0.8361835055045395, 'confidence_non_target': 0.048463531892022, 'margin': 0.7877199736125174}\n",
            "\n",
            "2022-04-11 16:13:01 (INFO): Gradient mean 0.4281686842441559 std 0.13024260103702545 with base learning rate 0.05\n",
            "2022-04-11 16:13:01 (INFO): Cuda memory 0.033170223236083984\n",
            " 64% 320/500 [02:16<01:16,  2.35it/s]2022-04-11 16:13:09 (INFO): \n",
            "Epoch: 320 Loss: -2.848085641860962 Statstics: {'logit_target': -0.17889921367168427, 'logit_best_non_target': -3.0269861221313477, 'confidence_target': 0.8361901716873948, 'confidence_non_target': 0.048461475215722125, 'margin': 0.7877286964716728}\n",
            "\n",
            "2022-04-11 16:13:09 (INFO): Gradient mean 0.42844295501708984 std 0.13399988412857056 with base learning rate 0.05\n",
            "2022-04-11 16:13:09 (INFO): Cuda memory 0.03317070007324219\n",
            " 68% 340/500 [02:24<01:07,  2.36it/s]2022-04-11 16:13:18 (INFO): \n",
            "Epoch: 340 Loss: -2.848085641860962 Statstics: {'logit_target': -0.17889921367168427, 'logit_best_non_target': -3.0269861221313477, 'confidence_target': 0.8361901716873948, 'confidence_non_target': 0.048461475215722125, 'margin': 0.7877286964716728}\n",
            "\n",
            "2022-04-11 16:13:18 (INFO): Gradient mean 0.4286723732948303 std 0.13408009707927704 with base learning rate 0.05\n",
            "2022-04-11 16:13:18 (INFO): Cuda memory 0.03317070007324219\n",
            " 72% 360/500 [02:33<00:59,  2.37it/s]2022-04-11 16:13:26 (INFO): \n",
            "Epoch: 360 Loss: -2.84808611869812 Statstics: {'logit_target': -0.17890878021717072, 'logit_best_non_target': -3.026935577392578, 'confidence_target': 0.8361821722743455, 'confidence_non_target': 0.048463924750232305, 'margin': 0.7877182475241132}\n",
            "\n",
            "2022-04-11 16:13:26 (INFO): Gradient mean 0.4279438853263855 std 0.12956339120864868 with base learning rate 0.05\n",
            "2022-04-11 16:13:26 (INFO): Cuda memory 0.033170223236083984\n",
            " 76% 380/500 [02:41<00:51,  2.35it/s]2022-04-11 16:13:35 (INFO): \n",
            "Epoch: 380 Loss: -2.848086357116699 Statstics: {'logit_target': -0.17889921367168427, 'logit_best_non_target': -3.0269861221313477, 'confidence_target': 0.8361901716873948, 'confidence_non_target': 0.048461475215722125, 'margin': 0.7877286964716728}\n",
            "\n",
            "2022-04-11 16:13:35 (INFO): Gradient mean 0.42727357149124146 std 0.12944744527339935 with base learning rate 0.05\n",
            "2022-04-11 16:13:35 (INFO): Cuda memory 0.03317070007324219\n",
            " 80% 399/500 [02:49<00:43,  2.35it/s]2022-04-11 16:13:43 (INFO): Loading search space of epoch 33 (margin=0.7846796514335194) for fine tuning\n",
            "\n",
            " 80% 400/500 [02:50<00:42,  2.34it/s]2022-04-11 16:13:43 (INFO): \n",
            "Epoch: 400 Loss: -2.837881565093994 Statstics: {'logit_target': -0.18176612257957458, 'logit_best_non_target': -3.020699977874756, 'confidence_target': 0.8337963237471054, 'confidence_non_target': 0.048767070541652066, 'margin': 0.7850292532054534}\n",
            "\n",
            "2022-04-11 16:13:43 (INFO): Gradient mean 0.42990970611572266 std 0.13666483759880066 with base learning rate 0.05\n",
            "2022-04-11 16:13:43 (INFO): Cuda memory 0.03311634063720703\n",
            " 84% 420/500 [02:58<00:33,  2.36it/s]2022-04-11 16:13:52 (INFO): \n",
            "Epoch: 420 Loss: -2.837116241455078 Statstics: {'logit_target': -0.18143011629581451, 'logit_best_non_target': -3.0185229778289795, 'confidence_target': 0.834076531624428, 'confidence_non_target': 0.04887335210195917, 'margin': 0.7852031795224688}\n",
            "\n",
            "2022-04-11 16:13:52 (INFO): Gradient mean 0.42920929193496704 std 0.13631375133991241 with base learning rate 0.05\n",
            "2022-04-11 16:13:52 (INFO): Cuda memory 0.03311634063720703\n",
            " 88% 440/500 [03:06<00:25,  2.36it/s]2022-04-11 16:14:00 (INFO): \n",
            "Epoch: 440 Loss: -2.8478307723999023 Statstics: {'logit_target': -0.17937816679477692, 'logit_best_non_target': -3.0272178649902344, 'confidence_target': 0.8357897716872353, 'confidence_non_target': 0.04845024591611501, 'margin': 0.7873395257711203}\n",
            "\n",
            "2022-04-11 16:14:00 (INFO): Gradient mean 0.42965468764305115 std 0.13451454043388367 with base learning rate 0.05\n",
            "2022-04-11 16:14:00 (INFO): Cuda memory 0.03311634063720703\n",
            " 92% 460/500 [03:15<00:16,  2.37it/s]2022-04-11 16:14:09 (INFO): \n",
            "Epoch: 460 Loss: -2.8475136756896973 Statstics: {'logit_target': -0.17929576337337494, 'logit_best_non_target': -3.026776075363159, 'confidence_target': 0.8358586464617147, 'confidence_non_target': 0.048471655461099224, 'margin': 0.7873869910006155}\n",
            "\n",
            "2022-04-11 16:14:09 (INFO): Gradient mean 0.42954614758491516 std 0.13445179164409637 with base learning rate 0.05\n",
            "2022-04-11 16:14:09 (INFO): Cuda memory 0.03311634063720703\n",
            " 96% 480/500 [03:23<00:08,  2.37it/s]2022-04-11 16:14:17 (INFO): \n",
            "Epoch: 480 Loss: -2.848433017730713 Statstics: {'logit_target': -0.17918366193771362, 'logit_best_non_target': -3.0275754928588867, 'confidence_target': 0.8359523526681949, 'confidence_non_target': 0.04843292185590036, 'margin': 0.7875194308122946}\n",
            "\n",
            "2022-04-11 16:14:17 (INFO): Gradient mean 0.4292598366737366 std 0.13442234694957733 with base learning rate 0.05\n",
            "2022-04-11 16:14:17 (INFO): Cuda memory 0.03311634063720703\n",
            "100% 500/500 [03:32<00:00,  2.36it/s]\n",
            "2022-04-11 16:14:27 (INFO): Cuda Memory before local evaluation on clean adjacency 0.03258085250854492\n",
            "2022-04-11 16:14:27 (INFO): Cuda Memory before local evaluation on perturbed adjacency 0.0326542854309082\n",
            "2022-04-11 16:14:27 (INFO): Evaluated model Vanilla GCN using LocalPRBCD with pert. edges for node 181 and budget 1: \n",
            "2022-04-11 16:14:27 (INFO): {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 1, 'degree': 2, 'logits': [[-1.031326413154602, -0.17260819673538208, -1.3101744651794434, -0.49335426092147827, 0.011285871267318726, -1.3193469047546387, 3.3251962661743164]], 'initial_logits': [[-0.9026460647583008, -0.8146041631698608, -1.0937892198562622, -0.540766179561615, -1.536024808883667, -1.7283616065979004, 6.085151672363281]], 'larget': 6, 'node_id': 181, 'perturbed_edges': [[181], [859]], 'logit_target': -0.11398593336343765, 'logit_best_non_target': -3.427896499633789, 'confidence_target': 0.8922705070390696, 'confidence_non_target': 0.03245513841642111, 'margin': 0.8598153686226485, 'initial_logit_target': -0.004901299253106117, 'initial_logit_best_non_target': -6.630819320678711, 'initial_confidence_target': 0.9951106925143317, 'initial_confidence_non_target': 0.001319081892087611, 'initial_margin': 0.9937916106222441}\n",
            "2022-04-11 16:14:27 (INFO): Completed attack and evaluation of Vanilla GCN using LocalPRBCD with pert. edges for node 181 and budget 1\n",
            "2022-04-11 16:14:27 (INFO): Original: Loss: -2.611431121826172 Statstics: {'logit_target': -0.1495024412870407, 'logit_best_non_target': -2.7609336376190186, 'confidence_target': 0.8611363357361582, 'confidence_non_target': 0.06323270436020226, 'margin': 0.797903631375956}\n",
            "\n",
            "  0% 0/500 [00:00<?, ?it/s]2022-04-11 16:14:28 (INFO): Initial: Loss: -2.611431121826172 Statstics: {'logit_target': -0.1495024412870407, 'logit_best_non_target': -2.7609336376190186, 'confidence_target': 0.8611363357361582, 'confidence_non_target': 0.06323270436020226, 'margin': 0.797903631375956}\n",
            "\n",
            "2022-04-11 16:14:28 (INFO): \n",
            "Epoch: 0 Loss: -2.611431121826172 Statstics: {'logit_target': -0.3429693579673767, 'logit_best_non_target': -1.9760451316833496, 'confidence_target': 0.7096599566505932, 'confidence_non_target': 0.1386163641572193, 'margin': 0.5710435924933739}\n",
            "\n",
            "2022-04-11 16:14:28 (INFO): Gradient mean 0.10878995060920715 std 0.0492217093706131 with base learning rate 0.25\n",
            "2022-04-11 16:14:28 (INFO): Cuda memory 0.03316974639892578\n",
            "  4% 20/500 [00:08<03:23,  2.36it/s]2022-04-11 16:14:36 (INFO): \n",
            "Epoch: 20 Loss: -0.450154185295105 Statstics: {'logit_target': -0.6626325845718384, 'logit_best_non_target': -1.1055364608764648, 'confidence_target': 0.5154924690914278, 'confidence_non_target': 0.33103324821291236, 'margin': 0.1844592208785154}\n",
            "\n",
            "2022-04-11 16:14:36 (INFO): Gradient mean 0.021812129765748978 std 0.019207896664738655 with base learning rate 0.25\n",
            "2022-04-11 16:14:36 (INFO): Cuda memory 0.03316974639892578\n",
            "  8% 40/500 [00:16<03:15,  2.35it/s]2022-04-11 16:14:45 (INFO): \n",
            "Epoch: 40 Loss: -0.3713158369064331 Statstics: {'logit_target': -0.6901187300682068, 'logit_best_non_target': -1.0607497692108154, 'confidence_target': 0.5015165204403406, 'confidence_non_target': 0.3461961457869679, 'margin': 0.15532037465337267}\n",
            "\n",
            "2022-04-11 16:14:45 (INFO): Gradient mean 0.020151911303400993 std 0.021055925637483597 with base learning rate 0.25\n",
            "2022-04-11 16:14:45 (INFO): Cuda memory 0.033168792724609375\n",
            " 12% 60/500 [00:25<03:05,  2.37it/s]2022-04-11 16:14:53 (INFO): \n",
            "Epoch: 60 Loss: -0.3648040294647217 Statstics: {'logit_target': -0.6916205286979675, 'logit_best_non_target': -1.0563435554504395, 'confidence_target': 0.5007639088940886, 'confidence_non_target': 0.34772492560024665, 'margin': 0.15303898329384197}\n",
            "\n",
            "2022-04-11 16:14:53 (INFO): Gradient mean 0.019760506227612495 std 0.021242668852210045 with base learning rate 0.25\n",
            "2022-04-11 16:14:53 (INFO): Cuda memory 0.03316974639892578\n",
            " 16% 80/500 [00:33<02:56,  2.38it/s]2022-04-11 16:15:02 (INFO): \n",
            "Epoch: 80 Loss: -0.36423587799072266 Statstics: {'logit_target': -0.6916373372077942, 'logit_best_non_target': -1.0558795928955078, 'confidence_target': 0.5007554918697442, 'confidence_non_target': 0.3478862943767818, 'margin': 0.1528691974929624}\n",
            "\n",
            "2022-04-11 16:15:02 (INFO): Gradient mean 0.019665181636810303 std 0.02127571403980255 with base learning rate 0.25\n",
            "2022-04-11 16:15:02 (INFO): Cuda memory 0.03316974639892578\n",
            " 20% 100/500 [00:42<02:51,  2.34it/s]2022-04-11 16:15:10 (INFO): \n",
            "Epoch: 100 Loss: -0.3644416332244873 Statstics: {'logit_target': -0.6914145946502686, 'logit_best_non_target': -1.0558538436889648, 'confidence_target': 0.5008670438519239, 'confidence_non_target': 0.34789525228815826, 'margin': 0.15297179156376567}\n",
            "\n",
            "2022-04-11 16:15:10 (INFO): Gradient mean 0.01948484405875206 std 0.02065957896411419 with base learning rate 0.25\n",
            "2022-04-11 16:15:10 (INFO): Cuda memory 0.033170223236083984\n",
            " 24% 120/500 [00:50<02:41,  2.35it/s]2022-04-11 16:15:19 (INFO): \n",
            "Epoch: 120 Loss: -0.36492955684661865 Statstics: {'logit_target': -0.6911914944648743, 'logit_best_non_target': -1.056103229522705, 'confidence_target': 0.5009787998481934, 'confidence_non_target': 0.3478085029580887, 'margin': 0.1531702968901047}\n",
            "\n",
            "2022-04-11 16:15:19 (INFO): Gradient mean 0.019529234617948532 std 0.02087751403450966 with base learning rate 0.25\n",
            "2022-04-11 16:15:19 (INFO): Cuda memory 0.03316974639892578\n",
            " 28% 140/500 [00:59<02:32,  2.35it/s]2022-04-11 16:15:27 (INFO): \n",
            "Epoch: 140 Loss: -0.3645200729370117 Statstics: {'logit_target': -0.6913015246391296, 'logit_best_non_target': -1.0558032989501953, 'confidence_target': 0.5009236800960214, 'confidence_non_target': 0.3479128370072081, 'margin': 0.1530108430888133}\n",
            "\n",
            "2022-04-11 16:15:27 (INFO): Gradient mean 0.0195457860827446 std 0.020983722060918808 with base learning rate 0.25\n",
            "2022-04-11 16:15:27 (INFO): Cuda memory 0.03316974639892578\n",
            " 32% 160/500 [01:07<02:24,  2.35it/s]2022-04-11 16:15:36 (INFO): \n",
            "Epoch: 160 Loss: -0.3638730049133301 Statstics: {'logit_target': -0.6915181279182434, 'logit_best_non_target': -1.055406093597412, 'confidence_target': 0.5008151901343918, 'confidence_non_target': 0.34805105729747166, 'margin': 0.15276413283692014}\n",
            "\n",
            "2022-04-11 16:15:36 (INFO): Gradient mean 0.01973089948296547 std 0.021569889038801193 with base learning rate 0.25\n",
            "2022-04-11 16:15:36 (INFO): Cuda memory 0.03316974639892578\n",
            " 36% 180/500 [01:16<02:15,  2.36it/s]2022-04-11 16:15:44 (INFO): \n",
            "Epoch: 180 Loss: -0.3638577461242676 Statstics: {'logit_target': -0.691501796245575, 'logit_best_non_target': -1.0553624629974365, 'confidence_target': 0.5008233693509344, 'confidence_non_target': 0.3480662433052086, 'margin': 0.15275712604572583}\n",
            "\n",
            "2022-04-11 16:15:44 (INFO): Gradient mean 0.01963629759848118 std 0.021236257627606392 with base learning rate 0.25\n",
            "2022-04-11 16:15:44 (INFO): Cuda memory 0.03316974639892578\n",
            " 40% 200/500 [01:24<02:07,  2.36it/s]2022-04-11 16:15:53 (INFO): \n",
            "Epoch: 200 Loss: -0.3636658191680908 Statstics: {'logit_target': -0.6915484666824341, 'logit_best_non_target': -1.0552325248718262, 'confidence_target': 0.5007999962509182, 'confidence_non_target': 0.3481114733189398, 'margin': 0.15268852293197838}\n",
            "\n",
            "2022-04-11 16:15:53 (INFO): Gradient mean 0.019672395661473274 std 0.021430272608995438 with base learning rate 0.25\n",
            "2022-04-11 16:15:53 (INFO): Cuda memory 0.033168792724609375\n",
            " 44% 220/500 [01:33<01:59,  2.35it/s]2022-04-11 16:16:01 (INFO): \n",
            "Epoch: 220 Loss: -0.3636643886566162 Statstics: {'logit_target': -0.6915320754051208, 'logit_best_non_target': -1.0552122592926025, 'confidence_target': 0.5008082050698115, 'confidence_non_target': 0.34811852807106514, 'margin': 0.15268967699874636}\n",
            "\n",
            "2022-04-11 16:16:01 (INFO): Gradient mean 0.019698798656463623 std 0.02142743580043316 with base learning rate 0.25\n",
            "2022-04-11 16:16:01 (INFO): Cuda memory 0.03316974639892578\n",
            " 48% 240/500 [01:41<01:50,  2.34it/s]2022-04-11 16:16:10 (INFO): \n",
            "Epoch: 240 Loss: -0.3636465072631836 Statstics: {'logit_target': -0.6914995312690735, 'logit_best_non_target': -1.0551481246948242, 'confidence_target': 0.500824503705382, 'confidence_non_target': 0.3481408552288061, 'margin': 0.1526836484765759}\n",
            "\n",
            "2022-04-11 16:16:10 (INFO): Gradient mean 0.01969360001385212 std 0.021406937390565872 with base learning rate 0.25\n",
            "2022-04-11 16:16:10 (INFO): Cuda memory 0.03316974639892578\n",
            " 52% 260/500 [01:50<01:42,  2.34it/s]2022-04-11 16:16:18 (INFO): \n",
            "Epoch: 260 Loss: -0.3636389970779419 Statstics: {'logit_target': -0.6914950013160706, 'logit_best_non_target': -1.055131196975708, 'confidence_target': 0.5008267724219851, 'confidence_non_target': 0.34814674850929606, 'margin': 0.1526800239126891}\n",
            "\n",
            "2022-04-11 16:16:18 (INFO): Gradient mean 0.019693732261657715 std 0.02144443243741989 with base learning rate 0.25\n",
            "2022-04-11 16:16:18 (INFO): Cuda memory 0.03316974639892578\n",
            " 56% 280/500 [01:58<01:33,  2.34it/s]2022-04-11 16:16:27 (INFO): \n",
            "Epoch: 280 Loss: -0.36363184452056885 Statstics: {'logit_target': -0.69149249792099, 'logit_best_non_target': -1.055116891860962, 'confidence_target': 0.5008280261908328, 'confidence_non_target': 0.3481517288241039, 'margin': 0.15267629736672889}\n",
            "\n",
            "2022-04-11 16:16:27 (INFO): Gradient mean 0.01976681686937809 std 0.021621746942400932 with base learning rate 0.25\n",
            "2022-04-11 16:16:27 (INFO): Cuda memory 0.033168792724609375\n",
            " 60% 300/500 [02:07<01:25,  2.35it/s]2022-04-11 16:16:35 (INFO): \n",
            "Epoch: 300 Loss: -0.3636305332183838 Statstics: {'logit_target': -0.6914886832237244, 'logit_best_non_target': -1.0551080703735352, 'confidence_target': 0.5008299367017788, 'confidence_non_target': 0.3481548000537487, 'margin': 0.15267513664803006}\n",
            "\n",
            "2022-04-11 16:16:35 (INFO): Gradient mean 0.019765909761190414 std 0.02157568372786045 with base learning rate 0.25\n",
            "2022-04-11 16:16:35 (INFO): Cuda memory 0.03316974639892578\n",
            " 64% 320/500 [02:15<01:16,  2.36it/s]2022-04-11 16:16:44 (INFO): \n",
            "Epoch: 320 Loss: -0.3636397123336792 Statstics: {'logit_target': -0.6914807558059692, 'logit_best_non_target': -1.0551090240478516, 'confidence_target': 0.5008339070056484, 'confidence_non_target': 0.34815446802761607, 'margin': 0.15267943897803238}\n",
            "\n",
            "2022-04-11 16:16:44 (INFO): Gradient mean 0.019676709547638893 std 0.021129539236426353 with base learning rate 0.25\n",
            "2022-04-11 16:16:44 (INFO): Cuda memory 0.033170223236083984\n",
            " 68% 340/500 [02:24<01:07,  2.36it/s]2022-04-11 16:16:52 (INFO): \n",
            "Epoch: 340 Loss: -0.36362993717193604 Statstics: {'logit_target': -0.6914713382720947, 'logit_best_non_target': -1.0551130771636963, 'confidence_target': 0.5008386236481427, 'confidence_non_target': 0.34815305692008497, 'margin': 0.15268556672805772}\n",
            "\n",
            "2022-04-11 16:16:52 (INFO): Gradient mean 0.019457414746284485 std 0.020334331318736076 with base learning rate 0.25\n",
            "2022-04-11 16:16:52 (INFO): Cuda memory 0.03316974639892578\n",
            " 72% 360/500 [02:32<00:59,  2.35it/s]2022-04-11 16:17:01 (INFO): \n",
            "Epoch: 360 Loss: -0.36362600326538086 Statstics: {'logit_target': -0.6914781332015991, 'logit_best_non_target': -1.0550984144210815, 'confidence_target': 0.500835220496564, 'confidence_non_target': 0.3481581618361751, 'margin': 0.15267705866038894}\n",
            "\n",
            "2022-04-11 16:17:01 (INFO): Gradient mean 0.019680848345160484 std 0.021372821182012558 with base learning rate 0.25\n",
            "2022-04-11 16:17:01 (INFO): Cuda memory 0.033170223236083984\n",
            " 76% 380/500 [02:41<00:50,  2.36it/s]2022-04-11 16:17:09 (INFO): \n",
            "Epoch: 380 Loss: -0.36362457275390625 Statstics: {'logit_target': -0.6914746165275574, 'logit_best_non_target': -1.055098533630371, 'confidence_target': 0.50083698177388, 'confidence_non_target': 0.34815812033249044, 'margin': 0.1526788614413896}\n",
            "\n",
            "2022-04-11 16:17:09 (INFO): Gradient mean 0.019647104665637016 std 0.021407028660178185 with base learning rate 0.25\n",
            "2022-04-11 16:17:09 (INFO): Cuda memory 0.03316974639892578\n",
            " 80% 399/500 [02:49<00:42,  2.37it/s]2022-04-11 16:17:17 (INFO): Loading search space of epoch 233 (margin=0.15267406687013052) for fine tuning\n",
            "\n",
            " 80% 400/500 [02:49<00:42,  2.38it/s]2022-04-11 16:17:18 (INFO): \n",
            "Epoch: 400 Loss: -0.3636270761489868 Statstics: {'logit_target': -0.691504716873169, 'logit_best_non_target': -1.055149793624878, 'confidence_target': 0.5008219066345182, 'confidence_non_target': 0.34814027420655475, 'margin': 0.15268163242796345}\n",
            "\n",
            "2022-04-11 16:17:18 (INFO): Gradient mean 0.01971849612891674 std 0.021511657163500786 with base learning rate 0.25\n",
            "2022-04-11 16:17:18 (INFO): Cuda memory 0.033115386962890625\n",
            " 84% 420/500 [02:58<00:33,  2.39it/s]2022-04-11 16:17:26 (INFO): \n",
            "Epoch: 420 Loss: -0.3636467456817627 Statstics: {'logit_target': -0.691506028175354, 'logit_best_non_target': -1.0551389455795288, 'confidence_target': 0.5008212499060882, 'confidence_non_target': 0.34814405086852185, 'margin': 0.15267719903756638}\n",
            "\n",
            "2022-04-11 16:17:26 (INFO): Gradient mean 0.01972164399921894 std 0.021512629464268684 with base learning rate 0.25\n",
            "2022-04-11 16:17:26 (INFO): Cuda memory 0.033115386962890625\n",
            " 88% 440/500 [03:06<00:25,  2.37it/s]2022-04-11 16:17:34 (INFO): \n",
            "Epoch: 440 Loss: -0.3636329174041748 Statstics: {'logit_target': -0.6915054321289062, 'logit_best_non_target': -1.0551350116729736, 'confidence_target': 0.5008215484189042, 'confidence_non_target': 0.3481454204373796, 'margin': 0.15267612798152463}\n",
            "\n",
            "2022-04-11 16:17:34 (INFO): Gradient mean 0.01972213201224804 std 0.021512992680072784 with base learning rate 0.25\n",
            "2022-04-11 16:17:34 (INFO): Cuda memory 0.033115386962890625\n",
            " 92% 460/500 [03:15<00:16,  2.37it/s]2022-04-11 16:17:43 (INFO): \n",
            "Epoch: 460 Loss: -0.3636265993118286 Statstics: {'logit_target': -0.6915059089660645, 'logit_best_non_target': -1.055130958557129, 'confidence_target': 0.5008213096086372, 'confidence_non_target': 0.3481468315139591, 'margin': 0.15267447809467816}\n",
            "\n",
            "2022-04-11 16:17:43 (INFO): Gradient mean 0.019722674041986465 std 0.02151326835155487 with base learning rate 0.25\n",
            "2022-04-11 16:17:43 (INFO): Cuda memory 0.033115386962890625\n",
            " 96% 480/500 [03:23<00:08,  2.39it/s]2022-04-11 16:17:51 (INFO): \n",
            "Epoch: 480 Loss: -0.36364293098449707 Statstics: {'logit_target': -0.6914992332458496, 'logit_best_non_target': -1.055136799812317, 'confidence_target': 0.5008246529627375, 'confidence_non_target': 0.3481447979054127, 'margin': 0.1526798550573248}\n",
            "\n",
            "2022-04-11 16:17:51 (INFO): Gradient mean 0.019723912701010704 std 0.021513553336262703 with base learning rate 0.25\n",
            "2022-04-11 16:17:51 (INFO): Cuda memory 0.033115386962890625\n",
            "100% 500/500 [03:31<00:00,  2.36it/s]\n",
            "2022-04-11 16:18:01 (INFO): Cuda Memory before local evaluation on clean adjacency 0.032581329345703125\n",
            "2022-04-11 16:18:02 (INFO): Cuda Memory before local evaluation on perturbed adjacency 0.032654762268066406\n",
            "2022-04-11 16:18:02 (INFO): Evaluated model Vanilla GCN using LocalPRBCD with pert. edges for node 920 and budget 5: \n",
            "2022-04-11 16:18:02 (INFO): {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 5, 'degree': 10, 'logits': [[-1.173211932182312, 1.5413639545440674, -0.5650124549865723, -0.6682339906692505, 2.001338005065918, -1.6457635164260864, -0.7107100486755371]], 'initial_logits': [[-1.2673898935317993, 0.5244807004928589, -0.516231894493103, -0.5536350011825562, 3.135911703109741, -1.8813700675964355, -0.8787693977355957]], 'larget': 4, 'node_id': 920, 'perturbed_edges': [[920, 920, 920, 920, 920], [940, 1011, 1074, 1143, 2609]], 'logit_target': -0.6479810476303101, 'logit_best_non_target': -1.1079550981521606, 'confidence_target': 0.5231008270088131, 'confidence_non_target': 0.33023356631934014, 'margin': 0.19286726068947296, 'initial_logit_target': -0.1495024412870407, 'initial_logit_best_non_target': -2.7609336376190186, 'initial_confidence_target': 0.8611363357361582, 'initial_confidence_non_target': 0.06323270436020226, 'initial_margin': 0.797903631375956}\n",
            "2022-04-11 16:18:02 (INFO): Completed attack and evaluation of Vanilla GCN using LocalPRBCD with pert. edges for node 920 and budget 5\n",
            "2022-04-11 16:18:02 (INFO): Result: {'results': [{'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 1, 'degree': 2, 'logits': [[-0.8856097459793091, -1.534730315208435, 3.812166213989258, -1.9702682495117188, -1.6980338096618652, 2.5939292907714844, -1.3224011659622192]], 'initial_logits': [[-1.4388686418533325, -1.9843776226043701, 8.674994468688965, -2.0514442920684814, -1.4827415943145752, -0.9712307453155518, -1.355878233909607]], 'larget': 2, 'node_id': 475, 'perturbed_edges': [[475], [1244]], 'logit_target': -0.279634028673172, 'logit_best_non_target': -1.497870922088623, 'confidence_target': 0.7560603872535082, 'confidence_non_target': 0.2236057277243299, 'margin': 0.5324546595291784, 'initial_logit_target': -0.00023326536756940186, 'initial_logit_best_non_target': -9.646458625793457, 'initial_confidence_target': 0.9997667618366811, 'initial_confidence_non_target': 6.465412654419923e-05, 'initial_margin': 0.9997021077101369}, {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 15, 'degree': 30, 'logits': [[0.18504709005355835, 0.2727407217025757, -0.6521108150482178, -0.27883657813072205, -0.6258774399757385, 0.7944414615631104, -0.61902916431427]], 'initial_logits': [[-0.0848335325717926, 0.5635748505592346, -0.8179147243499756, 0.0325467586517334, -0.46229881048202515, 0.5566858053207397, -0.7532972097396851]], 'larget': 1, 'node_id': 2210, 'perturbed_edges': [[2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210, 2210], [264, 441, 443, 1103, 1198, 1233, 1283, 1294, 1343, 1536, 1654, 1655, 1661, 2206, 2567]], 'logit_target': -1.6846612691879272, 'logit_best_non_target': -1.1629605293273926, 'confidence_target': 0.18550725833901685, 'confidence_non_target': 0.3125594683071146, 'margin': -0.12705220996809777, 'initial_logit_target': -1.3817017078399658, 'initial_logit_best_non_target': -1.3885908126831055, 'initial_confidence_target': 0.25115080391890404, 'initial_confidence_non_target': 0.24942654581609308, 'initial_margin': 0.0017242581028109605}, {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 1, 'degree': 2, 'logits': [[-1.031326413154602, -0.17260819673538208, -1.3101744651794434, -0.49335426092147827, 0.011285871267318726, -1.3193469047546387, 3.3251962661743164]], 'initial_logits': [[-0.9026460647583008, -0.8146041631698608, -1.0937892198562622, -0.540766179561615, -1.536024808883667, -1.7283616065979004, 6.085151672363281]], 'larget': 6, 'node_id': 181, 'perturbed_edges': [[181], [859]], 'logit_target': -0.11398593336343765, 'logit_best_non_target': -3.427896499633789, 'confidence_target': 0.8922705070390696, 'confidence_non_target': 0.03245513841642111, 'margin': 0.8598153686226485, 'initial_logit_target': -0.004901299253106117, 'initial_logit_best_non_target': -6.630819320678711, 'initial_confidence_target': 0.9951106925143317, 'initial_confidence_non_target': 0.001319081892087611, 'initial_margin': 0.9937916106222441}, {'label': 'Vanilla GCN', 'epsilon': 0.5, 'n_perturbations': 5, 'degree': 10, 'logits': [[-1.173211932182312, 1.5413639545440674, -0.5650124549865723, -0.6682339906692505, 2.001338005065918, -1.6457635164260864, -0.7107100486755371]], 'initial_logits': [[-1.2673898935317993, 0.5244807004928589, -0.516231894493103, -0.5536350011825562, 3.135911703109741, -1.8813700675964355, -0.8787693977355957]], 'larget': 4, 'node_id': 920, 'perturbed_edges': [[920, 920, 920, 920, 920], [940, 1011, 1074, 1143, 2609]], 'logit_target': -0.6479810476303101, 'logit_best_non_target': -1.1079550981521606, 'confidence_target': 0.5231008270088131, 'confidence_non_target': 0.33023356631934014, 'margin': 0.19286726068947296, 'initial_logit_target': -0.1495024412870407, 'initial_logit_best_non_target': -2.7609336376190186, 'initial_confidence_target': 0.8611363357361582, 'initial_confidence_non_target': 0.06323270436020226, 'initial_margin': 0.797903631375956}]}\n",
            "2022-04-11 16:18:02 (INFO): Completed after 0:14:20\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python script_execute_experiment.py --config-file 'config/attack_evasion_local_direct/demo_localprbcd.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMtpxI6cCkwz"
      },
      "source": [
        "### 2.2 PR-BCD Attack\n",
        "\n",
        "Now let's do the same with a non-local `PR-BCD` attack:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nxQ5bc_Hy2Kj"
      },
      "outputs": [],
      "source": [
        "demo_prbcd_config = {\n",
        "    'seml': {'name': 'rgnn_at_scale_attack_evasion_global_direct', \n",
        "             'executable': 'experiments/experiment_global_attack_direct.py', \n",
        "             'project_root_dir': '../..', \n",
        "             'output_dir': 'config/attack_evasion_global_direct/output'}, \n",
        "    'slurm': {'experiments_per_job': 4, \n",
        "              'sbatch_options': {'gres': 'gpu:1', 'mem': '4G', 'cpus-per-task': 4, 'time': '1-00:00'}}, \n",
        "    'fixed': {'data_dir': 'data/', \n",
        "              'artifact_dir': 'cache', \n",
        "              'pert_adj_storage_type': 'evasion_global_adj',\n",
        "              'pert_attr_storage_type': 'evasion_global_attr',\n",
        "              'device': 0, \n",
        "              'data_device': 0, \n",
        "              'binary_attr': False},  \n",
        "    'grid': {'epsilons': {'type': 'choice', 'options': [[0.5]]}, \n",
        "             'seed': {'type': 'choice', 'options': [0]}, \n",
        "             'dataset': {'type': 'choice', 'options': ['cora_ml']}}, \n",
        "    'prbcd_gcn': {'fixed': {'attack': 'PRBCD',\n",
        "                            'model_label': 'Vanilla GCN', \n",
        "                            'model_storage_type': 'demo',\n",
        "                            'attack_params': {\n",
        "                              'epochs': 500,\n",
        "                              'fine_tune_epochs': 100,\n",
        "                              'keep_heuristic': 'WeightOnly',\n",
        "                              'search_space_size': 100_000,\n",
        "                              'do_synchronize': True,\n",
        "                              'loss_type': 'tanhMargin'}}}\n",
        "}                      \n",
        "\n",
        "with open(r'/content/robustness_of_gnns_at_scale/config/attack_evasion_global_direct/demo_prbcd.yaml', 'w') as file:\n",
        "    documents = yaml.dump(demo_prbcd_config, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIbmV_OmDKvC",
        "outputId": "47271fc5-8b65-4485-ad3d-fc8383794325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-11 16:18:03,865 - root - DEBUG - Namespace(config_file='config/attack_evasion_global_direct/demo_prbcd.yaml', kwargs={}, output='output')\n",
            "2022-04-11 16:18:06,859 - git.cmd - DEBUG - Popen(['git', 'version'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:18:06,873 - git.cmd - DEBUG - Popen(['git', 'version'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:18:06,886 - git.cmd - DEBUG - Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:18:06,900 - git.cmd - DEBUG - Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:18:06,986 - git.cmd - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=<valid stream>)\n",
            "2022-04-11 16:18:07,002 - git.cmd - DEBUG - Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:18:07,015 - git.cmd - DEBUG - Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=None)\n",
            "2022-04-11 16:18:07,095 - git.cmd - DEBUG - Popen(['git', 'cat-file', '--batch-check'], cwd=/content/robustness_of_gnns_at_scale, universal_newlines=False, shell=None, istream=<valid stream>)\n",
            "2022-04-11 16:18:07 (WARNING): No observers have been added to this run\n",
            "2022-04-11 16:18:07 (INFO): Running command 'run'\n",
            "2022-04-11 16:18:07 (INFO): Started\n",
            "2022-04-11 16:18:07 (INFO): {'dataset': 'cora_ml', 'attack': 'PRBCD', 'attack_params': {'epochs': 500, 'fine_tune_epochs': 100, 'keep_heuristic': 'WeightOnly', 'search_space_size': 100000, 'do_synchronize': True, 'loss_type': 'tanhMargin'}, 'epsilons': [0.5], 'make_undirected': True, 'binary_attr': False, 'seed': 0, 'artifact_dir': 'cache', 'pert_adj_storage_type': 'evasion_global_adj', 'pert_attr_storage_type': 'evasion_global_attr', 'model_label': 'Vanilla GCN', 'model_storage_type': 'demo', 'device': 0, 'data_device': 0}\n",
            "2022-04-11 16:18:09 (INFO): Evaluate  PRBCD for model 'Vanilla GCN'.\n",
            "2022-04-11 16:18:09 (INFO): No cached perturbations found for model 'Vanilla GCN' and eps 0.5. Execute attack...\n",
            "/content/robustness_of_gnns_at_scale/rgnn_at_scale/attacks/prbcd.py:390: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  + (n - row_idx) * ((n - row_idx) - 1) // 2\n",
            "2022-04-11 16:18:09 (INFO): \n",
            "Before the attack - Loss: -0.6321962475776672 Accuracy: 81.858 %\n",
            "\n",
            "  0% 0/500 [00:00<?, ?it/s]2022-04-11 16:18:10 (INFO): \n",
            "Epoch: 0 Loss: -0.6322021484375 Accuracy: 79.209 %\n",
            "\n",
            "  4% 20/500 [00:09<03:38,  2.19it/s]2022-04-11 16:18:19 (INFO): \n",
            "Epoch: 20 Loss: -0.12230443954467773 Accuracy: 54.308 %\n",
            "\n",
            "  8% 40/500 [00:18<03:32,  2.17it/s]2022-04-11 16:18:28 (INFO): \n",
            "Epoch: 40 Loss: -0.02975119650363922 Accuracy: 50.277 %\n",
            "\n",
            " 12% 60/500 [00:27<03:25,  2.14it/s]2022-04-11 16:18:38 (INFO): \n",
            "Epoch: 60 Loss: 0.016467900946736336 Accuracy: 47.905 %\n",
            "\n",
            " 16% 80/500 [00:37<03:13,  2.17it/s]2022-04-11 16:18:47 (INFO): \n",
            "Epoch: 80 Loss: 0.04827171936631203 Accuracy: 46.601 %\n",
            "\n",
            " 20% 100/500 [00:46<03:05,  2.16it/s]2022-04-11 16:18:56 (INFO): \n",
            "Epoch: 100 Loss: 0.0718868225812912 Accuracy: 45.257 %\n",
            "\n",
            " 24% 120/500 [00:55<02:55,  2.17it/s]2022-04-11 16:19:05 (INFO): \n",
            "Epoch: 120 Loss: 0.08804845809936523 Accuracy: 44.506 %\n",
            "\n",
            " 28% 140/500 [01:04<02:47,  2.14it/s]2022-04-11 16:19:15 (INFO): \n",
            "Epoch: 140 Loss: 0.09564992040395737 Accuracy: 44.387 %\n",
            "\n",
            " 32% 160/500 [01:14<02:38,  2.15it/s]2022-04-11 16:19:24 (INFO): \n",
            "Epoch: 160 Loss: 0.10069368034601212 Accuracy: 44.269 %\n",
            "\n",
            " 36% 180/500 [01:23<02:28,  2.15it/s]2022-04-11 16:19:33 (INFO): \n",
            "Epoch: 180 Loss: 0.10818690806627274 Accuracy: 43.874 %\n",
            "\n",
            " 40% 200/500 [01:32<02:18,  2.16it/s]2022-04-11 16:19:43 (INFO): \n",
            "Epoch: 200 Loss: 0.11330818384885788 Accuracy: 43.715 %\n",
            "\n",
            " 44% 220/500 [01:42<02:10,  2.15it/s]2022-04-11 16:19:52 (INFO): \n",
            "Epoch: 220 Loss: 0.11825647205114365 Accuracy: 43.518 %\n",
            "\n",
            " 48% 240/500 [01:51<01:59,  2.18it/s]2022-04-11 16:20:01 (INFO): \n",
            "Epoch: 240 Loss: 0.1215701624751091 Accuracy: 43.399 %\n",
            "\n",
            " 52% 260/500 [02:00<01:52,  2.14it/s]2022-04-11 16:20:10 (INFO): \n",
            "Epoch: 260 Loss: 0.12430325895547867 Accuracy: 43.320 %\n",
            "\n",
            " 56% 280/500 [02:09<01:42,  2.15it/s]2022-04-11 16:20:20 (INFO): \n",
            "Epoch: 280 Loss: 0.12926708161830902 Accuracy: 43.043 %\n",
            "\n",
            " 60% 300/500 [02:19<01:32,  2.15it/s]2022-04-11 16:20:29 (INFO): \n",
            "Epoch: 300 Loss: 0.13210272789001465 Accuracy: 42.925 %\n",
            "\n",
            " 64% 320/500 [02:28<01:23,  2.16it/s]2022-04-11 16:20:38 (INFO): \n",
            "Epoch: 320 Loss: 0.13432323932647705 Accuracy: 42.846 %\n",
            "\n",
            " 68% 340/500 [02:37<01:14,  2.15it/s]2022-04-11 16:20:48 (INFO): \n",
            "Epoch: 340 Loss: 0.13655239343643188 Accuracy: 42.767 %\n",
            "\n",
            " 72% 360/500 [02:47<01:05,  2.13it/s]2022-04-11 16:20:57 (INFO): \n",
            "Epoch: 360 Loss: 0.13854114711284637 Accuracy: 42.688 %\n",
            "\n",
            " 76% 380/500 [02:56<00:56,  2.13it/s]2022-04-11 16:21:06 (INFO): \n",
            "Epoch: 380 Loss: 0.14249953627586365 Accuracy: 42.490 %\n",
            "\n",
            " 80% 399/500 [03:05<00:47,  2.14it/s]2022-04-11 16:21:15 (INFO): Loading search space of epoch 394 (accuarcy=0.42411065101623535) for fine tuning\n",
            "\n",
            " 80% 400/500 [03:05<00:46,  2.14it/s]2022-04-11 16:21:16 (INFO): \n",
            "Epoch: 400 Loss: 0.14467491209506989 Accuracy: 42.411 %\n",
            "\n",
            " 84% 420/500 [03:15<00:37,  2.16it/s]2022-04-11 16:21:25 (INFO): \n",
            "Epoch: 420 Loss: 0.1461082249879837 Accuracy: 42.372 %\n",
            "\n",
            " 88% 440/500 [03:24<00:28,  2.14it/s]2022-04-11 16:21:34 (INFO): \n",
            "Epoch: 440 Loss: 0.14654074609279633 Accuracy: 42.332 %\n",
            "\n",
            " 92% 460/500 [03:33<00:18,  2.14it/s]2022-04-11 16:21:44 (INFO): \n",
            "Epoch: 460 Loss: 0.14689961075782776 Accuracy: 42.332 %\n",
            "\n",
            " 96% 480/500 [03:43<00:09,  2.15it/s]2022-04-11 16:21:53 (INFO): \n",
            "Epoch: 480 Loss: 0.14723528921604156 Accuracy: 42.332 %\n",
            "\n",
            "100% 500/500 [03:52<00:00,  2.15it/s]\n",
            "2022-04-11 16:22:03 (INFO): 3-th sampling: too many samples 4035.0\n",
            "2022-04-11 16:22:03 (INFO): 5-th sampling: too many samples 4026.0\n",
            "2022-04-11 16:22:03 (INFO): 6-th sampling: too many samples 4043.0\n",
            "2022-04-11 16:22:03 (INFO): 8-th sampling: too many samples 4024.0\n",
            "2022-04-11 16:22:03 (INFO): 10-th sampling: too many samples 4073.0\n",
            "2022-04-11 16:22:04 (INFO): 14-th sampling: too many samples 4045.0\n",
            "2022-04-11 16:22:04 (INFO): 17-th sampling: too many samples 4021.0\n",
            "2022-04-11 16:22:05 (INFO): Result: {'results': [{'label': 'Vanilla GCN', 'epsilon': 0.5, 'accuracy': 0.43438735604286194}]}\n",
            "2022-04-11 16:22:05 (INFO): Completed after 0:03:58\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python script_execute_experiment.py --config-file 'config/attack_evasion_global_direct/demo_prbcd.yaml'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMTBYyJ0b1pxR7RHLtRlMS8",
      "include_colab_link": true,
      "name": "Quick_start-robustness_gnns_at_scale.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0250afcb7080447cb1420d3257c01891": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d310d3b5704d09a6736ff7a4f02074",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d234a4a2e564ef4b661abd09a579d67",
            "value": 535
          }
        },
        "09354c2e30c24e73a7296291d61e7c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d234a4a2e564ef4b661abd09a579d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13b730ce7edd4674aa3f105cfe7bc3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "295ff31ab4e6450d8cd0502c69d97288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c1b6ca2211c422f97aff856fd04baaa",
            "placeholder": "​",
            "style": "IPY_MODEL_7761eeede57e40b7999ec8df737fad3f",
            "value": "Training...:  18%"
          }
        },
        "2c1b6ca2211c422f97aff856fd04baaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7761eeede57e40b7999ec8df737fad3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d310d3b5704d09a6736ff7a4f02074": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac11939e90534d398eb3f256ea174853": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_295ff31ab4e6450d8cd0502c69d97288",
              "IPY_MODEL_0250afcb7080447cb1420d3257c01891",
              "IPY_MODEL_e5183a9fe79140bca1bf699b1fa8c553"
            ],
            "layout": "IPY_MODEL_cad60df04ed544e6a09e20558a244976"
          }
        },
        "cad60df04ed544e6a09e20558a244976": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5183a9fe79140bca1bf699b1fa8c553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09354c2e30c24e73a7296291d61e7c4c",
            "placeholder": "​",
            "style": "IPY_MODEL_13b730ce7edd4674aa3f105cfe7bc3cd",
            "value": " 535/3000 [00:04&lt;00:19, 125.05it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
